{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">1. Import Required Libraries & Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "cwd = Path(os.getcwd()) / 'AI-Module'\n",
    "#cwd = Path(os.getcwd()).parent\n",
    "base_path = cwd / 'Modules'\n",
    "sys.path.append(str(base_path))\n",
    "from KeyFrameExtractorClass import KeyFrameExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = pd.DataFrame(columns=['video_name', 'video_path', 'translation'])\n",
    "videos_path = cwd / 'Resources' / 'Datasets' / 'Videos'\n",
    "df_translation = pd.read_csv(str(videos_path.parent.parent / 'Translations' / 'how2sign_train.csv'), sep='\\t', usecols=['VIDEO_NAME', 'SENTENCE']).groupby('VIDEO_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>video_path</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-00cp1iGiDw-5-rgb_front</td>\n",
       "      <td>/app/AI-Module/Resources/Datasets/Videos/filte...</td>\n",
       "      <td>If you like to find more about my services you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-08ZGCviCm4-5-rgb_front</td>\n",
       "      <td>/app/AI-Module/Resources/Datasets/Videos/filte...</td>\n",
       "      <td>Hello welcome my name is Julio Nutt and I am a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1KpdDGPCq4-5-rgb_front</td>\n",
       "      <td>/app/AI-Module/Resources/Datasets/Videos/filte...</td>\n",
       "      <td>All right, the drink we're about to make is ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1XUnputrgk-5-rgb_front</td>\n",
       "      <td>/app/AI-Module/Resources/Datasets/Videos/filte...</td>\n",
       "      <td>Okay, another thing that for me is important. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3sFcDBFxc8-5-rgb_front</td>\n",
       "      <td>/app/AI-Module/Resources/Datasets/Videos/filte...</td>\n",
       "      <td>All right, we're ready to start our fire. What...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                video_name                                         video_path  \\\n",
       "0  -00cp1iGiDw-5-rgb_front  /app/AI-Module/Resources/Datasets/Videos/filte...   \n",
       "0  -08ZGCviCm4-5-rgb_front  /app/AI-Module/Resources/Datasets/Videos/filte...   \n",
       "0  -1KpdDGPCq4-5-rgb_front  /app/AI-Module/Resources/Datasets/Videos/filte...   \n",
       "0  -1XUnputrgk-5-rgb_front  /app/AI-Module/Resources/Datasets/Videos/filte...   \n",
       "0  -3sFcDBFxc8-5-rgb_front  /app/AI-Module/Resources/Datasets/Videos/filte...   \n",
       "\n",
       "                                         translation  \n",
       "0  If you like to find more about my services you...  \n",
       "0  Hello welcome my name is Julio Nutt and I am a...  \n",
       "0  All right, the drink we're about to make is ca...  \n",
       "0  Okay, another thing that for me is important. ...  \n",
       "0  All right, we're ready to start our fire. What...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for video in os.listdir(str(videos_path)):\n",
    "    video_path = videos_path / video\n",
    "    video_name = video.removeprefix('filtered_').removesuffix('.mp4')\n",
    "    sentence = df_translation.get_group(video_name)['SENTENCE'].str.cat(sep=' ')\n",
    "    new_row = pd.DataFrame({'video_name': [video_name], 'video_path': [video_path], 'translation': [sentence]})\n",
    "    df_videos = pd.concat([df_videos, new_row])\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "#specify GPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Pytorch is using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.device(0), torch.cuda.get_device_name(0), device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_df = df_videos['video_path'].to_frame()\n",
    "y_df = df_videos['translation'].to_frame()\n",
    "seed = 31991\n",
    "batch_size = 32\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=(100/500), random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=(100/400), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(), y_train.head(), X_val.head(), y_val.head(), X_test.head(), y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">3. Import ViT</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T11:49:24.18724Z",
     "iopub.status.busy": "2021-05-20T11:49:24.186902Z",
     "iopub.status.idle": "2021-05-20T11:50:15.137513Z",
     "shell.execute_reply": "2021-05-20T11:50:15.135994Z",
     "shell.execute_reply.started": "2021-05-20T11:49:24.187212Z"
    }
   },
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gpt = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_label_train = tokenizer(\n",
    "    y_train['translation'].tolist(),\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokens_label_val = tokenizer(\n",
    "    y_val['translation'].tolist(),\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokens_label_test = tokenizer(\n",
    "    y_test['translation'].tolist(),\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">5. List to Tensors</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:16:52.707444Z",
     "iopub.status.busy": "2021-05-20T12:16:52.707085Z",
     "iopub.status.idle": "2021-05-20T12:16:52.744645Z",
     "shell.execute_reply": "2021-05-20T12:16:52.743449Z",
     "shell.execute_reply.started": "2021-05-20T12:16:52.707415Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train['token_ids'] = tokens_label_train['input_ids'].tolist()\n",
    "y_val['token_ids'] = tokens_label_val['input_ids'].tolist()\n",
    "y_test['token_ids'] = tokens_label_test['input_ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/AI-Module/Resources/Datasets/Videos/filtered_1NnjqlLI_6s-8-rgb_front.mp4\n",
      "It's a history, a living history, paper, paper, paper. Just make sure that you take care of your paper because just like every other piece of collectibles condition is everything so pieces that are torn or damaged or water soiled, the value just goes down dramatically so it is all about condition so enjoy your paper and treat it with kindness. I'm Jan, see ya.\n"
     ]
    }
   ],
   "source": [
    "# Path a un video de ejemplo\n",
    "row = df_videos[df_videos['video_name'] == \"1NnjqlLI_6s-8-rgb_front\"]\n",
    "video_path = row[\"video_path\"].iloc[0]\n",
    "translation = row[\"translation\"].iloc[0]\n",
    "print(video_path)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">7. Model Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoToTextModel(nn.Module):\n",
    "    def __init__(self, vit, gpt, tokenizer):\n",
    "        super(VideoToTextModel, self).__init__()\n",
    "        self.vit = vit\n",
    "        self.gpt = gpt\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def extract_frames(self, video_path: str) -> torch.Tensor:\n",
    "        keyframe_extractor = KeyFrameExtractor()  # Instanciamos el extractor de frames\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        frames = keyframe_extractor.extractKeyFrames(return_frame=True, draw=False, video=video)\n",
    "        # Convertimos las imágenes a tensores para el modelo ViT\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "        frames_tensors = torch.stack([preprocess(frame) for frame in frames])\n",
    "        return frames_tensors\n",
    "\n",
    "    def forward(self, video_path: str, translation_df: pd.DataFrame = None) -> str:\n",
    "        # Extraemos los frames del video y los convertimos a tensores\n",
    "        frames = self.extract_frames(video_path)\n",
    "        \n",
    "        # Pasamos los frames por el modelo ViT para obtener embeddings visuales\n",
    "        with torch.no_grad():\n",
    "            visual_embeddings = self.vit(frames).last_hidden_state\n",
    "\n",
    "        # Usamos la representación visual como contexto para el modelo GPT\n",
    "        # Ajustamos la forma del input para que el GPT lo pueda manejar\n",
    "        gpt_input = visual_embeddings.view(visual_embeddings.size(0), -1)\n",
    "\n",
    "        # Generamos secuencia de tokens de texto con GPT\n",
    "        generated_tokens = self.gpt.generate(inputs_embeds=gpt_input, pad_token_id=self.tokenizer.pad_token_id)\n",
    "\n",
    "        # Decodificamos los tokens generados a texto\n",
    "        predicted_text = self.tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Si se proporciona el dataframe de traducción, obtenemos la traducción correcta\n",
    "        if translation_df is not None:\n",
    "            video_name = video_path.split('/')[-1].removeprefix('filtered_').removesuffix('.mp4')\n",
    "            true_translation = translation_df.get_group(video_name)['SENTENCE'].str.cat(sep=' ')\n",
    "            print(f\"Traducción correcta: {true_translation}\")\n",
    "        \n",
    "        # Retornamos la predicción del texto\n",
    "        return predicted_text\n",
    "\n",
    "# Inicializamos el modelo\n",
    "video_to_text_model = VideoToTextModel(model, gpt, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = video_to_text_model.extract_frames(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">8. Fine - Tune</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(preds, references):\n",
    "    smoothie = SmoothingFunction().method4  # Smoothing method to avoid zero scores for short sentences\n",
    "    # Calcula el BLEU score para cada predicción en el batch\n",
    "    scores = []\n",
    "    for pred, ref in zip(preds, references):\n",
    "        score = sentence_bleu([ref], pred, smoothing_function=smoothie)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Function to convert token ids back to words (predictions and labels)\n",
    "def decode_predictions(predictions, tokenizer):\n",
    "    decoded_preds = []\n",
    "    for pred in predictions:\n",
    "        # Convert token IDs to tokens (words) using the tokenizer's decode method\n",
    "        decoded = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "        decoded_preds.append(decoded)\n",
    "    return decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: torch.nn.Module, \n",
    "        train_loader: DataLoader, \n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        criterion: torch.nn.Module, \n",
    "        device: torch.device, \n",
    "        epoch: int):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "\n",
    "    for batch_idx, (inputs, mask, labels) in enumerate(train_loader):\n",
    "        inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # (batch_size, seq_length, vocab_size)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # Backward pass y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Cálculo de métricas\n",
    "        running_loss += loss.item()\n",
    "        total += labels.numel()\n",
    "        _, predicted = torch.max(outputs.view(-1, outputs.size(-1)), 1)\n",
    "        correct += (predicted == labels.view(-1)).sum().item()\n",
    "\n",
    "        # Decodificación para BLEU\n",
    "        all_preds.extend(predicted.tolist())\n",
    "        all_refs.extend(labels.view(-1).tolist())\n",
    "        \n",
    "        # Logs en consola\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Logs en W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"batch_idx\": batch_idx,\n",
    "            \"loss\": loss.item(),\n",
    "            \"accuracy_batch\": 100 * correct / total\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    # Calcular BLEU\n",
    "    train_bleu = calculate_bleu(all_preds, all_refs)\n",
    "\n",
    "    print(f\"Epoch [{epoch}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    return epoch_loss, train_bleu\n",
    "\n",
    "def evaluate(\n",
    "        model: torch.nn.Module, \n",
    "        val_loader: DataLoader, \n",
    "        criterion: torch.nn.Module, \n",
    "        device: torch.device, \n",
    "        epoch: int):\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, mask, labels) in enumerate(val_loader):\n",
    "            inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            \n",
    "            # Cálculo de métricas\n",
    "            running_loss += loss.item()\n",
    "            total += labels.numel()\n",
    "            _, predicted = torch.max(outputs.view(-1, outputs.size(-1)), 1)\n",
    "            correct += (predicted == labels.view(-1)).sum().item()\n",
    "\n",
    "            # Decodificación para BLEU\n",
    "            all_preds.extend(predicted.tolist())\n",
    "            all_refs.extend(labels.view(-1).tolist())\n",
    "            \n",
    "            # Logs en consola\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Eval Step [{batch_idx}/{len(val_loader)}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            # Logs en W&B\n",
    "            wandb.log({\n",
    "                \"eval_batch_idx\": batch_idx,\n",
    "                \"eval_loss\": loss.item(),\n",
    "                \"eval_accuracy_batch\": 100 * correct / total\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    # Calcular BLEU\n",
    "    valid_bleu = calculate_bleu(all_preds, all_refs)\n",
    "\n",
    "    print(f\"Validation Epoch [{epoch}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    return epoch_loss, valid_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de variables para seguimiento del mejor modelo\n",
    "best_valid_loss = float('inf')\n",
    "best_bleu_score = 0\n",
    "\n",
    "# Listas para almacenar loss y BLEU de cada epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_bleu_scores = []\n",
    "valid_bleu_scores = []\n",
    "\n",
    "# Configuración de hiperparámetros\n",
    "wandb.config = {\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "}\n",
    "\n",
    "# Ciclo de entrenamiento y evaluación por cada epoch\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    train_loss, train_bleu = train(model, train_dataloader, optimizer, cross_entropy, device, epoch)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    valid_loss, valid_bleu = evaluate(model, val_dataloader, cross_entropy, device, epoch)\n",
    "\n",
    "    # Guardar el mejor modelo basado en la validación de loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"Modelo guardado en epoch {epoch + 1} con loss de validación {valid_loss:.4f}\")\n",
    "\n",
    "    # Guardar loss y BLEU para entrenamiento y validación\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_bleu_scores.append(train_bleu)\n",
    "    valid_bleu_scores.append(valid_bleu)\n",
    "\n",
    "    # Mostrar estadísticas de entrenamiento y validación\n",
    "    print(f\"Training Loss: {train_loss:.4f} | Training BLEU: {train_bleu:.4f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:.4f} | Validation BLEU: {valid_bleu:.4f}\")\n",
    "    \n",
    "    # Registros de la época en W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_bleu\": train_bleu,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"valid_bleu\": valid_bleu\n",
    "    })\n",
    "\n",
    "# Finaliza el seguimiento de W&B\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:24:47.832686Z",
     "iopub.status.busy": "2021-05-20T12:24:47.832372Z",
     "iopub.status.idle": "2021-05-20T12:24:48.147778Z",
     "shell.execute_reply": "2021-05-20T12:24:48.147016Z",
     "shell.execute_reply.started": "2021-05-20T12:24:47.832657Z"
    }
   },
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'best_model.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">9. Make Predictions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:25:07.436008Z",
     "iopub.status.busy": "2021-05-20T12:25:07.435685Z",
     "iopub.status.idle": "2021-05-20T12:25:08.019816Z",
     "shell.execute_reply": "2021-05-20T12:25:08.018976Z",
     "shell.execute_reply.started": "2021-05-20T12:25:07.435979Z"
    }
   },
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:25:23.838719Z",
     "iopub.status.busy": "2021-05-20T12:25:23.838389Z",
     "iopub.status.idle": "2021-05-20T12:25:23.852193Z",
     "shell.execute_reply": "2021-05-20T12:25:23.850795Z",
     "shell.execute_reply.started": "2021-05-20T12:25:23.838691Z"
    }
   },
   "outputs": [],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 863500,
     "sourceId": 1471804,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30096,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
