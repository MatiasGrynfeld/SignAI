{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">1. Import Required Libraries & Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, BertTokenizer\n",
    "\n",
    "#specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Pytorch is using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.device(0), torch.cuda.get_device_name(0), device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\", usecols=[\"translation\", \"id\"])\n",
    "#df_videos = pd.read_csv(\"C:/Users/48519558/Desktop/SignAI-ML/AI-Module/Resources/Datasets/how2sign.csv\", usecols=[\"translation\", \"id\"])\n",
    "#df_videos = pd.read_csv(\"D:/SignAI-ML/AI-Module/Resources/Datasets/how2sign.csv\", usecols=[\"translation\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_videos = pd.read_csv(\"D:/Sprinfil/Dataset/how2sign.csv\")\n",
    "# df_videos = df_videos.sort_values([\"id\"]).reset_index().drop([\"index\"], axis=1)\n",
    "# df_videos.head()\n",
    "# import ast\n",
    "# points = df_videos[\"points\"]\n",
    "# for index, point in enumerate(points):\n",
    "#     print(index)\n",
    "#     points.at[index] = ast.literal_eval(point)\n",
    "# type(points.iloc[0])\n",
    "# df_videos[\"points\"] = points\n",
    "# df_videos.head()\n",
    "# for index, item in enumerate(points):\n",
    "#     item = np.array(item)\n",
    "#     print(index, item.shape)\n",
    "#     item_with_index = np.array((item, index), dtype=object)\n",
    "#     points.iloc[index]=item\n",
    "#     np.save(f\"{index}.npy\", item_with_index)\n",
    "# df_videos[\"points\"] = points\n",
    "# df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puntos_folder = \"/app/AI-Module/Resources/Datasets/Points\"\n",
    "#puntos_folder = \"C:/Users/48519558/Desktop/SignAI-ML/AI-Module/Resources/Datasets/Points\"\n",
    "#puntos_folder = \"D:/SignAI-ML/AI-Module/Resources/Datasets/Points\"\n",
    "files = [puntos_folder + \"/\" + file for file in os.listdir(puntos_folder)]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(files):\n",
    "    for file in files:\n",
    "        item_with_index = np.load(file, allow_pickle=True)\n",
    "        item = item_with_index[0].astype(np.float16)\n",
    "        index = item_with_index[1]\n",
    "        yield item, index\n",
    "\n",
    "puntos_list = []\n",
    "ids_list = []\n",
    "\n",
    "for item, index in load_points(files):\n",
    "    puntos_list.append(item)\n",
    "    ids_list.append(index)\n",
    "\n",
    "df_puntos = pd.DataFrame({\n",
    "    'points': puntos_list,\n",
    "    'id': ids_list\n",
    "})\n",
    "df_puntos = df_puntos.sort_values([\"id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = df_videos.merge(df_puntos, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = df_videos['points'].apply(lambda x: len(x)).max()\n",
    "print(max_len)\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(max_frames, point_series: pd.Series):\n",
    "    for i in range(len(point_series)):\n",
    "        current_length = len(point_series[i])\n",
    "        if current_length < max_frames:\n",
    "            padding = np.full(\n",
    "                (max_frames - current_length, 2172), \n",
    "                -1,\n",
    "                dtype=np.float16\n",
    "            )\n",
    "            padding[:, 3::4] = 0\n",
    "            point_series[i] = np.concatenate((point_series[i], padding), axis=0)\n",
    "    return point_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos['points'] = add_padding(max_len, df_videos['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_df = df_videos['points'].to_frame()\n",
    "y_df = df_videos['translation'].to_frame()\n",
    "seed = 31991\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=(100/500), random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=(100/400), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(), y_train.head(), X_val.head(), y_val.head(), X_test.head(), y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "globals().pop(\"df_videos\", None)\n",
    "globals().pop(\"puntos_list\", None)\n",
    "globals().pop(\"ids_list\", None)\n",
    "globals().pop(\"df_puntos\", None)\n",
    "globals().pop(\"X_df\", None)\n",
    "globals().pop(\"item\", None)\n",
    "globals().pop(\"y_df\", None)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">3. Import Bert - base- uncased</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T11:49:24.18724Z",
     "iopub.status.busy": "2021-05-20T11:49:24.186902Z",
     "iopub.status.idle": "2021-05-20T11:50:15.137513Z",
     "shell.execute_reply": "2021-05-20T11:50:15.135994Z",
     "shell.execute_reply.started": "2021-05-20T11:49:24.187212Z"
    }
   },
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_label_train = tokenizer.batch_encode_plus(\n",
    "    y_train['translation'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_label_val = tokenizer.batch_encode_plus(\n",
    "    y_val['translation'].tolist(),\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_label_test = tokenizer.batch_encode_plus(\n",
    "    y_test['translation'].tolist(),\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n",
    "\n",
    "<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">5. List to Tensors</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask_from_points(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    # Crear una m치scara inicial llena de unos\n",
    "    mask = torch.ones(tensor.size(), dtype=torch.int8, device=tensor.device)\n",
    "\n",
    "    # Reestructurar el tensor para facilitar la verificaci칩n de condiciones\n",
    "    points = tensor.view(tensor.size(0), tensor.size(1), -1, 4)  # Cambiar la forma para agrupar por 4\n",
    "    conditions = (points[:, :, :, 0] == -1) & (points[:, :, :, 1] == -1) & (points[:, :, :, 2] == -1) & (points[:, :, :, 3] == 0)\n",
    "\n",
    "    # Aplicar la condici칩n directamente a la m치scara\n",
    "    mask.view(tensor.size(0), tensor.size(1), -1, 4)[conditions] = 0\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:16:52.707444Z",
     "iopub.status.busy": "2021-05-20T12:16:52.707085Z",
     "iopub.status.idle": "2021-05-20T12:16:52.744645Z",
     "shell.execute_reply": "2021-05-20T12:16:52.743449Z",
     "shell.execute_reply.started": "2021-05-20T12:16:52.707415Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert lists to tensors\n",
    "gc.collect()\n",
    "train_np = np.array(X_train['points'].tolist(), dtype=np.float16)\n",
    "train_seq = torch.tensor(train_np, dtype=torch.float16)\n",
    "gc.collect()\n",
    "train_mask = create_attention_mask_from_points(train_seq)\n",
    "train_y = tokens_label_train['input_ids']\n",
    "globals().pop(\"X_train\", None)\n",
    "globals().pop(\"train_np\", None)\n",
    "globals().pop(\"y_train\", None)\n",
    "globals().pop(\"tokens_label_train\", None)\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "val_np = np.array(X_val['points'].tolist(), dtype=np.float16)\n",
    "val_seq = torch.tensor(val_np, dtype=torch.float16)\n",
    "gc.collect()\n",
    "val_mask = create_attention_mask_from_points(val_seq)\n",
    "val_y = tokens_label_val['input_ids']\n",
    "globals().pop(\"X_val\", None)\n",
    "globals().pop(\"val_np\", None)\n",
    "globals().pop(\"y_val\", None)\n",
    "globals().pop(\"tokens_label_val\", None)\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "test_np = np.array(X_test['points'].tolist(), dtype=np.float16)\n",
    "test_seq = torch.tensor(test_np, dtype=torch.float16)\n",
    "gc.collect()\n",
    "test_mask = create_attention_mask_from_points(test_seq)\n",
    "test_y = tokens_label_test['input_ids']\n",
    "globals().pop(\"X_test\", None)\n",
    "globals().pop(\"test_np\", None)\n",
    "globals().pop(\"y_test\", None)\n",
    "globals().pop(\"tokens_label_test\", None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">6. Data Loader</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:16:54.073069Z",
     "iopub.status.busy": "2021-05-20T12:16:54.072724Z",
     "iopub.status.idle": "2021-05-20T12:16:54.079387Z",
     "shell.execute_reply": "2021-05-20T12:16:54.078289Z",
     "shell.execute_reply.started": "2021-05-20T12:16:54.073019Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "globals().pop(\"train_seq\", None)\n",
    "globals().pop(\"train_mask\", None)\n",
    "globals().pop(\"train_y\", None)\n",
    "gc.collect()\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "globals().pop(\"train_data\", None)\n",
    "globals().pop(\"train_sampler\", None)\n",
    "gc.collect()\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "globals().pop(\"val_seq\", None)\n",
    "globals().pop(\"val_mask\", None)\n",
    "globals().pop(\"val_y\", None)\n",
    "gc.collect()\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "globals().pop(\"val_data\", None)\n",
    "globals().pop(\"val_sampler\", None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">7. Model Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#float(\"sfgldiubjknstrgflbjhk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "class FloatToTextModel(nn.Module):\n",
    "    def __init__(self, model_name=\"t5-small\"):\n",
    "        super(FloatToTextModel, self).__init__()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def float_to_text(self, float_sequence):\n",
    "        \"\"\"Convierte una secuencia de floats a texto.\"\"\"\n",
    "        return \" \".join(map(str, float_sequence))\n",
    "\n",
    "    def forward(self, float_sequence, labels=None):\n",
    "        \"\"\"Propaga la entrada a trav칠s del modelo.\n",
    "        \n",
    "        Args:\n",
    "            float_sequence: Tensor de entrada con forma (batch_size, 2537, 2172).\n",
    "            labels: (Opcional) IDs de tokens de la salida esperada para calcular la p칠rdida.\n",
    "\n",
    "        Returns:\n",
    "            Salida del modelo, que puede incluir la p칠rdida si se proporcionan etiquetas.\n",
    "        \"\"\"\n",
    "        # Asegurarse de que float_sequence tenga la forma correcta\n",
    "        batch_size = float_sequence.shape[0]\n",
    "        \n",
    "        # Procesar cada elemento del batch de manera independiente\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            # Extraer el primer elemento de la segunda dimensi칩n\n",
    "            input_seq = float_sequence[i, 0, :]  # Extrae el primer \"vector\" de la dimensi칩n 1\n",
    "            input_text = self.float_to_text(input_seq.numpy())  # Convertir a texto\n",
    "\n",
    "            # Tokenizar el texto de entrada\n",
    "            input_ids = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "            \n",
    "            # Propagar a trav칠s del modelo\n",
    "            output = self.model(input_ids=input_ids, labels=labels[i].unsqueeze(0) if labels is not None else None)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return outputs  # Devuelve una lista de salidas para cada elemento del batch\n",
    "\n",
    "    def generate(self, float_sequence):\n",
    "        \"\"\"Genera texto a partir de una secuencia de floats.\"\"\"\n",
    "        self.eval()  # Cambia a modo evaluaci칩n\n",
    "        batch_size = float_sequence.shape[0]\n",
    "        generated_texts = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            input_seq = float_sequence[i, 0, :]  # Extraer el primer \"vector\" de la dimensi칩n 1\n",
    "            input_text = self.float_to_text(input_seq.numpy())  # Convertir a texto\n",
    "\n",
    "            # Tokenizar el texto de entrada\n",
    "            input_ids = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "\n",
    "            # Generar la salida\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(input_ids)\n",
    "            \n",
    "            # Decodificar los IDs de salida a texto\n",
    "            output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated_texts.append(output_text)\n",
    "\n",
    "        return generated_texts  # Devuelve una lista de textos generados para cada elemento del batch\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    model = FloatToTextModel()\n",
    "    float_sequence = torch.randn((2, 2537, 2172))  # Ejemplo de batch_size = 2\n",
    "\n",
    "    # Generar texto\n",
    "    generated_texts = model.generate(float_sequence)\n",
    "    for idx, text in enumerate(generated_texts):\n",
    "        print(f\"Texto generado para el batch {idx}: {text}\")\n",
    "\n",
    "    # Ejemplo de forward con etiquetas (para entrenamiento)\n",
    "    labels = model.tokenizer(\"hola mundo\", return_tensors=\"pt\").input_ids.unsqueeze(0).repeat(2, 1)  # Ajustar etiquetas al tama침o del batch\n",
    "    outputs = model(float_sequence, labels=labels)\n",
    "    for idx, output in enumerate(outputs):\n",
    "        print(f\"P칠rdida para el batch {idx}: {output.loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">8. Fine - Tune</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(preds, references):\n",
    "    smoothie = SmoothingFunction().method4  # Smoothing method to avoid zero scores for short sentences\n",
    "    # Calcula el BLEU score para cada predicci칩n en el batch\n",
    "    scores = []\n",
    "    for pred, ref in zip(preds, references):\n",
    "        score = sentence_bleu([ref], pred, smoothing_function=smoothie)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Function to convert token ids back to words (predictions and labels)\n",
    "def decode_predictions(predictions, tokenizer):\n",
    "    decoded_preds = []\n",
    "    for pred in predictions:\n",
    "        # Convert token IDs to tokens (words) using the tokenizer's decode method\n",
    "        decoded = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "        decoded_preds.append(decoded)\n",
    "    return decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: torch.nn.Module, \n",
    "        train_loader: DataLoader, \n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        criterion: torch.nn.Module, \n",
    "        device: torch.device, \n",
    "        epoch: int):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "\n",
    "    for batch_idx, (inputs, mask, labels) in enumerate(train_loader):\n",
    "        inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # (batch_size, seq_length, vocab_size)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # Backward pass y optimizaci칩n\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # C치lculo de m칠tricas\n",
    "        running_loss += loss.item()\n",
    "        total += labels.numel()\n",
    "        _, predicted = torch.max(outputs.view(-1, outputs.size(-1)), 1)\n",
    "        correct += (predicted == labels.view(-1)).sum().item()\n",
    "\n",
    "        # Decodificaci칩n para BLEU\n",
    "        all_preds.extend(predicted.tolist())\n",
    "        all_refs.extend(labels.view(-1).tolist())\n",
    "        \n",
    "        # Logs en consola\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Logs en W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"batch_idx\": batch_idx,\n",
    "            \"loss\": loss.item(),\n",
    "            \"accuracy_batch\": 100 * correct / total\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    # Calcular BLEU\n",
    "    train_bleu = calculate_bleu(all_preds, all_refs)\n",
    "\n",
    "    print(f\"Epoch [{epoch}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    return epoch_loss, train_bleu\n",
    "\n",
    "def evaluate(\n",
    "        model: torch.nn.Module, \n",
    "        val_loader: DataLoader, \n",
    "        criterion: torch.nn.Module, \n",
    "        device: torch.device, \n",
    "        epoch: int):\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, mask, labels) in enumerate(val_loader):\n",
    "            inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            \n",
    "            # C치lculo de m칠tricas\n",
    "            running_loss += loss.item()\n",
    "            total += labels.numel()\n",
    "            _, predicted = torch.max(outputs.view(-1, outputs.size(-1)), 1)\n",
    "            correct += (predicted == labels.view(-1)).sum().item()\n",
    "\n",
    "            # Decodificaci칩n para BLEU\n",
    "            all_preds.extend(predicted.tolist())\n",
    "            all_refs.extend(labels.view(-1).tolist())\n",
    "            \n",
    "            # Logs en consola\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Eval Step [{batch_idx}/{len(val_loader)}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            # Logs en W&B\n",
    "            wandb.log({\n",
    "                \"eval_batch_idx\": batch_idx,\n",
    "                \"eval_loss\": loss.item(),\n",
    "                \"eval_accuracy_batch\": 100 * correct / total\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    # Calcular BLEU\n",
    "    valid_bleu = calculate_bleu(all_preds, all_refs)\n",
    "\n",
    "    print(f\"Validation Epoch [{epoch}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    return epoch_loss, valid_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializaci칩n de variables para seguimiento del mejor modelo\n",
    "best_valid_loss = float('inf')\n",
    "best_bleu_score = 0\n",
    "\n",
    "# Listas para almacenar loss y BLEU de cada epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_bleu_scores = []\n",
    "valid_bleu_scores = []\n",
    "\n",
    "# Configuraci칩n de hiperpar치metros\n",
    "wandb.config = {\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "}\n",
    "\n",
    "# Ciclo de entrenamiento y evaluaci칩n por cada epoch\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    train_loss, train_bleu = train(model, train_dataloader, optimizer, cross_entropy, device, epoch)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    valid_loss, valid_bleu = evaluate(model, val_dataloader, cross_entropy, device, epoch)\n",
    "\n",
    "    # Guardar el mejor modelo basado en la validaci칩n de loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"Modelo guardado en epoch {epoch + 1} con loss de validaci칩n {valid_loss:.4f}\")\n",
    "\n",
    "    # Guardar loss y BLEU para entrenamiento y validaci칩n\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_bleu_scores.append(train_bleu)\n",
    "    valid_bleu_scores.append(valid_bleu)\n",
    "\n",
    "    # Mostrar estad칤sticas de entrenamiento y validaci칩n\n",
    "    print(f\"Training Loss: {train_loss:.4f} | Training BLEU: {train_bleu:.4f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:.4f} | Validation BLEU: {valid_bleu:.4f}\")\n",
    "    \n",
    "    # Registros de la 칠poca en W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_bleu\": train_bleu,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"valid_bleu\": valid_bleu\n",
    "    })\n",
    "\n",
    "# Finaliza el seguimiento de W&B\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:24:47.832686Z",
     "iopub.status.busy": "2021-05-20T12:24:47.832372Z",
     "iopub.status.idle": "2021-05-20T12:24:48.147778Z",
     "shell.execute_reply": "2021-05-20T12:24:48.147016Z",
     "shell.execute_reply.started": "2021-05-20T12:24:47.832657Z"
    }
   },
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'best_model.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">9. Make Predictions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:25:07.436008Z",
     "iopub.status.busy": "2021-05-20T12:25:07.435685Z",
     "iopub.status.idle": "2021-05-20T12:25:08.019816Z",
     "shell.execute_reply": "2021-05-20T12:25:08.018976Z",
     "shell.execute_reply.started": "2021-05-20T12:25:07.435979Z"
    }
   },
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:25:23.838719Z",
     "iopub.status.busy": "2021-05-20T12:25:23.838389Z",
     "iopub.status.idle": "2021-05-20T12:25:23.852193Z",
     "shell.execute_reply": "2021-05-20T12:25:23.850795Z",
     "shell.execute_reply.started": "2021-05-20T12:25:23.838691Z"
    }
   },
   "outputs": [],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 863500,
     "sourceId": 1471804,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30096,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
