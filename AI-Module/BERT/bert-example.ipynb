{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">1. Import Required Libraries & Dataset</h1>"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:43.768047Z","iopub.status.busy":"2021-05-20T12:16:43.7677Z","iopub.status.idle":"2021-05-20T12:16:43.773166Z","shell.execute_reply":"2021-05-20T12:16:43.772221Z","shell.execute_reply.started":"2021-05-20T12:16:43.767999Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import ast\n","import time\n","import datetime\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoModel, BertTokenizer\n","import random\n","\n","#specify GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"markdown","metadata":{},"source":["### Check if Pytorch is using GPU"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["(True,\n"," 1,\n"," 0,\n"," <torch.cuda.device at 0x7febf4c3be50>,\n"," 'NVIDIA RTX 3500 Ada Generation Laptop GPU')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.device(0), torch.cuda.get_device_name(0)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:45.214458Z","iopub.status.busy":"2021-05-20T12:16:45.214126Z","iopub.status.idle":"2021-05-20T12:16:45.244325Z","shell.execute_reply":"2021-05-20T12:16:45.243406Z","shell.execute_reply.started":"2021-05-20T12:16:45.21443Z"},"trusted":true},"outputs":[],"source":["df_videos = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCanceled future for execute_request message before replies were done"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCanceled future for execute_request message before replies were done. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["def obj_to_list(serie: pd.Series) -> pd.Series:\n","    for index, element in serie.items():\n","        print(index)\n","        serie.at[index] = ast.literal_eval(element)\n","    return serie\n","\n","df_videos['points'] = obj_to_list(df_videos['points'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(df_videos[\"points\"].iloc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"/app/AI-Module/Modules\")\n","from VideoFormaterClass import VideoFormater"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vf = VideoFormater()\n","vf.concatAndExportVideos(df_videos, \"/app/AI-Module/Resources/Datasets/how2sign2.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_len = df_videos['points'].apply(lambda x: len(x)).max()\n","df_videos.head(), max_len"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def add_padding(max_frames, pointSeries: pd.Series):\n","    for i in range(len(pointSeries)):\n","        current_length = len(pointSeries[i])\n","        if current_length < max_frames:\n","            padding = np.full(\n","                (max_frames - current_length, 2172), \n","                -1\n","            )\n","            padding[:, 3::4] = 0\n","            pointSeries[i] = np.concatenate((pointSeries[i], padding), axis=0)\n","\n","    return pointSeries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_videos['points'] = add_padding(max_len, df_videos['points'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_df = df_videos['points'].to_frame()\n","y_df = df_videos['translation'].to_frame()\n","seed = 31991\n","X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=(100/500), random_state=seed)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=(100/400), random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.head(), y_train.head(), X_val.head(), y_val.head(), X_test.head(), y_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">3. Import Bert - base- uncased</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T11:49:24.18724Z","iopub.status.busy":"2021-05-20T11:49:24.186902Z","iopub.status.idle":"2021-05-20T11:50:15.137513Z","shell.execute_reply":"2021-05-20T11:50:15.135994Z","shell.execute_reply.started":"2021-05-20T11:49:24.187212Z"},"trusted":true},"outputs":[],"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# tokenize and encode sequences in the training set\n","tokens_label_train = tokenizer.batch_encode_plus(\n","    y_train['translation'].tolist(),\n","    padding = True\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_label_val = tokenizer.batch_encode_plus(\n","    y_val['translation'].tolist(),\n","    padding = True\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_label_test = tokenizer.batch_encode_plus(\n","    y_test['translation'].tolist(),\n","    padding = True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">5. List to Tensors</h1>"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def create_attention_mask_from_points(seq_tensor):\n","    mask = torch.ones(seq_tensor.shape, dtype=torch.long)\n","    missing_data = (seq_tensor[..., :3] == -1).all(dim=-1) & (seq_tensor[..., 3] == 0)\n","    mask[missing_data] = 0\n","    return mask"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:52.707444Z","iopub.status.busy":"2021-05-20T12:16:52.707085Z","iopub.status.idle":"2021-05-20T12:16:52.744645Z","shell.execute_reply":"2021-05-20T12:16:52.743449Z","shell.execute_reply.started":"2021-05-20T12:16:52.707415Z"},"trusted":true},"outputs":[],"source":["# convert lists to tensors\n","\n","train_seq = torch.stack([torch.tensor(seq) for seq in X_train['points'].tolist()])\n","train_mask = create_attention_mask_from_points(train_seq)\n","train_y = tokens_label_train['input_ids']\n","\n","val_seq = torch.stack([torch.tensor(seq) for seq in X_val['points'].tolist()])\n","val_mask = create_attention_mask_from_points(val_seq)\n","val_y = tokens_label_val['input_ids']\n","\n","test_seq = torch.stack([torch.tensor(seq) for seq in X_test['points'].tolist()])\n","test_mask = create_attention_mask_from_points(test_seq)\n","test_y = tokens_label_test['input_ids']"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">6. Data Loader</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:54.073069Z","iopub.status.busy":"2021-05-20T12:16:54.072724Z","iopub.status.idle":"2021-05-20T12:16:54.079387Z","shell.execute_reply":"2021-05-20T12:16:54.078289Z","shell.execute_reply.started":"2021-05-20T12:16:54.073019Z"},"trusted":true},"outputs":[],"source":["\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">7. Model Architecture</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab_size = tokenizer.vocab_size\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert, vocab_size):\n","        super(BERT_Arch, self).__init__()\n","        \n","        self.bert = bert \n","        \n","        # Dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # ReLU activation function\n","        self.relu = nn.ReLU()\n","\n","        # Dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        \n","        # Dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, vocab_size)  # Use tokenizer's vocab size\n","\n","        # Softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, seq_input, mask):\n","        outputs = self.bert(seq_input, attention_mask=mask, return_dict=False)\n","        \n","        # Use the last hidden state for each token (outputs[0])\n","        x = self.fc1(outputs[0])  # (batch_size, sequence_length, 512)\n","\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","\n","        x = self.fc2(x)  # (batch_size, sequence_length, vocab_size)\n","        \n","        # Apply softmax activation (for each token in the sequence)\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_bleu(preds, labels):\n","    # Aquí puedes usar la función BLEU de NLTK o cualquier otra implementación de BLEU\n","    # Convertir las predicciones y los labels a listas de tokens\n","    pred_tokens = torch.argmax(preds, dim=-1).tolist()\n","    label_tokens = labels.tolist()\n","    \n","    # Calcular BLEU usando, por ejemplo, nltk\n","    bleu_score = nltk.translate.bleu_score.sentence_bleu([label_tokens], pred_tokens)\n","    \n","    return bleu_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert, vocab_size)\n","\n","# push the model to GPU\n","model = model.to(device)\n","\n","# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),lr = 1e-5) \n","\n","# Define the loss function\n","cross_entropy = nn.CrossEntropyLoss()\n","\n","# number of training epochs\n","epochs = 10"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">8. Fine - Tune</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def perturb_hyperparameters(hyperparameters):\n","    new_hyperparameters = hyperparameters.copy()\n","    new_hyperparameters['learning_rate'] = hyperparameters['learning_rate'] * random.uniform(0.8, 1.2)\n","    return new_hyperparameters\n","\n","\n","def train(hyperparameters):\n","    model.train()  # Set the model to training mode\n","    total_loss = 0.0\n","    total_bleu = 0.0\n","\n","    # Loop through batches of training data (asegúrate de tener un dataloader)\n","    for batch in train_dataloader:\n","        optimizer.zero_grad()  # Reset gradients\n","        \n","        input_ids, attention_mask, labels = batch  # Extract input and labels\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(input_ids, attention_mask)\n","        loss = cross_entropy(outputs.view(-1, vocab_size), labels.view(-1))\n","        \n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Accumulate loss and BLEU score for the batch\n","        total_loss += loss.item()\n","        bleu_score = compute_bleu(outputs, labels)  # Implementar la función compute_bleu\n","        total_bleu += bleu_score\n","\n","    # Calculate average loss and BLEU score for the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","    avg_bleu = total_bleu / len(train_dataloader)\n","\n","    return avg_loss, avg_bleu\n","\n","\n","def evaluate(hyperparameters):\n","    model.eval()  # Set the model to evaluation mode\n","    total_loss = 0.0\n","    total_bleu = 0.0\n","\n","    with torch.no_grad():\n","        for batch in valid_dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","            \n","            # Forward pass\n","            outputs = model(input_ids, attention_mask)\n","            loss = cross_entropy(outputs.view(-1, vocab_size), labels.view(-1))\n","            \n","            # Accumulate loss and BLEU score\n","            total_loss += loss.item()\n","            bleu_score = compute_bleu(outputs, labels)  # Implementar la función compute_bleu\n","            total_bleu += bleu_score\n","\n","    # Calculate average loss and BLEU score for the validation set\n","    avg_loss = total_loss / len(valid_dataloader)\n","    avg_bleu = total_bleu / len(valid_dataloader)\n","\n","    return avg_loss, avg_bleu\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Configuración inicial\n","best_valid_loss = float('inf')\n","best_bleu_score = 0\n","train_losses, valid_losses = [], []\n","train_bleu_scores, valid_bleu_scores = []\n","\n","# Simulación de Population Based Training con múltiples ensayos\n","num_trials = 8\n","trials = [{'hyperparameters': hyperparameters.copy(), 'state': None} for _ in range(num_trials)]\n","\n","for epoch in range(epochs):\n","    \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    for trial_id, trial in enumerate(trials):\n","        # Restaurar el estado del modelo si existe (evitando reentrenamiento desde cero)\n","        if trial['state']:\n","            model.load_state_dict(trial['state'])\n","        \n","        # Entrenar y evaluar el modelo con los hiperparámetros actuales\n","        train_loss, train_bleu = train(trial['hyperparameters'])\n","        valid_loss, valid_bleu = evaluate(trial['hyperparameters'])\n","        \n","        # Si la pérdida de validación mejora, actualizar el mejor modelo\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            best_bleu_score = valid_bleu\n","            torch.save(model.state_dict(), 'saved_weights.pt')\n","        \n","        # Guardar estado y resultados\n","        trial['state'] = model.state_dict().copy()\n","        train_losses.append(train_loss)\n","        valid_losses.append(valid_loss)\n","        train_bleu_scores.append(train_bleu)\n","        valid_bleu_scores.append(valid_bleu)\n","        \n","        # Perturbar los hiperparámetros en los ensayos de bajo rendimiento\n","        if valid_loss > best_valid_loss * 1.1:\n","            print(f'\\nTrial {trial_id} underperforming, copying weights from better performing trials...')\n","            good_trial = random.choice([t for t in trials if t['hyperparameters'] != trial['hyperparameters']])\n","            trial['state'] = good_trial['state']\n","            trial['hyperparameters'] = perturb_hyperparameters(good_trial['hyperparameters'])\n","        \n","        # Imprimir estadísticas de entrenamiento\n","        print(f'Trial {trial_id} | Training Loss: {train_loss:.3f} | Training BLEU: {train_bleu:.3f}')\n","        print(f'Validation Loss: {valid_loss:.3f} | Validation BLEU: {valid_bleu:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:24:47.832686Z","iopub.status.busy":"2021-05-20T12:24:47.832372Z","iopub.status.idle":"2021-05-20T12:24:48.147778Z","shell.execute_reply":"2021-05-20T12:24:48.147016Z","shell.execute_reply.started":"2021-05-20T12:24:47.832657Z"},"trusted":true},"outputs":[],"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">9. Make Predictions</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:07.436008Z","iopub.status.busy":"2021-05-20T12:25:07.435685Z","iopub.status.idle":"2021-05-20T12:25:08.019816Z","shell.execute_reply":"2021-05-20T12:25:08.018976Z","shell.execute_reply.started":"2021-05-20T12:25:07.435979Z"},"trusted":true},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:23.838719Z","iopub.status.busy":"2021-05-20T12:25:23.838389Z","iopub.status.idle":"2021-05-20T12:25:23.852193Z","shell.execute_reply":"2021-05-20T12:25:23.850795Z","shell.execute_reply.started":"2021-05-20T12:25:23.838691Z"},"trusted":true},"outputs":[],"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":863500,"sourceId":1471804,"sourceType":"datasetVersion"}],"dockerImageVersionId":30096,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
