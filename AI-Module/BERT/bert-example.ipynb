{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">1. Import Required Libraries & Dataset</h1>"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:43.768047Z","iopub.status.busy":"2021-05-20T12:16:43.7677Z","iopub.status.idle":"2021-05-20T12:16:43.773166Z","shell.execute_reply":"2021-05-20T12:16:43.772221Z","shell.execute_reply.started":"2021-05-20T12:16:43.767999Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import ast\n","import time\n","import datetime\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoModel, BertTokenizer\n","\n","#specify GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"markdown","metadata":{},"source":["### Check if Pytorch is using GPU"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["(True,\n"," 1,\n"," 0,\n"," <torch.cuda.device at 0x7f87e6d32050>,\n"," 'NVIDIA RTX 3500 Ada Generation Laptop GPU')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.device(0), torch.cuda.get_device_name(0)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:45.214458Z","iopub.status.busy":"2021-05-20T12:16:45.214126Z","iopub.status.idle":"2021-05-20T12:16:45.244325Z","shell.execute_reply":"2021-05-20T12:16:45.243406Z","shell.execute_reply.started":"2021-05-20T12:16:45.21443Z"},"trusted":true},"outputs":[],"source":["df_videos = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df_videos['points'] = df_videos['points'].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"/app/AI-Module/Modules\")\n","from VideoFormaterClass import VideoFormater"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vf = VideoFormater()\n","vf.concatAndExportVideos(df_videos, \"/app/AI-Module/Resources/Datasets/how2sign2.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_len = df_videos['points'].apply(lambda x: len(x)).max()\n","df_videos.head(), max_len"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def add_padding(max_frames, pointSeries: pd.Series):\n","    for i in range(len(pointSeries)):\n","        current_length = len(pointSeries[i])\n","        if current_length < max_frames:\n","            padding = np.full(\n","                (max_frames - current_length, 2172), \n","                -1\n","            )\n","            padding[:, 3::4] = 0\n","            pointSeries[i] = np.concatenate((pointSeries[i], padding), axis=0)\n","\n","    return pointSeries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_videos['points'] = add_padding(max_len, df_videos['points'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_df = df_videos['points'].to_frame()\n","y_df = df_videos['translation'].to_frame()\n","seed = 31991\n","X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=(100/500), random_state=seed)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=(100/400), random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.head(), y_train.head(), X_val.head(), y_val.head(), X_test.head(), y_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">3. Import Bert - base- uncased</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T11:49:24.18724Z","iopub.status.busy":"2021-05-20T11:49:24.186902Z","iopub.status.idle":"2021-05-20T11:50:15.137513Z","shell.execute_reply":"2021-05-20T11:50:15.135994Z","shell.execute_reply.started":"2021-05-20T11:49:24.187212Z"},"trusted":true},"outputs":[],"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# tokenize and encode sequences in the training set\n","tokens_label_train = tokenizer.batch_encode_plus(\n","    y_train['translation'].tolist(),\n","    padding = True\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_label_val = tokenizer.batch_encode_plus(\n","    y_val['translation'].tolist(),\n","    padding = True\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_label_test = tokenizer.batch_encode_plus(\n","    y_test['translation'].tolist(),\n","    padding = True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">5. List to Tensors</h1>"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def create_attention_mask_from_points(seq_tensor):\n","    mask = torch.ones(seq_tensor.shape, dtype=torch.long)\n","    missing_data = (seq_tensor[..., :3] == -1).all(dim=-1) & (seq_tensor[..., 3] == 0)\n","    mask[missing_data] = 0\n","    return mask"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:52.707444Z","iopub.status.busy":"2021-05-20T12:16:52.707085Z","iopub.status.idle":"2021-05-20T12:16:52.744645Z","shell.execute_reply":"2021-05-20T12:16:52.743449Z","shell.execute_reply.started":"2021-05-20T12:16:52.707415Z"},"trusted":true},"outputs":[],"source":["# convert lists to tensors\n","\n","train_seq = torch.stack([torch.tensor(seq) for seq in X_train['points'].tolist()])\n","train_mask = create_attention_mask_from_points(train_seq)\n","train_y = tokens_label_train['input_ids']\n","\n","val_seq = torch.stack([torch.tensor(seq) for seq in X_val['points'].tolist()])\n","val_mask = create_attention_mask_from_points(val_seq)\n","val_y = tokens_label_val['input_ids']\n","\n","test_seq = torch.stack([torch.tensor(seq) for seq in X_test['points'].tolist()])\n","test_mask = create_attention_mask_from_points(test_seq)\n","test_y = tokens_label_test['input_ids']"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">6. Data Loader</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:54.073069Z","iopub.status.busy":"2021-05-20T12:16:54.072724Z","iopub.status.idle":"2021-05-20T12:16:54.079387Z","shell.execute_reply":"2021-05-20T12:16:54.078289Z","shell.execute_reply.started":"2021-05-20T12:16:54.073019Z"},"trusted":true},"outputs":[],"source":["\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">7. Model Architecture</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab_size = tokenizer.vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:02.994288Z","iopub.status.busy":"2021-05-20T12:23:02.99389Z","iopub.status.idle":"2021-05-20T12:23:03.006243Z","shell.execute_reply":"2021-05-20T12:23:03.005349Z","shell.execute_reply.started":"2021-05-20T12:23:02.994244Z"},"trusted":true},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert, vocab_size):\n","        super(BERT_Arch, self).__init__()\n","        \n","        self.bert = bert \n","        \n","        # Dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # ReLU activation function\n","        self.relu = nn.ReLU()\n","\n","        # Dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        \n","        # Dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, vocab_size)  # Use tokenizer's vocab size\n","\n","        # Softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, seq_input, mask):\n","        outputs = self.bert(seq_input, attention_mask=mask, return_dict=False)\n","        \n","        # Use the last hidden state for each token (outputs[0])\n","        x = self.fc1(outputs[0])  # (batch_size, sequence_length, 512)\n","\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","\n","        x = self.fc2(x)  # (batch_size, sequence_length, vocab_size)\n","        \n","        # Apply softmax activation (for each token in the sequence)\n","        x = self.softmax(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:03.953226Z","iopub.status.busy":"2021-05-20T12:23:03.952902Z","iopub.status.idle":"2021-05-20T12:23:03.968064Z","shell.execute_reply":"2021-05-20T12:23:03.967316Z","shell.execute_reply.started":"2021-05-20T12:23:03.953198Z"},"trusted":true},"outputs":[],"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert, vocab_size)\n","\n","# push the model to GPU\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:04.903784Z","iopub.status.busy":"2021-05-20T12:23:04.903449Z","iopub.status.idle":"2021-05-20T12:23:04.910844Z","shell.execute_reply":"2021-05-20T12:23:04.909852Z","shell.execute_reply.started":"2021-05-20T12:23:04.90375Z"},"trusted":true},"outputs":[],"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),lr = 1e-5) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:06.96248Z","iopub.status.busy":"2021-05-20T12:23:06.96215Z","iopub.status.idle":"2021-05-20T12:23:06.967108Z","shell.execute_reply":"2021-05-20T12:23:06.966225Z","shell.execute_reply.started":"2021-05-20T12:23:06.962452Z"},"trusted":true},"outputs":[],"source":["# Define the loss function\n","cross_entropy = nn.CrossEntropyLoss()\n","\n","# number of training epochs\n","epochs = 10"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">8. Fine - Tune</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:08.233269Z","iopub.status.busy":"2021-05-20T12:23:08.232922Z","iopub.status.idle":"2021-05-20T12:23:08.241591Z","shell.execute_reply":"2021-05-20T12:23:08.240484Z","shell.execute_reply.started":"2021-05-20T12:23:08.233239Z"},"trusted":true},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","# Define the BLEU score function for a single prediction and reference\n","def calculate_bleu(preds, references):\n","    smoothie = SmoothingFunction().method4  # Smoothing method to avoid zero scores for short sentences\n","    score = sentence_bleu([references], preds, smoothing_function=smoothie)\n","    return score\n","\n","# Function to convert token ids back to words (predictions and labels)\n","def decode_predictions(predictions, tokenizer):\n","    # Convert token IDs to tokens (words) using the tokenizer's decode method\n","    return [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n","\n","def train():\n","    \n","    model.train()\n","    total_loss = 0\n","    total_preds = []\n","    total_bleu_score = 0  # To accumulate BLEU scores\n","    \n","    # iterate over batches\n","    for step, batch in enumerate(train_dataloader):\n","        \n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print(f'  Batch {step:>5,} of {len(train_dataloader):>5,}.')\n","\n","        # push the batch to GPU\n","        batch = [r.to(device) for r in batch]\n","\n","        sent_id, mask, labels = batch\n","        \n","        # clear previously calculated gradients \n","        model.zero_grad()        \n","\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss += loss.item()\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the gradients to 1.0 to prevent exploding gradients\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # move model predictions to CPU for further processing\n","        preds = preds.detach().cpu().numpy()\n","        \n","        # append predictions for final concatenation (reshaping later)\n","        total_preds.append(preds)\n","\n","        # Decode predictions and labels to human-readable sequences\n","        decoded_preds = decode_predictions(preds.argmax(axis=-1), tokenizer)\n","        decoded_labels = decode_predictions(labels.cpu().numpy(), tokenizer)\n","\n","        # Calculate BLEU score for this batch (by averaging the scores for each sentence)\n","        batch_bleu_score = sum([calculate_bleu(pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)]) / len(decoded_preds)\n","        total_bleu_score += batch_bleu_score\n","\n","    # Average loss for this epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","\n","    # Concatenate all predictions\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    # Average BLEU score for this epoch\n","    avg_bleu_score = total_bleu_score / len(train_dataloader)\n","\n","    # return the average loss and average BLEU score\n","    return avg_loss, avg_bleu_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:09.063944Z","iopub.status.busy":"2021-05-20T12:23:09.063641Z","iopub.status.idle":"2021-05-20T12:23:09.073677Z","shell.execute_reply":"2021-05-20T12:23:09.070943Z","shell.execute_reply.started":"2021-05-20T12:23:09.063916Z"},"trusted":true},"outputs":[],"source":["def evaluate():\n","    print(\"\\nEvaluating...\")\n","\n","    # Set the model to evaluation mode (deactivates dropout)\n","    model.eval()\n","\n","    total_loss, total_bleu_score = 0, 0\n","    total_preds = []\n","\n","    # Start time for progress update\n","    t0 = time.time()\n","\n","    # Iterate over batches\n","    for step, batch in enumerate(val_dataloader):\n","        \n","        # Progress update every 50 batches\n","        if step % 50 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print(f'  Batch {step:>5,}  of  {len(val_dataloader):>5,}.    Elapsed: {elapsed}.')\n","        \n","        # Push the batch to GPU\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # Deactivate autograd for evaluation\n","        with torch.no_grad():\n","            # Model predictions\n","            preds = model(sent_id, mask)\n","\n","            # Compute the validation loss\n","            loss = cross_entropy(preds, labels)\n","            total_loss += loss.item()\n","\n","            # Move predictions to CPU\n","            preds = preds.detach().cpu().numpy()\n","\n","            # Store predictions\n","            total_preds.append(preds)\n","\n","            # Decode predictions and labels\n","            decoded_preds = decode_predictions(preds.argmax(axis=-1), tokenizer)\n","            decoded_labels = decode_predictions(labels.cpu().numpy(), tokenizer)\n","\n","            # Calculate BLEU score for the batch\n","            batch_bleu_score = sum([calculate_bleu(pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)]) / len(decoded_preds)\n","            total_bleu_score += batch_bleu_score\n","\n","    # Compute the average validation loss\n","    avg_loss = total_loss / len(val_dataloader)\n","\n","    # Compute the average BLEU score\n","    avg_bleu_score = total_bleu_score / len(val_dataloader)\n","\n","    # Concatenate all predictions\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    # Return the average loss and BLEU score\n","    return avg_loss, avg_bleu_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:09.897241Z","iopub.status.busy":"2021-05-20T12:23:09.896772Z","iopub.status.idle":"2021-05-20T12:24:20.56839Z","shell.execute_reply":"2021-05-20T12:24:20.567287Z","shell.execute_reply.started":"2021-05-20T12:23:09.897201Z"},"trusted":true},"outputs":[],"source":["# set initial loss and BLEU score to infinite/zero\n","best_valid_loss = float('inf')\n","best_bleu_score = 0\n","\n","# empty lists to store training and validation loss and BLEU score of each epoch\n","train_losses = []\n","valid_losses = []\n","train_bleu_scores = []\n","valid_bleu_scores = []\n","\n","# for each epoch\n","for epoch in range(epochs):\n","    \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    # train model\n","    train_loss, train_bleu = train()\n","    \n","    # evaluate model\n","    valid_loss, valid_bleu = evaluate()\n","    \n","    # save the best model based on validation loss\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","    # Optionally: Save the model based on BLEU score\n","    # if valid_bleu > best_bleu_score:\n","    #     best_bleu_score = valid_bleu\n","    #     torch.save(model.state_dict(), 'saved_weights_best_bleu.pt')\n","    \n","    # append training and validation loss and BLEU score\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    train_bleu_scores.append(train_bleu)\n","    valid_bleu_scores.append(valid_bleu)\n","    \n","    # print training and validation statistics\n","    print(f'\\nTraining Loss: {train_loss:.3f} | Training BLEU: {train_bleu:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f} | Validation BLEU: {valid_bleu:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:24:47.832686Z","iopub.status.busy":"2021-05-20T12:24:47.832372Z","iopub.status.idle":"2021-05-20T12:24:48.147778Z","shell.execute_reply":"2021-05-20T12:24:48.147016Z","shell.execute_reply.started":"2021-05-20T12:24:47.832657Z"},"trusted":true},"outputs":[],"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">9. Make Predictions</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:07.436008Z","iopub.status.busy":"2021-05-20T12:25:07.435685Z","iopub.status.idle":"2021-05-20T12:25:08.019816Z","shell.execute_reply":"2021-05-20T12:25:08.018976Z","shell.execute_reply.started":"2021-05-20T12:25:07.435979Z"},"trusted":true},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:23.838719Z","iopub.status.busy":"2021-05-20T12:25:23.838389Z","iopub.status.idle":"2021-05-20T12:25:23.852193Z","shell.execute_reply":"2021-05-20T12:25:23.850795Z","shell.execute_reply.started":"2021-05-20T12:25:23.838691Z"},"trusted":true},"outputs":[],"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":863500,"sourceId":1471804,"sourceType":"datasetVersion"}],"dockerImageVersionId":30096,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
