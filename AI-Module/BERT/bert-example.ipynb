{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">1. Import Required Libraries & Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoModel, BertTokenizer\n",
    "# import random\n",
    "\n",
    "# #specify GPU\n",
    "# device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Pytorch is using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.device(0), torch.cuda.get_device_name(0), device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_videos = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")\n",
    "#df_videos = pd.read_csv(\"C:/Users/48519558/Desktop/SignAI-ML/AI-Module/Resources/Datasets/how2sign.csv\")\n",
    "df_videos = pd.read_csv(\"D:/Sprinfil/Dataset/how2sign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>We did some exercises today that are going to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>And now we're going to make a, it's called a, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>It should really spring back and feel dry. Thi...</td>\n",
       "      <td>2</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...</td>\n",
       "      <td>So let's begin with the basics. As you can see...</td>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....</td>\n",
       "      <td>Hi! A lot of us has started it into the journe...</td>\n",
       "      <td>4</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              points  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "3  [[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...   \n",
       "4  [[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....   \n",
       "\n",
       "                                         translation  id  len_keyframes  \n",
       "0  We did some exercises today that are going to ...   0            427  \n",
       "1  And now we're going to make a, it's called a, ...   1            282  \n",
       "2  It should really spring back and feel dry. Thi...   2            335  \n",
       "3  So let's begin with the basics. As you can see...   3            492  \n",
       "4  Hi! A lot of us has started it into the journe...   4            334  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos = df_videos.sort_values([\"id\"]).reset_index().drop([\"index\"], axis=1)\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "points = df_videos[\"points\"]\n",
    "for index, point in enumerate(points):\n",
    "    print(index)\n",
    "    points.at[index] = ast.literal_eval(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(points.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>We did some exercises today that are going to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>And now we're going to make a, it's called a, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>It should really spring back and feel dry. Thi...</td>\n",
       "      <td>2</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...</td>\n",
       "      <td>So let's begin with the basics. As you can see...</td>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....</td>\n",
       "      <td>Hi! A lot of us has started it into the journe...</td>\n",
       "      <td>4</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              points  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "3  [[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...   \n",
       "4  [[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....   \n",
       "\n",
       "                                         translation  id  len_keyframes  \n",
       "0  We did some exercises today that are going to ...   0            427  \n",
       "1  And now we're going to make a, it's called a, ...   1            282  \n",
       "2  It should really spring back and feel dry. Thi...   2            335  \n",
       "3  So let's begin with the basics. As you can see...   3            492  \n",
       "4  Hi! A lot of us has started it into the journe...   4            334  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos[\"points\"] = points\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488 (227, 2172)\n",
      "489 (330, 2172)\n",
      "490 (383, 2172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n",
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491 (357, 2172)\n",
      "492 (447, 2172)\n",
      "493 (380, 2172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n",
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n",
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n",
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494 (337, 2172)\n",
      "495 (354, 2172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496 (1086, 2172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n",
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497 (574, 2172)\n",
      "498 (459, 2172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n",
      "C:\\Users\\48519558\\AppData\\Local\\Temp\\ipykernel_10844\\100605647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  points.iloc[index]=item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 (321, 2172)\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(points):\n",
    "    item = np.array(item)\n",
    "    print(index, item.shape)\n",
    "    item_with_index = np.array((item, index), dtype=object)\n",
    "    points.iloc[index]=item\n",
    "    np.save(f\"{index}.npy\", item_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>We did some exercises today that are going to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>And now we're going to make a, it's called a, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>It should really spring back and feel dry. Thi...</td>\n",
       "      <td>2</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...</td>\n",
       "      <td>So let's begin with the basics. As you can see...</td>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....</td>\n",
       "      <td>Hi! A lot of us has started it into the journe...</td>\n",
       "      <td>4</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              points  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "3  [[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...   \n",
       "4  [[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....   \n",
       "\n",
       "                                         translation  id  len_keyframes  \n",
       "0  We did some exercises today that are going to ...   0            427  \n",
       "1  And now we're going to make a, it's called a, ...   1            282  \n",
       "2  It should really spring back and feel dry. Thi...   2            335  \n",
       "3  So let's begin with the basics. As you can see...   3            492  \n",
       "4  Hi! A lot of us has started it into the journe...   4            334  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videos[\"points\"] = points\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#puntos_folder = \"/app/AI-Module/Resources/Datasets/Points\"\n",
    "puntos_folder = \"C:/Users/48519558/Desktop/SignAI-ML/AI-Module/Resources/Datasets/Points\"\n",
    "files = [puntos_folder + \"/\" + file for file in os.listdir(puntos_folder)]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "puntos_list = []\n",
    "ids_list = []\n",
    "\n",
    "for file in files:\n",
    "    item_with_index = np.load(file, allow_pickle=True)\n",
    "    \n",
    "    item = item_with_index[0]\n",
    "    index = item_with_index[1]\n",
    "\n",
    "    puntos_list.append(item)\n",
    "    ids_list.append(index)\n",
    "\n",
    "df_puntos = pd.DataFrame({\n",
    "    'points': puntos_list,\n",
    "    'id': ids_list\n",
    "})\n",
    "df_puntos = df_puntos.sort_values([\"id\"]).reset_index().drop([\"index\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...\n",
       " 1    [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...\n",
       " 2    [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...\n",
       " 3    [[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...\n",
       " 4    [[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....\n",
       " Name: points, dtype: object,\n",
       " 0    [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...\n",
       " 1    [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...\n",
       " 2    [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...\n",
       " 3    [[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...\n",
       " 4    [[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....\n",
       " Name: points, dtype: object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_puntos[\"points\"].head(), df_videos[\"points\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos[\"points\"] = df_puntos[\"points\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2537\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>We did some exercises today that are going to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>And now we're going to make a, it's called a, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>It should really spring back and feel dry. Thi...</td>\n",
       "      <td>2</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...</td>\n",
       "      <td>So let's begin with the basics. As you can see...</td>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....</td>\n",
       "      <td>Hi! A lot of us has started it into the journe...</td>\n",
       "      <td>4</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              points  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "3  [[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...   \n",
       "4  [[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....   \n",
       "\n",
       "                                         translation  id  len_keyframes  \n",
       "0  We did some exercises today that are going to ...   0            427  \n",
       "1  And now we're going to make a, it's called a, ...   1            282  \n",
       "2  It should really spring back and feel dry. Thi...   2            335  \n",
       "3  So let's begin with the basics. As you can see...   3            492  \n",
       "4  Hi! A lot of us has started it into the journe...   4            334  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df_videos['points'].apply(lambda x: len(x)).max()\n",
    "print(max_len)\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(max_frames, pointSeries: pd.Series):\n",
    "    for i in range(len(pointSeries)):\n",
    "        current_length = len(pointSeries[i])\n",
    "        if current_length < max_frames:\n",
    "            padding = np.full(\n",
    "                (max_frames - current_length, 2172), \n",
    "                -1\n",
    "            )\n",
    "            padding[:, 3::4] = 0\n",
    "            pointSeries[i] = np.concatenate((pointSeries[i], padding), axis=0)\n",
    "\n",
    "    return pointSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos['points'] = add_padding(max_len, df_videos['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df_videos['points'].to_frame()\n",
    "y_df = df_videos['translation'].to_frame()\n",
    "seed = 31991\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=(100/500), random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=(100/400), random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(), y_train.head(), X_val.head(), y_val.head(), X_test.head(), y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">3. Import Bert - base- uncased</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T11:49:24.18724Z",
     "iopub.status.busy": "2021-05-20T11:49:24.186902Z",
     "iopub.status.idle": "2021-05-20T11:50:15.137513Z",
     "shell.execute_reply": "2021-05-20T11:50:15.135994Z",
     "shell.execute_reply.started": "2021-05-20T11:49:24.187212Z"
    }
   },
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_label_train = tokenizer.batch_encode_plus(\n",
    "    y_train['translation'].tolist(),\n",
    "    padding = True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_label_val = tokenizer.batch_encode_plus(\n",
    "    y_val['translation'].tolist(),\n",
    "    padding = True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_label_test = tokenizer.batch_encode_plus(\n",
    "    y_test['translation'].tolist(),\n",
    "    padding = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n",
    "\n",
    "<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">5. List to Tensors</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask_from_points(seq_tensor):\n",
    "    mask = torch.ones(seq_tensor.shape, dtype=torch.long)\n",
    "    missing_data = (seq_tensor[..., :3] == -1).all(dim=-1) & (seq_tensor[..., 3] == 0)\n",
    "    mask[missing_data] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:16:52.707444Z",
     "iopub.status.busy": "2021-05-20T12:16:52.707085Z",
     "iopub.status.idle": "2021-05-20T12:16:52.744645Z",
     "shell.execute_reply": "2021-05-20T12:16:52.743449Z",
     "shell.execute_reply.started": "2021-05-20T12:16:52.707415Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert lists to tensors\n",
    "\n",
    "train_seq = torch.stack([torch.tensor(seq) for seq in X_train['points'].tolist()])\n",
    "train_mask = create_attention_mask_from_points(train_seq)\n",
    "train_y = tokens_label_train['input_ids']\n",
    "\n",
    "val_seq = torch.stack([torch.tensor(seq) for seq in X_val['points'].tolist()])\n",
    "val_mask = create_attention_mask_from_points(val_seq)\n",
    "val_y = tokens_label_val['input_ids']\n",
    "\n",
    "test_seq = torch.stack([torch.tensor(seq) for seq in X_test['points'].tolist()])\n",
    "test_mask = create_attention_mask_from_points(test_seq)\n",
    "test_y = tokens_label_test['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">6. Data Loader</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:16:54.073069Z",
     "iopub.status.busy": "2021-05-20T12:16:54.072724Z",
     "iopub.status.idle": "2021-05-20T12:16:54.079387Z",
     "shell.execute_reply": "2021-05-20T12:16:54.078289Z",
     "shell.execute_reply.started": "2021-05-20T12:16:54.073019Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">7. Model Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert, vocab_size):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        \n",
    "        # Dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, vocab_size)  # Use tokenizer's vocab size\n",
    "\n",
    "        # Softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, seq_input, mask):\n",
    "        outputs = self.bert(seq_input, attention_mask=mask, return_dict=False)\n",
    "        \n",
    "        # Use the last hidden state for each token (outputs[0])\n",
    "        x = self.fc1(outputs[0])  # (batch_size, sequence_length, 512)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)  # (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        # Apply softmax activation (for each token in the sequence)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert, vocab_size)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5) \n",
    "# Define the loss function\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">8. Fine - Tune</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Define the BLEU score function for a single prediction and reference\n",
    "def calculate_bleu(preds, references):\n",
    "    smoothie = SmoothingFunction().method4  # Smoothing method to avoid zero scores for short sentences\n",
    "    score = sentence_bleu([references], preds, smoothing_function=smoothie)\n",
    "    return score\n",
    "\n",
    "# Function to convert token ids back to words (predictions and labels)\n",
    "def decode_predictions(predictions, tokenizer):\n",
    "    # Convert token IDs to tokens (words) using the tokenizer's decode method\n",
    "    return [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "    total_bleu_score = 0  # To accumulate BLEU scores\n",
    "    \n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(f'  Batch {step:>5,} of {len(train_dataloader):>5,}.')\n",
    "\n",
    "        # push the batch to GPU\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the gradients to 1.0 to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # move model predictions to CPU for further processing\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        \n",
    "        # append predictions for final concatenation (reshaping later)\n",
    "        total_preds.append(preds)\n",
    "\n",
    "        # Decode predictions and labels to human-readable sequences\n",
    "        decoded_preds = decode_predictions(preds.argmax(axis=-1), tokenizer)\n",
    "        decoded_labels = decode_predictions(labels.cpu().numpy(), tokenizer)\n",
    "\n",
    "        # Calculate BLEU score for this batch (by averaging the scores for each sentence)\n",
    "        batch_bleu_score = sum([calculate_bleu(pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)]) / len(decoded_preds)\n",
    "        total_bleu_score += batch_bleu_score\n",
    "\n",
    "    # Average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    # Average BLEU score for this epoch\n",
    "    avg_bleu_score = total_bleu_score / len(train_dataloader)\n",
    "\n",
    "    # return the average loss and average BLEU score\n",
    "    return avg_loss, avg_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # Set the model to evaluation mode (deactivates dropout)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_bleu_score = 0, 0\n",
    "    total_preds = []\n",
    "\n",
    "    # Start time for progress update\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f'  Batch {step:>5,}  of  {len(val_dataloader):>5,}.    Elapsed: {elapsed}.')\n",
    "        \n",
    "        # Push the batch to GPU\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # Deactivate autograd for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # Compute the validation loss\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Move predictions to CPU\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # Store predictions\n",
    "            total_preds.append(preds)\n",
    "\n",
    "            # Decode predictions and labels\n",
    "            decoded_preds = decode_predictions(preds.argmax(axis=-1), tokenizer)\n",
    "            decoded_labels = decode_predictions(labels.cpu().numpy(), tokenizer)\n",
    "\n",
    "            # Calculate BLEU score for the batch\n",
    "            batch_bleu_score = sum([calculate_bleu(pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)]) / len(decoded_preds)\n",
    "            total_bleu_score += batch_bleu_score\n",
    "\n",
    "    # Compute the average validation loss\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    # Compute the average BLEU score\n",
    "    avg_bleu_score = total_bleu_score / len(val_dataloader)\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    # Return the average loss and BLEU score\n",
    "    return avg_loss, avg_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial loss and BLEU score to infinite/zero\n",
    "best_valid_loss = float('inf')\n",
    "best_bleu_score = 0\n",
    "\n",
    "# empty lists to store training and validation loss and BLEU score of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_bleu_scores = []\n",
    "valid_bleu_scores = []\n",
    "\n",
    "# for each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    # train model\n",
    "    train_loss, train_bleu = train()\n",
    "    \n",
    "    # evaluate model\n",
    "    valid_loss, valid_bleu = evaluate()\n",
    "    \n",
    "    # save the best model based on validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss and BLEU score\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_bleu_scores.append(train_bleu)\n",
    "    valid_bleu_scores.append(valid_bleu)\n",
    "    \n",
    "    # print training and validation statistics\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f} | Training BLEU: {train_bleu:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f} | Validation BLEU: {valid_bleu:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:24:47.832686Z",
     "iopub.status.busy": "2021-05-20T12:24:47.832372Z",
     "iopub.status.idle": "2021-05-20T12:24:48.147778Z",
     "shell.execute_reply": "2021-05-20T12:24:48.147016Z",
     "shell.execute_reply.started": "2021-05-20T12:24:47.832657Z"
    }
   },
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:200%; font-family:cursive; color:white;\">9. Make Predictions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:25:07.436008Z",
     "iopub.status.busy": "2021-05-20T12:25:07.435685Z",
     "iopub.status.idle": "2021-05-20T12:25:08.019816Z",
     "shell.execute_reply": "2021-05-20T12:25:08.018976Z",
     "shell.execute_reply.started": "2021-05-20T12:25:07.435979Z"
    }
   },
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T12:25:23.838719Z",
     "iopub.status.busy": "2021-05-20T12:25:23.838389Z",
     "iopub.status.idle": "2021-05-20T12:25:23.852193Z",
     "shell.execute_reply": "2021-05-20T12:25:23.850795Z",
     "shell.execute_reply.started": "2021-05-20T12:25:23.838691Z"
    }
   },
   "outputs": [],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 863500,
     "sourceId": 1471804,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30096,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
