{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 14:46:09.095562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 14:46:09.253182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 14:46:09.304438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 14:46:09.646187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores entre 0 y 250: 34\n",
      "Valores entre 250 y 400: 224\n",
      "Valores entre 400 y 550: 121\n",
      "Valores entre 550 y 700: 66\n",
      "Valores entre 700 y 1000: 42\n",
      "Valores entre 1000 y 3000: 13\n"
     ]
    }
   ],
   "source": [
    "#dataset[\"len_keyframes\"].max()\n",
    "#dataset[\"len_keyframes\"].mean()\n",
    "#dataset[\"len_keyframes\"].min()\n",
    "limites = [0,250,400,550, 700, 1000,3000]\n",
    "\n",
    "# Crear un DataFrame vacío para almacenar los resultados\n",
    "resultados = []\n",
    "\n",
    "# Recorrer los pares consecutivos de la lista de límites\n",
    "for lower, upper in zip(limites[:-1], limites[1:]):\n",
    "    # Contar cuántos valores están entre 'lower' y 'upper'\n",
    "    count = ((dataset['len_keyframes'] > lower) & (dataset['len_keyframes'] <= upper)).sum()\n",
    "    \n",
    "    # Guardar el resultado en una lista\n",
    "    resultados.append((lower, upper, count))\n",
    "\n",
    "# Mostrar los resultados\n",
    "for lower, upper, count in resultados:\n",
    "    print(f\"Valores entre {lower} y {upper}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_points=pd.read_pickle(\"/app/AI-Module/Resources/Datasets/how2sign_points.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"points\"]=dataframe_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset[\"points\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el formato de points para que sea una lista de 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Agrega la ruta donde está ubicado Points2VecClass.py\n",
    "sys.path.append('/app/AI-Module/Modules')\n",
    "from Points2VecClass import Point2Vec\n",
    "def pointsToCnnInputForm(points):\n",
    "    p2v = Point2Vec(4)\n",
    "    return p2v.CNNMatrix(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cnn_input_form(serie):\n",
    "    for index, element in enumerate(serie):\n",
    "        print(index)\n",
    "        element= pointsToCnnInputForm(element)\n",
    "        serie.at[index]=element\n",
    "    return serie\n",
    "dataset['points']=to_cnn_input_form(dataset['points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar texto de traducciones para que sea compatible con el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide en train y test\n",
    "points_train_val, points_test, translation_train_val, translation_test = train_test_split(\n",
    "    dataset[\"points\", \"len_keyframes\"], dataset[\"translation\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "points_train, points_val, translation_train, translation_val=train_test_split(\n",
    "    points_train_val,translation_train_val, test_size=0.25,random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = pd.DataFrame({\n",
    "    \"points\": points_train[\"points\"],\n",
    "    \"len_keyframes\": points_train[\"len_keyframes\"],\n",
    "    \"translation\": translation_train\n",
    "})\n",
    "\n",
    "val_dataset = pd.DataFrame({\n",
    "    \"points\": points_val,\n",
    "    \"len_keyframes\": points_val[\"len_keyframes\"],\n",
    "    \"translation\": translation_val\n",
    "})\n",
    "\n",
    "test_dataset = pd.DataFrame({\n",
    "    \"points\": points_test,\n",
    "    \"len_keyframes\": points_test[\"len_keyframes\"],\n",
    "    \"translation\": translation_test\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizo las traducciones para que sean compatibles con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Crear el tokenizador\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustar el tokenizador al texto de las traducciones\n",
    "tokenizer.fit_on_texts(dataset['translation'])\n",
    "\n",
    "# Convertir las oraciones en secuencias de enteros\n",
    "dataset['translation_sequence'] = tokenizer.texts_to_sequences(dataset['translation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo datasets de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(df):\n",
    "    points = tf.ragged.constant(df['points'].values)\n",
    "    sequence = tf.ragged.constant(df['translation_sequence'].values)\n",
    "    return tf.data.Dataset.from_tensor_slices((points, sequence))\n",
    "\n",
    "test_dataset = create_tf_dataset(train_dataset)\n",
    "val_dataset=create_tf_dataset(val_dataset)\n",
    "train_dataset= create_tf_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_by_sequence_length(dataset, bucket_values):\n",
    "    # Defino los buckets basados en los límites de longitud\n",
    "    bucket_datasets = []\n",
    "    \n",
    "    for i in range(len(bucket_values) - 1):\n",
    "        lower_bound = bucket_values[i]\n",
    "        upper_bound = bucket_values[i+1]\n",
    "        \n",
    "        # filtro secuencias que caen dentro de los límites de este bucket\n",
    "        bucket = dataset.filter(lambda seq: tf.size(seq) >= lower_bound and tf.size(seq) < upper_bound)\n",
    "        bucket = bucket.shuffle(buffer_size=1000)\n",
    "        bucket=bucket.batch(32)\n",
    "        bucket_datasets.append(bucket)\n",
    "    \n",
    "    #Combino los buckets para hacer un dataset\n",
    "    combined_dataset = bucket_datasets[0]\n",
    "    for bucket_ds in bucket_datasets[1:]:\n",
    "        combined_dataset = combined_dataset.concatenate(bucket_ds)\n",
    "    combined_dataset=combined_dataset.shuffle(buffer_size=1000)\n",
    "    return combined_dataset\n",
    "\n",
    "limites = [0,350,700,1000,3000]\n",
    "train_dataset = bucket_by_sequence_length(train_dataset, limites)\n",
    "val_dataset = bucket_by_sequence_length(val_dataset, limites)\n",
    "test_dataset = bucket_by_sequence_length(test_dataset, limites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el padding para que las secuencias dentro de los batches tengan la misma longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_batch_fn(batch,column):\n",
    "    column=batch[column]\n",
    "    \n",
    "    # Encuentra la longitud máxima para los puntos\n",
    "    points_lengths = tf.map_fn(lambda x: tf.shape(x)[0], column, dtype=tf.int32)\n",
    "    max_len_points = tf.reduce_max(points_lengths)\n",
    "    \n",
    "    # Aplica el padding dinámico a los puntos usando numpy_function\n",
    "    column_padded = tf.numpy_function(\n",
    "        lambda pts: pad_sequences(pts, maxlen=max_len_points, dtype='float32', padding='post'), \n",
    "        [column], tf.float32\n",
    "    )\n",
    "    \n",
    "    return column_padded\n",
    "\n",
    "# Función principal para mapear y agrupar en batches\n",
    "def pad_batches(dataset,column):\n",
    "    return dataset.batch(32).map(pad_batch_fn(column))\n",
    "\n",
    "train_dataset = pad_batches(train_dataset,\"points\")\n",
    "val_dataset=pad_batches(val_dataset,\"points\")\n",
    "test_dataset = pad_batches(test_dataset,\"points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pad_batches(train_dataset,\"translation\")\n",
    "val_dataset=pad_batches(val_dataset,\"translation\")\n",
    "test_dataset = pad_batches(test_dataset,\"translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscar mejores parámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "num_classes=len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "def ctc_loss(translation, prediction):\n",
    "    \n",
    "    input_length = tf.fill([tf.shape(prediction)[0], 1], tf.shape(prediction)[1])  # Longitud de la secuencia de entrada\n",
    "    label_length = tf.math.count_nonzero(translation, axis=-1, keepdims=True)   # Longitud de la secuencia de traducción\n",
    "    # Calcular la pérdida CTC usando ctc_batch_cost\n",
    "    loss = tf.keras.backend.ctc_batch_cost(translation, prediction, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Definir el modelo para la búsqueda de hiperparámetros\n",
    "def build_model(hp):\n",
    "\n",
    "    def createCNNkt():\n",
    "\n",
    "        model= models.Sequential()\n",
    "        # Añadir capas CNN dinámicamente según el número de capas que se elija (1 a 5 capas CNN)\n",
    "        for i in range(hp.Int('num_cnn_layers', 2, 5)):  # De 1 a 5 capas CNN\n",
    "            model.add(layers.Conv2D(\n",
    "                filters=hp.Int(f'conv_{i+1}_filters', min_value=64, max_value=256, step=32),\n",
    "                kernel_size=(3, 3),\n",
    "                activation='relu',\n",
    "                input_shape=(48, 48, 1) if i == 0 else None))\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        return model\n",
    "\n",
    "    def createLSTMkt():\n",
    "\n",
    "        model= models.Sequential()\n",
    "        # Añadir capas LSTM dinámicamente según el número de capas que se elija (1 a 5 capas LSTM)\n",
    "        for i in range(hp.Int('num_lstm_layers', 2, 5)):  # De 1 a 5 capas LSTM\n",
    "            model.add(layers.LSTM(\n",
    "                units=hp.Int(f'lstm_{i+1}_units', min_value=64, max_value=256, step=32),\n",
    "                return_sequences=True))\n",
    "\n",
    "            # Añadir Dropout para cada capa LSTM\n",
    "            model.add(layers.Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.4, step=0.1)))\n",
    "        return model\n",
    "\n",
    "    cnn = createCNNkt()\n",
    "\n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1))\n",
    "\n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "\n",
    "    lstm=createLSTMkt()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Definir la búsqueda de hiperparámetros con Keras Tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss', # Métrica de evaluación\n",
    "    max_epochs= 50,\n",
    "    factor=3,\n",
    "    directory='',\n",
    "    project_name='cnn_lstm_hyperparam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar la búsqueda de hiperparámetros\n",
    "tuner.search(train_dataset, validation_data=val_dataset)\n",
    "\n",
    "best_model=tuner.get_best_model(1)[0]\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_hyperparams = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear modelo,**EJEMPLO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3)) # Añadir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Tercera capa LSTM con return_sequences=False\n",
    "    model.add(layers.LSTM(64, return_sequences=False))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    cnn = create_cnn()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1)) \n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "# Compilar el modelo\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='RUTA',  # Ruta donde se guarda el modelo\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_bleu',        # Métrica a monitorear\n",
    "    mode='max',\n",
    "    monitor= 'val_loss',\n",
    "    mode='min'                 # Modo: minimizar la métrica monitorizada\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "loss_metric=ctc_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainstep(model, points_batch, translation_batch,step, epoch_loss_avg, epoch_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(points_batch, training=True)\n",
    "        loss = tf.reduce_mean(loss_metric(translation_batch, predictions))\n",
    "        \n",
    "    # Calcular y aplicar gradientes\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # if step % 50 == 0:\n",
    "    #     print(f\"Step {step}: loss = {loss.numpy()}\")\n",
    "    epoch_accuracy.update_state(translation_batch, predictions)\n",
    "    epoch_loss_avg.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg):\n",
    "    # Evaluar en el conjunto de validación al final de cada epoch usando BLEU\n",
    "   \n",
    "    val_predictions = model.predict(points_batch)\n",
    "\n",
    "    val_loss=tf.reduce_mean(ctc_loss(translation_batch,val_predictions))\n",
    "    val_loss_avg.update_state(val_loss)\n",
    "    val_acc_metric.update_state(translation_batch,val_predictions)\n",
    "    val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "    val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "    val_bleu.update_state(val_bleu)\n",
    "    # print(f\"Validation BLEU: {val_bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 100\n",
    "\n",
    "# Iterar a través de los batches y ajustar manualmente los parámetros\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_dataset_tqdm = tqdm(train_dataset, desc=\"Entrenamiento\", unit=\"batch\")\n",
    "    for step, (points_batch, translation_batch) in enumerate(train_dataset_tqdm):\n",
    "        \n",
    "        trainstep(model, points_batch,translation_batch, step, epoch_loss_avg, epoch_accuracy)\n",
    "        \n",
    "        train_dataset_tqdm.set_postfix({\n",
    "            \"Loss\": epoch_loss_avg.result().numpy(),\n",
    "            \"Accuracy\": epoch_accuracy.result().numpy()\n",
    "        })\n",
    "    \n",
    "    # Métricas para la validación\n",
    "    val_loss_avg = tf.keras.metrics.Mean()\n",
    "    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_bleu_avg = tf.keras.metrics.Mean()\n",
    "    \n",
    "    val_dataset_tqdm = tqdm(val_dataset, desc=\"Validación\", unit=\"batch\")\n",
    "    for step, (points_batch, translation_batch) in enumerate(val_dataset_tqdm):\n",
    "        \n",
    "        val_acc, val_loss, val_bleu = valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg)\n",
    "        \n",
    "        val_dataset_tqdm.set_postfix({\n",
    "            \"Val Loss\": val_loss.numpy(),\n",
    "            \"Val Accuracy\": val_acc.numpy(),\n",
    "            \"Val BLEU\": val_bleu\n",
    "        })\n",
    "\n",
    "    checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu})\n",
    "\n",
    "    if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu}):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "    print(f'Epoch: {epoch}, val_loss: {val_loss_avg}, val_acc: {val_acc_metric}, val_bleu: {val_bleu_avg}')\n",
    "\n",
    "# Evaluar el modelo en el dataset de test\n",
    "#test_loss, test_acc = model.evaluate(test_dataset)\n",
    "#print(f'Test accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
