{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 14:35:29.530209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-01 14:35:29.626864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-01 14:35:29.657766: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-01 14:35:29.825943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727793340.636180       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727793340.748661       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727793340.748716       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntos_folder = \"/app/AI-Module/Resources/Datasets/Points\"\n",
    "files = [puntos_folder + \"/\" + file for file in os.listdir(puntos_folder)]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(files):\n",
    "    for file in files:\n",
    "        item_with_index = np.load(file, allow_pickle=True)\n",
    "        # Convertir los arrays a float32 para reducir memoria\n",
    "        item = item_with_index[0].astype(np.float32)\n",
    "        index = item_with_index[1]\n",
    "        yield item, index\n",
    "\n",
    "puntos_list = []\n",
    "ids_list = []\n",
    "\n",
    "for item, index in load_points(files):\n",
    "    puntos_list.append(item)\n",
    "    ids_list.append(index)\n",
    "\n",
    "df_puntos = pd.DataFrame({\n",
    "    'points': puntos_list,\n",
    "    'id': ids_list\n",
    "})\n",
    "df_puntos = df_puntos.sort_values([\"id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(df_puntos, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points_x</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "      <th>points_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>If you like to find more about my services you...</td>\n",
       "      <td>448</td>\n",
       "      <td>817</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>Hello welcome my name is Julio Nutt and I am a...</td>\n",
       "      <td>210</td>\n",
       "      <td>401</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "      <td>All right, the drink we're about to make is ca...</td>\n",
       "      <td>228</td>\n",
       "      <td>308</td>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "      <td>Okay, another thing that for me is important. ...</td>\n",
       "      <td>187</td>\n",
       "      <td>367</td>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>All right, we're ready to start our fire. What...</td>\n",
       "      <td>99</td>\n",
       "      <td>453</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            points_x  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...   \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...   \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "\n",
       "                                         translation   id  len_keyframes  \\\n",
       "0  If you like to find more about my services you...  448            817   \n",
       "1  Hello welcome my name is Julio Nutt and I am a...  210            401   \n",
       "2  All right, the drink we're about to make is ca...  228            308   \n",
       "3  Okay, another thing that for me is important. ...  187            367   \n",
       "4  All right, we're ready to start our fire. What...   99            453   \n",
       "\n",
       "                                            points_y  \n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...  \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...  \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"points_y\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"points\"]=dataset[\"points_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el formato de points para que sea una lista de 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Agrega la ruta donde est√° ubicado Points2VecClass.py\n",
    "sys.path.append('/app/AI-Module/Modules')\n",
    "from Points2VecClass import Point2Vec\n",
    "def pointsToCnnInputForm(points):\n",
    "    p2v = Point2Vec(4)\n",
    "    return p2v.CNNMatrix(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cnn_input_form(serie):\n",
    "    for index, element in enumerate(serie):\n",
    "        element= pointsToCnnInputForm(element)\n",
    "        serie.at[index]=element\n",
    "    return serie\n",
    "dataset['points']=to_cnn_input_form(dataset['points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizo las traducciones para que sean compatibles con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Crear el tokenizador\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustar el tokenizador al texto de las traducciones\n",
    "tokenizer.fit_on_texts(dataset['translation'])\n",
    "\n",
    "# Convertir las oraciones en secuencias de enteros\n",
    "dataset['translation'] = tokenizer.texts_to_sequences(dataset['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Divide en train y test\n",
    "points_train_val, points_test, translation_train_val, translation_test = train_test_split(\n",
    "    dataset[[\"points\", \"len_keyframes\"]], dataset[\"translation\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "points_train, points_val, translation_train, translation_val=train_test_split(\n",
    "    points_train_val,translation_train_val, test_size=0.25,random_state=42\n",
    ")\n",
    "\n",
    "train = {\n",
    "    \"points\": points_train[\"points\"],\n",
    "    \"len_keyframes\": points_train[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_train\n",
    "}\n",
    "\n",
    "val = {\n",
    "    \"points\": points_val[\"points\"],\n",
    "    \"len_keyframes\": points_val[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_val\n",
    "}\n",
    "\n",
    "test = {\n",
    "    \"points\": points_test[\"points\"],\n",
    "    \"len_keyframes\": points_test[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_test\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "globals().pop('dataset', None)\n",
    "globals().pop('puntos_folder', None)\n",
    "globals().pop('puntos_list', None)\n",
    "globals().pop('df_puntos', None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo datasets de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(df):\n",
    "    def generator():\n",
    "        for i in range(len(df)):\n",
    "            points = df['points'].iloc[i]\n",
    "            sequence = df['translation_sequence'].iloc[i]\n",
    "            yield points, sequence\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 48, 48), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727793513.663000       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727793513.663218       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727793513.663327       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727793520.104450       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727793520.104539       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-01 14:38:40.104558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1727793520.104614       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-01 14:38:40.105257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9355 MB memory:  -> device: 0, name: NVIDIA RTX 3500 Ada Generation Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "test_dataset = create_tf_dataset(test)\n",
    "val_dataset=create_tf_dataset(val)\n",
    "train_dataset= create_tf_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 48, 48), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1133, 48, 48), dtype=float32, numpy=\n",
      "array([[[ 1.    ,  0.6049,  1.    , ...,  0.7828,  0.3171,  1.    ],\n",
      "        [ 0.308 ,  0.7701,  0.2518, ...,  0.198 ,  0.5576,  1.    ],\n",
      "        [ 0.811 ,  0.109 ,  0.3641, ...,  0.7605,  0.3062,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4812,  0.3349,  0.2868, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[ 1.    ,  0.5889,  1.    , ...,  0.7625,  0.3176,  1.    ],\n",
      "        [ 0.0208,  0.7787,  0.2834, ...,  0.1822,  0.5087,  1.    ],\n",
      "        [ 0.7561,  0.1014,  0.2969, ...,  0.7796,  0.3118,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4853,  0.3326,  0.2908, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[ 1.    ,  0.5205,  1.    , ...,  0.7323,  0.2591,  1.    ],\n",
      "        [ 0.0196,  0.7315,  0.2167, ...,  0.2206,  0.4395,  1.    ],\n",
      "        [ 0.7376,  0.1138,  0.2324, ...,  0.8485,  0.4687,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4757,  0.3234,  0.2857, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.5452,  0.8084,  1.    ],\n",
      "        [ 0.5986,  0.2849,  0.6504, ...,  0.1393,  0.0702,  1.    ],\n",
      "        ...,\n",
      "        [ 0.6448,  0.3857,  0.2927, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.6777,  0.8154,  1.    ],\n",
      "        [ 1.    ,  0.3773,  0.6848, ...,  0.0315,  0.2242,  1.    ],\n",
      "        ...,\n",
      "        [ 0.6363,  0.401 ,  0.2768, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.0925,  0.4356,  1.    ],\n",
      "        [ 0.7102,  0.1923,  0.2803, ...,  0.8032,  0.4166,  1.    ],\n",
      "        ...,\n",
      "        [ 0.577 ,  0.3636,  0.2905, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(646,), dtype=int32, numpy=\n",
      "array([ 997,   32,    2,  135,  144, 2281,    6,    5, 1353,  906,  115,\n",
      "          8,    7,   50,  276, 3070,   11, 3043,    2,    1,  235,  352,\n",
      "       1681,    3,   14,   15,    3,   41,   19,   96, 7197,   43,   35,\n",
      "       3133,    8,    4, 7198,   11,    1,  906,   41,   19,   59,  117,\n",
      "          8,   19, 7199,   78,  641,    5,  526,  356,  352,  906,   87,\n",
      "         16,   64, 4410,    2,   16,  155,   24,   86,   67,    6,    1,\n",
      "        591,   42,   13,    7,  115,   63, 2978,   11, 4383,   30, 1000,\n",
      "        100,    1, 7200,    6,    1,  356,  352, 2306,    3,    1,  356,\n",
      "        352, 7201,   14,  356,  594,    6,  291,   24, 1869,  155,    6,\n",
      "         19, 1533,    3, 3520,   42,   41,   19,   98,  356,  594,   30,\n",
      "        111,  356,  944,   22,  333,   27,    4,  226,    1, 4411, 4203,\n",
      "        226, 4411, 7202,  261,  356,  944,   11,  261, 2567, 1031,   11,\n",
      "          1, 2045,    3,    8,    7,  356,  352,  128,  356, 3254,  530,\n",
      "         32,  356,  463,    3,   11,   59, 1396,   13,    7, 1577,   50,\n",
      "         48,  356,  463,  255,   66,    5,  984, 3254,    3,   35,    8,\n",
      "        268,    7,    1, 7203,    7, 3231,   15,    5,  458, 2375,   11,\n",
      "         59, 1760, 2274,    2,  104,   71,  238,    3,    2, 1013, 4412,\n",
      "         14,    5,  984, 3254, 1316,   38,    5,   58,  851,  356,  352,\n",
      "        481,   11,   51, 7204,    3,  356,  594,  133,   81,   23, 4413,\n",
      "       4194, 3788, 7205,  128,  202,  152, 1691,    6, 4414, 4414, 3980,\n",
      "          6,    5,   85,    6,   96,  339,    6,  356,  202,    3,   24,\n",
      "         17,  135,   43,    1, 3133,    1, 1199,    6,  356,  352,    3,\n",
      "          1, 1199,    6,  936,  352,   30,   16,  153,   16,   64, 4410,\n",
      "          2,    5,  526,  356,  906,   42,   16, 1012,   30,  124,   30,\n",
      "       3002, 1732,    6,    1,  906,   17,   23,  356,    3,    1,  638,\n",
      "         20,  356,  594,    4,  131,   86,    2,   23,  668,   43,  211,\n",
      "        356,  594, 7206, 4412,   32,  356,  202, 2486,    8,  131,   23,\n",
      "         15,  664,  356,  594,  137,  124,  356,  352,   11,    1,  906,\n",
      "         17,   93, 7207, 7208,    3,  664, 2698,    4,   80,   50,   25,\n",
      "          2,  637,    5,   85,    6,  356,  594,   32,   27,    5,  379,\n",
      "        259, 7209,   66, 7210,   22,  333,   87,    7,  137,  124, 4415,\n",
      "         11,    1,  157,   24,   64,   25,    2, 4159,   67, 4415,  100,\n",
      "       1463,    5,   85,    6,  356,  594,   59,    6,    1, 3565,    6,\n",
      "        936,  594,  936,  594,   93, 1237,  270,   59,    6,    1, 4094,\n",
      "         67, 1577,    2,    1,  157,   32,   98, 2379,    6, 7211,   49,\n",
      "        249,   22,  591,   52,   30,   11,    1,  360,    6, 7212,    4,\n",
      "         17,   34, 7213,   67, 7214,  100, 1237,    1, 1533,   50,   94,\n",
      "       1237,   71,   42, 7215, 7216,   30,   47,  153, 4416, 4416, 7217,\n",
      "       7218,  144, 2764,    6,    1, 7219, 1027,    3,   24,   84,    8,\n",
      "       1307,  262,   34,   67,  936, 3145,  530,   11,    1,  906,   18,\n",
      "          5,  364, 3459,    6, 7220, 7221,    8,  193,   50, 1000,   11,\n",
      "        114, 4339,    1,  356, 3145,  530,   14,  108,   24,   86,    5,\n",
      "        113,  591,    6,  356,    3,  936,  594,   11,    1,  906,  111,\n",
      "        372,    5, 3255, 1784,    2,  356,  594,    3,   29,  150,    2,\n",
      "         34,  356,  352,   11,    1,  906,   22,    1, 1134,   16,  167,\n",
      "        153,    1, 4278, 4417,   11,   41,   13,    7,  115,   32,    4,\n",
      "        131,  134,   11,   59,    6,    1,  356,  352, 2306,   13,    7,\n",
      "          5, 4418, 7222,   29,  102,   73, 2446, 1331,    3,    9,   97,\n",
      "       4418, 7223, 4419, 7224, 2632,    3,   29,  102,  527,    6,    5,\n",
      "       2115, 2446, 7225, 3356,  285, 4420,  372,  144,   37,   13,    7,\n",
      "          5, 7226,  636,   13,   97, 4413, 7227,    3, 7228,   11,   52,\n",
      "         81, 4420, 4417,   19, 2563,    1,  499, 1215,    6,  594,   14,\n",
      "         78,  170,    6, 3255,  356,  594,    4,   80,   18,    2,   89,\n",
      "        101,  171,   10,  471,  350,  594,  334,  169,   69,   67,    3,\n",
      "         67,  139, 1561,   59,    6,   78, 3255,  356, 4153,   14,   27,\n",
      "          4,   65,    5,  356,  352,   32,    5,  356, 1373,   11,    1,\n",
      "        537,    9,   54, 1321,   76,    5,  649,   27,    4,   65,    5,\n",
      "        936, 1373,   11,    1,  537,  485,   21, 7229,   14,   63,   73,\n",
      "        333,    6,    1,  499, 1215,   11,  356,  594], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 14:39:15.063155: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for element in train_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: test\n",
      "Valores entre 0 y 350: 35\n",
      "Valores entre 350 y 700: 55\n",
      "Valores entre 700 y 1000: 7\n",
      "Valores entre 1000 y 3000: 3\n",
      "Dataset: train\n",
      "Valores entre 0 y 350: 108\n",
      "Valores entre 350 y 700: 158\n",
      "Valores entre 700 y 1000: 24\n",
      "Valores entre 1000 y 3000: 10\n",
      "Dataset: val\n",
      "Valores entre 0 y 350: 30\n",
      "Valores entre 350 y 700: 59\n",
      "Valores entre 700 y 1000: 11\n",
      "Valores entre 1000 y 3000: 0\n"
     ]
    }
   ],
   "source": [
    "def print_lenForBucketing(ds,limites):\n",
    "\n",
    "    # Crear un DataFrame vac√≠o para almacenar los resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Recorrer los pares consecutivos de la lista de l√≠mites\n",
    "    for lower, upper in zip(limites[:-1], limites[1:]):\n",
    "        # Contar cu√°ntos valores est√°n entre 'lower' y 'upper'\n",
    "        count = ((ds['len_keyframes'] > lower) & (ds['len_keyframes'] <= upper)).sum()\n",
    "        \n",
    "        # Guardar el resultado en una lista\n",
    "        resultados.append((lower, upper, count))\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    for lower, upper, count in resultados:\n",
    "        print(f\"Valores entre {lower} y {upper}: {count}\")\n",
    "\n",
    "\n",
    "limites = [0,350,700, 1000,3000]\n",
    "print(f'Dataset: test')\n",
    "print_lenForBucketing(test,limites)\n",
    "print(f'Dataset: train')\n",
    "print_lenForBucketing(train,limites)\n",
    "print(f'Dataset: val')\n",
    "\n",
    "print_lenForBucketing(val,limites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_fn(points, translations):\n",
    "    # La longitud ser√° la cantidad de keyframes (primera dimensi√≥n de los puntos)\n",
    "    return tf.shape(points)[0]\n",
    "\n",
    "# Definir los l√≠mites para los buckets basados en la longitud de la secuencia (primer dimensi√≥n)\n",
    "limites = [350,700,1100,3000]\n",
    "\n",
    "# Tama√±o de batch para cada bucket\n",
    "bucket_batch_sizes = [32, 32, 8, 8]  # Puedes ajustar estos tama√±os seg√∫n tus necesidades\n",
    "\n",
    "def bucketed_dataset(dataset):\n",
    "    return dataset.apply(\n",
    "        tf.data.experimental.bucket_by_sequence_length(\n",
    "            element_length_func=length_fn,       # Funci√≥n para obtener la longitud de la secuencia\n",
    "            bucket_boundaries=limites, # L√≠mites de los buckets\n",
    "            bucket_batch_sizes=bucket_batch_sizes,  # Tama√±os de batch para cada bucket\n",
    "            padded_shapes=(\n",
    "                [None, 48, 48],  # Para los puntos (None en la primera dimensi√≥n para permitir padding din√°mico)\n",
    "                [None]           # Para las traducciones (None en la primera dimensi√≥n)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Aplicar bucketing y padding\n",
    "train_dataset= bucketed_dataset(train_dataset)\n",
    "val_dataset= bucketed_dataset(val_dataset)\n",
    "test_dataset= bucketed_dataset(test_dataset)\n",
    "\n",
    "def count_batches(dataset):\n",
    "    batch_count = 0\n",
    "    for _ in dataset:\n",
    "        batch_count += 1\n",
    "    return batch_count\n",
    "print(f\"Cantidad de batches en train: {count_batches(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bucket_by_sequence_length(ds, bucket_values):\n",
    "#     # Defino los buckets basados en los l√≠mites de longitud\n",
    "#     bucket_datasets = []\n",
    "    \n",
    "#     for i in range(len(bucket_values) - 1):\n",
    "#         lower_bound = bucket_values[i]\n",
    "#         upper_bound = bucket_values[i+1]\n",
    "        \n",
    "#         # Filtro secuencias que caen dentro de los l√≠mites de este bucket\n",
    "#         bucket = ds.filter(lambda x, y: tf.logical_and(\n",
    "#             tf.shape(x)[0] >= lower_bound,  # x se refiere a \"points\"\n",
    "#             tf.shape(x)[0] < upper_bound\n",
    "#         ))\n",
    "        \n",
    "#         bucket = bucket.shuffle(buffer_size=1000)\n",
    "#         bucket = bucket.batch(32)\n",
    "        \n",
    "#         bucket_datasets.append(bucket)\n",
    "    \n",
    "#     # Combino los buckets para hacer un dataset\n",
    "#     combined_dataset = bucket_datasets[0]\n",
    "\n",
    "#     for bucket_ds in bucket_datasets[1:]:\n",
    "#         combined_dataset = combined_dataset.concatenate(bucket_ds)\n",
    "    \n",
    "#     # Pongo los batches en orden random\n",
    "#     combined_dataset = combined_dataset.shuffle(buffer_size=1000)\n",
    "#     return combined_dataset\n",
    "\n",
    "# limites = [0,350,700,1100,3000]\n",
    "# train_dataset = bucket_by_sequence_length(ds=train_dataset, bucket_values=limites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = bucket_by_sequence_length(val_dataset, limites)\n",
    "#test_dataset = bucket_by_sequence_length(val_dataset, limites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el padding para que las secuencias dentro de los batches tengan la misma longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch_fn(points, translations):\n",
    "    \n",
    "    # Encuentra la longitud m√°xima para los puntos en el batch\n",
    "    points_lengths = tf.map_fn(lambda x: tf.shape(x)[0], points, dtype=tf.int32)\n",
    "    max_len_points = tf.reduce_max(points_lengths)\n",
    "\n",
    "    # Imprimir la forma antes del padding (para el primer elemento de 'points')\n",
    "    tf.print(\"Forma de 'points' antes del padding (primer elemento):\", tf.shape(points[0]))\n",
    "\n",
    "    points_padded = tf.map_fn(\n",
    "        lambda point: tf.pad(point, [[0, max_len_points - tf.shape(point)[0]], [0, 0], [0, 0]]), \n",
    "        points\n",
    "    )\n",
    "\n",
    "    # Imprimir la forma despu√©s del padding (para el primer elemento de 'points')\n",
    "    tf.print(\"Forma de 'points' despu√©s del padding (primer elemento):\", tf.shape(points_padded[0]))\n",
    "\n",
    "    # Encuentra la longitud m√°xima para las traducciones en el batch\n",
    "    translations_lengths = tf.map_fn(lambda x: tf.shape(x)[0], translations, dtype=tf.int32)\n",
    "    max_len_translations = tf.reduce_max(translations_lengths)\n",
    "\n",
    "    # Imprimir la forma antes del padding (para el primer elemento de 'translations')\n",
    "    tf.print(\"Forma de 'translations' antes del padding (primer elemento):\", tf.shape(translations[0]))\n",
    "\n",
    "    translations_padded = tf.map_fn(\n",
    "        lambda translation: tf.pad(translation, [[0, max_len_translations - tf.shape(translation)[0]]]), \n",
    "        translations\n",
    "    )\n",
    "\n",
    "    # Imprimir la forma despu√©s del padding (para el primer elemento de 'translations')\n",
    "    tf.print(\"Forma de 'translations' despu√©s del padding (primer elemento):\", tf.shape(translations_padded[0]))\n",
    "\n",
    "    return points_padded, translations_padded\n",
    "# Funci√≥n principal para mapear y agrupar en batches\n",
    "def pad_batches(ds):\n",
    "    return ds.map(pad_batch_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de 'points' antes del padding (primer elemento): Tensor(\"Shape:0\", shape=(3,), dtype=int32)\n",
      "Forma de 'points' despu√©s del padding (primer elemento): Tensor(\"Shape_1:0\", shape=(3,), dtype=int32)\n",
      "Forma de 'translations' antes del padding (primer elemento): Tensor(\"Shape_2:0\", shape=(1,), dtype=int32)\n",
      "Forma de 'translations' despu√©s del padding (primer elemento): Tensor(\"Shape_3:0\", shape=(1,), dtype=int32)\n",
      "Forma de 'points' antes del padding (primer elemento): Tensor(\"Shape:0\", shape=(3,), dtype=int32)\n",
      "Forma de 'points' despu√©s del padding (primer elemento): Tensor(\"Shape_1:0\", shape=(3,), dtype=int32)\n",
      "Forma de 'translations' antes del padding (primer elemento): Tensor(\"Shape_2:0\", shape=(1,), dtype=int32)\n",
      "Forma de 'translations' despu√©s del padding (primer elemento): Tensor(\"Shape_3:0\", shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pad_batches(train_dataset)\n",
    "val_dataset=pad_batches(val_dataset)\n",
    "#test_dataset = pad_batches(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funci√≥n de p√©rdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(translation, prediction):\n",
    "    \n",
    "    input_length = tf.fill([tf.shape(prediction)[0], 1], tf.shape(prediction)[1])  # Longitud de la secuencia de entrada\n",
    "    label_length = tf.math.count_nonzero(translation, axis=-1, keepdims=True)   # Longitud de la secuencia de traducci√≥n\n",
    "    # Calcular la p√©rdida CTC usando ctc_batch_cost\n",
    "    loss = tf.keras.backend.ctc_batch_cost(translation, prediction, input_length, label_length)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscar mejores par√°metros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_tuner as kt\n",
    "# num_classes=len(tokenizer.word_index) + 1\n",
    "\n",
    "# # Definir el modelo para la b√∫squeda de hiperpar√°metros\n",
    "# def build_model(hp):\n",
    "\n",
    "#     def createCNNkt():\n",
    "\n",
    "#         model= models.Sequential()\n",
    "#         # A√±adir capas CNN din√°micamente seg√∫n el n√∫mero de capas que se elija (1 a 5 capas CNN)\n",
    "#         for i in range(hp.Int('num_cnn_layers', 2, 5)):  # De 1 a 5 capas CNN\n",
    "#             model.add(layers.Conv2D(\n",
    "#                 filters=hp.Int(f'conv_{i+1}_filters', min_value=64, max_value=256, step=32),\n",
    "#                 kernel_size=(3, 3),\n",
    "#                 activation='relu',\n",
    "#                 input_shape=(48, 48, 1) if i == 0 else None))\n",
    "#             model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "#         model.add(layers.Flatten())\n",
    "#         return model\n",
    "\n",
    "#     def createLSTMkt():\n",
    "\n",
    "#         model= models.Sequential()\n",
    "#         # A√±adir capas LSTM din√°micamente seg√∫n el n√∫mero de capas que se elija (1 a 5 capas LSTM)\n",
    "#         for i in range(hp.Int('num_lstm_layers', 2, 5)):  # De 1 a 5 capas LSTM\n",
    "#             model.add(layers.LSTM(\n",
    "#                 units=hp.Int(f'lstm_{i+1}_units', min_value=64, max_value=256, step=32),\n",
    "#                 return_sequences=True))\n",
    "\n",
    "#             # A√±adir Dropout para cada capa LSTM\n",
    "#             model.add(layers.Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.4, step=0.1)))\n",
    "#         return model\n",
    "\n",
    "#     cnn = createCNNkt()\n",
    "\n",
    "#     video_input = layers.Input(shape=(None, 48, 48, 1))\n",
    "\n",
    "#     # Aplicar CNN a cada frame usando TimeDistributed\n",
    "#     cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "\n",
    "#     lstm=createLSTMkt()\n",
    "#     lstm_out= lstm(cnn_features)\n",
    "#     # Capa final de salida\n",
    "#     output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "\n",
    "#     # Compilar el modelo\n",
    "#     model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "#     model = models.Model(inputs=video_input, outputs=output)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Definir la b√∫squeda de hiperpar√°metros con Keras Tuner\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_loss', # M√©trica de evaluaci√≥n\n",
    "#     max_epochs= 50,\n",
    "#     factor=3,\n",
    "#     directory='',\n",
    "#     project_name='cnn_lstm_hyperparam'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Realizar la b√∫squeda de hiperpar√°metros\n",
    "# tuner.search(train_dataset, validation_data=val_dataset)\n",
    "\n",
    "# best_model=tuner.get_best_model(1)[0]\n",
    "# # Obtener los mejores hiperpar√°metros encontrados\n",
    "# best_hyperparams = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear modelo,**EJEMPLO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3)) # A√±adir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(64, return_sequences=False))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    cnn = create_cnn()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1)) \n",
    "    masked_input = layers.Masking(mask_value=0.0)(video_input)\n",
    "\n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(masked_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaci√≥n para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(tokenizer.word_index) + 1\n",
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "# Compilar el modelo\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/',  # Ruta donde se guarda el modelo\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_bleu',        # M√©trica a monitorear\n",
    "    mode='max',          # Modo: minimizar la m√©trica monitorizada\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "loss_metric=ctc_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainstep(model, points_batch, translation_batch,step, epoch_loss_avg, epoch_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(points_batch, training=True)\n",
    "        loss = tf.reduce_mean(loss_metric(translation_batch, predictions))\n",
    "        \n",
    "    # Calcular y aplicar gradientes\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # if step % 50 == 0:\n",
    "    #     print(f\"Step {step}: loss = {loss.numpy()}\")\n",
    "    epoch_accuracy.update_state(translation_batch, predictions)\n",
    "    epoch_loss_avg.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg):\n",
    "    # Evaluar en el conjunto de validaci√≥n al final de cada epoch usando BLEU\n",
    "   \n",
    "    val_predictions = model.predict(points_batch)\n",
    "\n",
    "    val_loss=tf.reduce_mean(ctc_loss(translation_batch,val_predictions))\n",
    "    val_loss_avg.update_state(val_loss)\n",
    "    val_acc_metric.update_state(translation_batch,val_predictions)\n",
    "    val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "    val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "    val_bleu_avg.update_state(val_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 100\n",
    "\n",
    "# Iterar a trav√©s de los batches y ajustar manualmente los par√°metros\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_dataset_tqdm = tqdm(train_dataset, desc=\"Entrenamiento\", unit=\"batch\")\n",
    "    for step, (points_batch, translation_batch) in enumerate(train_dataset_tqdm):\n",
    "        \n",
    "        trainstep(model, points_batch,translation_batch, step, epoch_loss_avg, epoch_accuracy)\n",
    "        \n",
    "        train_dataset_tqdm.set_postfix({\n",
    "            \"Loss\": epoch_loss_avg.result().numpy(),\n",
    "            \"Accuracy\": epoch_accuracy.result().numpy()\n",
    "        })\n",
    "    \n",
    "    # M√©tricas para la validaci√≥n\n",
    "    val_loss_avg = tf.keras.metrics.Mean()\n",
    "    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_bleu_avg = tf.keras.metrics.Mean()\n",
    "    \n",
    "    val_dataset_tqdm = tqdm(val_dataset, desc=\"Validaci√≥n\", unit=\"batch\")\n",
    "    for step, (points_batch, translation_batch) in enumerate(val_dataset_tqdm):\n",
    "        \n",
    "        val_acc, val_loss, val_bleu = valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg)\n",
    "        \n",
    "        val_dataset_tqdm.set_postfix({\n",
    "            \"Val Loss\": val_loss.numpy(),\n",
    "            \"Val Accuracy\": val_acc.numpy(),\n",
    "            \"Val BLEU\": val_bleu\n",
    "        })\n",
    "\n",
    "    checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu})\n",
    "\n",
    "    if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu}):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "    print(f'Epoch: {epoch}, val_loss: {val_loss_avg}, val_acc: {val_acc_metric}, val_bleu: {val_bleu_avg}')\n",
    "\n",
    "# Evaluar el modelo en el dataset de test\n",
    "#test_loss, test_acc = model.evaluate(test_dataset)\n",
    "#print(f'Test accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
