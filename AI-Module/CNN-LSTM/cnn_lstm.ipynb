{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 16:15:47.118168: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-23 16:15:47.270493: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-23 16:15:47.321454: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-23 16:15:47.601532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727108155.945248       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727108155.990648       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727108155.990689       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>If you like to find more about my services you...</td>\n",
       "      <td>448</td>\n",
       "      <td>817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>Hello welcome my name is Julio Nutt and I am a...</td>\n",
       "      <td>210</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "      <td>All right, the drink we're about to make is ca...</td>\n",
       "      <td>228</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "      <td>Okay, another thing that for me is important. ...</td>\n",
       "      <td>187</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>All right, we're ready to start our fire. What...</td>\n",
       "      <td>99</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              points  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...   \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...   \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "\n",
       "                                         translation   id  len_keyframes  \n",
       "0  If you like to find more about my services you...  448            817  \n",
       "1  Hello welcome my name is Julio Nutt and I am a...  210            401  \n",
       "2  All right, the drink we're about to make is ca...  228            308  \n",
       "3  Okay, another thing that for me is important. ...  187            367  \n",
       "4  All right, we're ready to start our fire. What...   99            453  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Agrega la ruta donde está ubicado Points2VecClass.py\n",
    "sys.path.append('/app/AI-Module/Modules')\n",
    "print(sys.version)\n",
    "from Points2VecClass import Point2Vec\n",
    "def pointsToCnnInputForm(video):\n",
    "    p2v = Point2Vec(4)\n",
    "    return p2v.CNNMatrix(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el formato de points para que sea una lista de 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['points'] = dataset['points'].apply(ast.literal_eval)\n",
    "dataset['points']=dataset['points'].apply(pointsToCnnInputForm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar texto de traducciones para que sea compatible con el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer =Tokenizer()\n",
    "tokens1=tokenizer.fit_on_texts(dataset[\"translation\"].iloc[0])\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide en train y test\n",
    "points_train_val, points_test, translation_train_val, translation_test = train_test_split(\n",
    "    dataset[\"points\", \"len_keyframes\"], dataset[\"translation\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "points_train, points_val, translation_train, translation_val=train_test_split(\n",
    "    points_train_val,translation_train_val, test_size=0.25,random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = pd.DataFrame({\n",
    "    \"points\": points_train,\n",
    "    \"translation\": translation_train\n",
    "})\n",
    "\n",
    "val_dataset = pd.DataFrame({\n",
    "    \"points\": points_val,\n",
    "    \"translation\": translation_val\n",
    "})\n",
    "\n",
    "test_dataset = pd.DataFrame({\n",
    "    \"points\": points_test,\n",
    "    \"translation\": translation_test\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el padding para que las secuencias dentro de los batches tengan la misma longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_batch_fn(batch,column):\n",
    "    column=batch[column]\n",
    "    \n",
    "    # Encuentra la longitud máxima para los puntos\n",
    "    points_lengths = tf.map_fn(lambda x: tf.shape(x)[0], column, dtype=tf.int32)\n",
    "    max_len_points = tf.reduce_max(points_lengths)\n",
    "    \n",
    "    # Aplica el padding dinámico a los puntos usando numpy_function\n",
    "    column_padded = tf.numpy_function(\n",
    "        lambda pts: pad_sequences(pts, maxlen=max_len_points, dtype='float32', padding='post'), \n",
    "        [column], tf.float32\n",
    "    )\n",
    "    \n",
    "    return column_padded\n",
    "\n",
    "# Función principal para mapear y agrupar en batches\n",
    "def pad_batches(dataset,column):\n",
    "    return dataset.batch(32).map(pad_batch_fn(column))\n",
    "\n",
    "train_dataset = pad_batches(train_dataset,\"points\")\n",
    "val_dataset=pad_batches(val_dataset,\"points\")\n",
    "test_dataset = pad_batches(test_dataset,\"points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VideoFormaterClass import VideoFormater\n",
    "\n",
    "path='../Resoruces/Dataset'\n",
    "VideoFormater(train_dataset, path)\n",
    "VideoFormater(val_dataset, path)\n",
    "VideoFormater(test_dataset, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizo las traducciones para que sean compatibles con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Crear el tokenizador\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustar el tokenizador al texto de las traducciones\n",
    "tokenizer.fit_on_texts(dataset['translation'])\n",
    "\n",
    "# Convertir las oraciones en secuencias de enteros\n",
    "dataset['translation_sequence'] = tokenizer.texts_to_sequences(dataset['translation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pad_batches(train_dataset,\"translation\")\n",
    "val_dataset=pad_batches(val_dataset,\"translation\")\n",
    "test_dataset = pad_batches(test_dataset,\"translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo datasets de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(df):\n",
    "    points = tf.ragged.constant(df['points'].values)\n",
    "    sequence = tf.ragged.constant(df['translation_sequence'].values)\n",
    "    return tf.data.Dataset.from_tensor_slices((points, sequence))\n",
    "\n",
    "test_dataset = create_tf_dataset(test_ds)\n",
    "val_dataset=create_tf_dataset(val_ds)\n",
    "train_dataset= create_tf_dataset(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscar mejores parámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "num_classes=len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "def ctc_loss(translation, prediction):\n",
    "    \n",
    "    input_length = tf.fill([tf.shape(prediction)[0], 1], tf.shape(prediction)[1])  # Longitud de la secuencia de entrada\n",
    "    label_length = tf.math.count_nonzero(translation, axis=-1, keepdims=True)   # Longitud de la secuencia de traducción\n",
    "    # Calcular la pérdida CTC usando ctc_batch_cost\n",
    "    loss = tf.keras.backend.ctc_batch_cost(translation, prediction, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Definir el modelo para la búsqueda de hiperparámetros\n",
    "def build_model(hp):\n",
    "\n",
    "    def createCNNkt():\n",
    "\n",
    "        model= models.Sequential()\n",
    "        # Añadir capas CNN dinámicamente según el número de capas que se elija (1 a 5 capas CNN)\n",
    "        for i in range(hp.Int('num_cnn_layers', 1, 5)):  # De 1 a 5 capas CNN\n",
    "            model.add(layers.Conv2D(\n",
    "                filters=hp.Int(f'conv_{i+1}_filters', min_value=32, max_value=128, step=32),\n",
    "                kernel_size=(3, 3),\n",
    "                activation='relu',\n",
    "                input_shape=(48, 48, 1) if i == 0 else None))\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        return model\n",
    "\n",
    "    def createLSTMkt():\n",
    "\n",
    "        model= models.Sequential()\n",
    "        # Añadir capas LSTM dinámicamente según el número de capas que se elija (1 a 5 capas LSTM)\n",
    "        for i in range(hp.Int('num_lstm_layers', 1, 5)):  # De 1 a 5 capas LSTM\n",
    "            model.add(layers.LSTM(\n",
    "                units=hp.Int(f'lstm_{i+1}_units', min_value=32, max_value=128, step=32),\n",
    "                return_sequences=True))\n",
    "\n",
    "            # Añadir Dropout para cada capa LSTM\n",
    "            model.add(layers.Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "        return model\n",
    "\n",
    "    cnn = createCNNkt()\n",
    "\n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1))\n",
    "\n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "\n",
    "    lstm=createLSTMkt()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Definir la búsqueda de hiperparámetros con Keras Tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss', # Métrica de evaluación\n",
    "    max_epochs= 50,\n",
    "    factor=3,\n",
    "    directory='',\n",
    "    project_name='cnn_lstm_hyperparam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar la búsqueda de hiperparámetros\n",
    "tuner.search(train_dataset, validation_data=val_dataset)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear modelo,**EJEMPLO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3)) # Añadir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Tercera capa LSTM con return_sequences=False\n",
    "    model.add(layers.LSTM(64, return_sequences=False))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    cnn = create_cnn()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1)) \n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='softmax')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "# Compilar el modelo\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='RUTA',  # Ruta donde se guarda el modelo\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_bleu',        # Métrica a monitorear\n",
    "    mode='max',\n",
    "    monitor= 'val_loss',\n",
    "    mode='min'                 # Modo: minimizar la métrica monitorizada\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "loss_metric=tf.keras.backend.ctc_batch_cost(from_logits=True, reduction= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainstep(model, points_batch, translation_batch,step, epoch_loss_avg, epoch_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(points_batch, training=True)\n",
    "        loss = tf.reduce_mean(loss_metric(translation_batch, predictions))\n",
    "        \n",
    "    # Calcular y aplicar gradientes\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}: loss = {loss.numpy()}\")\n",
    "    epoch_accuracy.update_state(translation_batch, predictions)\n",
    "    epoch_loss_avg.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valstep(model, points_batch, translation_batch):\n",
    "    val_loss, val_acc = model.evaluate(x=points_batch, y=translation_batch)\n",
    "    print(f\"Validation loss: {val_loss}, Validation accuracy: {val_acc}\")\n",
    "    \n",
    "    # Evaluar en el conjunto de validación al final de cada epoch usando BLEU\n",
    "    val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "    val_predictions = model.predict(points_batch)\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "    val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "    print(f\"Validation BLEU: {val_bleu}\")\n",
    "    return val_acc,val_loss,val_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 100\n",
    "\n",
    "# Iterar a través de los batches y ajustar manualmente los parámetros\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    for step, (points_batch, translation_batch) in enumerate(train_dataset):\n",
    "        trainstep(model, points_batch,translation_batch, step, epoch_loss_avg, epoch_accuracy)\n",
    "    \n",
    "    \n",
    "    for step, (points_batch,translation_batch) in enumerate(val_dataset):\n",
    "        val_acc, val_loss, val_bleu = valstep(model, points_batch, translation_batch)\n",
    "\n",
    "    checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu})\n",
    "\n",
    "    if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu}):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "\n",
    "# Evaluar el modelo en el dataset de test\n",
    "#test_loss, test_acc = model.evaluate(test_dataset)\n",
    "#print(f'Test accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
