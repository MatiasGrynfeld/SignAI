{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 13:34:16.755380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-23 13:34:16.772329: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-23 13:34:16.776880: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-23 13:34:16.789586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727098465.643944       8 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727098465.649055       8 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727098465.649090       8 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>We did some exercises today that are going to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>And now we're going to make a, it's called a, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>It should really spring back and feel dry. Thi...</td>\n",
       "      <td>2</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>[[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...</td>\n",
       "      <td>So let's begin with the basics. As you can see...</td>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>[[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....</td>\n",
       "      <td>Hi! A lot of us has started it into the journe...</td>\n",
       "      <td>4</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                points  \\\n",
       "223  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "242  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "358  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "492  [[0.7839, 0.0, 1.0, 1.0, 0.5049, 0.1065, 0.702...   \n",
       "71   [[0.8621, 0.0904, 1.0, 1.0, 0.5985, 0.0007, 0....   \n",
       "\n",
       "                                           translation  id  len_keyframes  \n",
       "223  We did some exercises today that are going to ...   0            427  \n",
       "242  And now we're going to make a, it's called a, ...   1            282  \n",
       "358  It should really spring back and feel dry. Thi...   2            335  \n",
       "492  So let's begin with the basics. As you can see...   3            492  \n",
       "71   Hi! A lot of us has started it into the journe...   4            334  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Agrega la ruta donde est√° ubicado Points2VecClass.py\n",
    "sys.path.append('/app/AI-Module/Modules')\n",
    "print(sys.version)\n",
    "from Points2VecClass import Point2Vec\n",
    "def pointsToCnnInputForm(video):\n",
    "    p2v = Point2Vec(4)\n",
    "    return p2v.CNNMatrix(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el formato de points para que sea una lista de 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fee53bec790>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fee53bec790>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "dataset['points'] = dataset['points'].apply(ast.literal_eval)\n",
    "dataset['points']=dataset['points'].apply(pointsToCnnInputForm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datset=dataset.sort_values(by='len_keyframes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar texto de traducciones para que sea compatible con el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Crear el tokenizador\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustar el tokenizador al texto de las traducciones\n",
    "tokenizer.fit_on_texts(df_train['translation'])\n",
    "\n",
    "# Convertir las oraciones en secuencias de enteros\n",
    "df_train['translation_sequence'] = tokenizer.texts_to_sequences(df_train['translation'])\n",
    "df_val['translation_sequence'] = tokenizer.texts_to_sequences(df_val['translation'])\n",
    "df_test['translation_sequence'] = tokenizer.texts_to_sequences(df_test['translation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo datasets de Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(df):\n",
    "    points = tf.ragged.constant(df['points'].values)\n",
    "    sequence = tf.ragged.constant(df['translation_sequence'].values)\n",
    "    return tf.data.Dataset.from_tensor_slices((points, sequence))\n",
    "\n",
    "# train_dataset = create_tf_dataset(df_train)\n",
    "# val_dataset = create_tf_dataset(df_val)\n",
    "test_dataset = create_tf_dataset(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el padding para que las secuencias dentro de los batches tengan la misma longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_batch_fn(batch):\n",
    "    points, sequences = batch\n",
    "    \n",
    "    # Encuentra la longitud m√°xima para los puntos\n",
    "    points_lengths = tf.map_fn(lambda x: tf.shape(x)[0], points, dtype=tf.int32)\n",
    "    max_len_points = tf.reduce_max(points_lengths)\n",
    "    \n",
    "    # Encuentra la longitud m√°xima para las secuencias\n",
    "    sequences_lengths = tf.map_fn(lambda x: tf.shape(x)[0], sequences, dtype=tf.int32)\n",
    "    max_len_sequences = tf.reduce_max(sequences_lengths)\n",
    "    \n",
    "    # Aplica el padding din√°mico a los puntos usando numpy_function\n",
    "    points_padded = tf.numpy_function(\n",
    "        lambda pts: pad_sequences(pts, maxlen=max_len_points, dtype='float32', padding='post'), \n",
    "        [points], tf.float32\n",
    "    )\n",
    "    \n",
    "    # Aplica el padding din√°mico a las secuencias usando numpy_function\n",
    "    sequences_padded = tf.numpy_function(\n",
    "        lambda seqs: pad_sequences(seqs, maxlen=max_len_sequences, dtype='int32', padding='post'),\n",
    "        [sequences], tf.int32\n",
    "    )\n",
    "    \n",
    "    return points_padded, sequences_padded\n",
    "\n",
    "# Funci√≥n principal para mapear y agrupar en batches\n",
    "def pad_batches(dataset):\n",
    "    return dataset.map(pad_batch_fn).batch(32)\n",
    "\n",
    "train_dataset = pad_batches(train_dataset)\n",
    "val_dataset = pad_batches(val_dataset)\n",
    "test_dataset = pad_batches(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscar mejores par√°metros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "num_classes=len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "def ctc_loss(translation, prediction):\n",
    "    \n",
    "    input_length = tf.fill([tf.shape(prediction)[0], 1], tf.shape(prediction)[1])  # Longitud de la secuencia de entrada\n",
    "    label_length = tf.math.count_nonzero(translation, axis=-1, keepdims=True)   # Longitud de la secuencia de traducci√≥n\n",
    "    # Calcular la p√©rdida CTC usando ctc_batch_cost\n",
    "    loss = tf.keras.backend.ctc_batch_cost(translation, prediction, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Definir el modelo para la b√∫squeda de hiperpar√°metros\n",
    "def build_model(hp):\n",
    "\n",
    "    def createCNNkt():\n",
    "\n",
    "        model= models.Sequential()\n",
    "        # A√±adir capas CNN din√°micamente seg√∫n el n√∫mero de capas que se elija (1 a 5 capas CNN)\n",
    "        for i in range(hp.Int('num_cnn_layers', 1, 5)):  # De 1 a 5 capas CNN\n",
    "            model.add(layers.Conv2D(\n",
    "                filters=hp.Int(f'conv_{i+1}_filters', min_value=32, max_value=128, step=32),\n",
    "                kernel_size=(3, 3),\n",
    "                activation='relu',\n",
    "                input_shape=(48, 48, 1) if i == 0 else None))\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        return model\n",
    "\n",
    "    def createLSTMkt():\n",
    "\n",
    "        model= models.Sequential()\n",
    "        # A√±adir capas LSTM din√°micamente seg√∫n el n√∫mero de capas que se elija (1 a 5 capas LSTM)\n",
    "        for i in range(hp.Int('num_lstm_layers', 1, 5)):  # De 1 a 5 capas LSTM\n",
    "            model.add(layers.LSTM(\n",
    "                units=hp.Int(f'lstm_{i+1}_units', min_value=32, max_value=128, step=32),\n",
    "                return_sequences=True))\n",
    "\n",
    "            # A√±adir Dropout para cada capa LSTM\n",
    "            model.add(layers.Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "        return model\n",
    "\n",
    "    cnn = createCNNkt()\n",
    "\n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1))\n",
    "\n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "\n",
    "    lstm=createLSTMkt()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Definir la b√∫squeda de hiperpar√°metros con Keras Tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss', # M√©trica de evaluaci√≥n\n",
    "    max_epochs= 50,\n",
    "    factor=3,\n",
    "    directory='',\n",
    "    project_name='cnn_lstm_hyperparam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar la b√∫squeda de hiperpar√°metros\n",
    "tuner.search(train_dataset, validation_data=val_dataset)\n",
    "\n",
    "# Obtener los mejores hiperpar√°metros encontrados\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear modelo,**EJEMPLO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3)) # A√±adir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Tercera capa LSTM con return_sequences=False\n",
    "    model.add(layers.LSTM(64, return_sequences=False))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    cnn = create_cnn()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1)) \n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='softmax')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaci√≥n para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "# Compilar el modelo\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='RUTA',  # Ruta donde se guarda el modelo\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_bleu',        # M√©trica a monitorear\n",
    "    mode='max',\n",
    "    monitor= 'val_loss',\n",
    "    mode='min'                 # Modo: minimizar la m√©trica monitorizada\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "loss_metric=tf.keras.backend.ctc_batch_cost(from_logits=True, reduction= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainstep(model, points_batch, translation_batch,step, epoch_loss_avg, epoch_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(points_batch, training=True)\n",
    "        loss = tf.reduce_mean(loss_metric(translation_batch, predictions))\n",
    "        \n",
    "    # Calcular y aplicar gradientes\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}: loss = {loss.numpy()}\")\n",
    "    epoch_accuracy.update_state(translation_batch, predictions)\n",
    "    epoch_loss_avg.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valstep(model, points_batch, translation_batch):\n",
    "    val_loss, val_acc = model.evaluate(x=points_batch, y=translation_batch)\n",
    "    print(f\"Validation loss: {val_loss}, Validation accuracy: {val_acc}\")\n",
    "    \n",
    "    # Evaluar en el conjunto de validaci√≥n al final de cada epoch usando BLEU\n",
    "    val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "    val_predictions = model.predict(points_batch)\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "    val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "    print(f\"Validation BLEU: {val_bleu}\")\n",
    "    return val_acc,val_loss,val_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 100\n",
    "\n",
    "# Iterar a trav√©s de los batches y ajustar manualmente los par√°metros\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    for step, (points_batch, translation_batch) in enumerate(train_dataset):\n",
    "        trainstep(model, points_batch,translation_batch, step, epoch_loss_avg, epoch_accuracy)\n",
    "    \n",
    "    \n",
    "    for step, (points_batch,translation_batch) in enumerate(val_dataset):\n",
    "        val_acc, val_loss, val_bleu = valstep(model, points_batch, translation_batch)\n",
    "\n",
    "    checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu})\n",
    "\n",
    "    if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu}):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "\n",
    "# Evaluar el modelo en el dataset de test\n",
    "#test_loss, test_acc = model.evaluate(test_dataset)\n",
    "#print(f'Test accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
