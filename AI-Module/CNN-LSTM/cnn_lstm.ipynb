{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728582338.723903       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728582338.810788       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728582338.810894       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "gpus=tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2=pd.read_csv('./AI-Module/Resources/Datasets/how2sign2.csv')\n",
    "dataset = pd.read_csv(\"./AI-Module/Resources/Datasets/how2sign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntos_folder = \"/app/AI-Module/Resources/Datasets/Points\"\n",
    "files = [puntos_folder + \"/\" + file for file in os.listdir(puntos_folder)]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(files):\n",
    "    for file in files:\n",
    "        item_with_index = np.load(file, allow_pickle=True)\n",
    "        # Convertir los arrays a float32 para reducir memoria\n",
    "        item = item_with_index[0].astype(np.float32)\n",
    "        index = item_with_index[1]\n",
    "        yield item, index\n",
    "\n",
    "puntos_list = []\n",
    "ids_list = []\n",
    "\n",
    "for item, index in load_points(files):\n",
    "    puntos_list.append(item)\n",
    "    ids_list.append(index)\n",
    "\n",
    "df_puntos = pd.DataFrame({\n",
    "    'points': puntos_list,\n",
    "    'id': ids_list\n",
    "})\n",
    "df_puntos = df_puntos.sort_values([\"id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(df_puntos, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points_x</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "      <th>points_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>If you like to find more about my services you...</td>\n",
       "      <td>448</td>\n",
       "      <td>817</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>Hello welcome my name is Julio Nutt and I am a...</td>\n",
       "      <td>210</td>\n",
       "      <td>401</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "      <td>All right, the drink we're about to make is ca...</td>\n",
       "      <td>228</td>\n",
       "      <td>308</td>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "      <td>Okay, another thing that for me is important. ...</td>\n",
       "      <td>187</td>\n",
       "      <td>367</td>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>All right, we're ready to start our fire. What...</td>\n",
       "      <td>99</td>\n",
       "      <td>453</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            points_x  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...   \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...   \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "\n",
       "                                         translation   id  len_keyframes  \\\n",
       "0  If you like to find more about my services you...  448            817   \n",
       "1  Hello welcome my name is Julio Nutt and I am a...  210            401   \n",
       "2  All right, the drink we're about to make is ca...  228            308   \n",
       "3  Okay, another thing that for me is important. ...  187            367   \n",
       "4  All right, we're ready to start our fire. What...   99            453   \n",
       "\n",
       "                                            points_y  \n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...  \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...  \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.    , -1.    , -1.    , ...,  0.3154,  0.4075,  1.    ],\n",
       "       [-1.    , -1.    , -1.    , ...,  0.2917,  0.4159,  1.    ],\n",
       "       [ 1.    ,  0.0773,  1.    , ...,  0.3163,  0.4409,  1.    ],\n",
       "       ...,\n",
       "       [ 1.    ,  0.1223,  0.9016, ...,  0.3382,  0.3776,  1.    ],\n",
       "       [ 1.    ,  0.1332,  0.8529, ...,  0.3337,  0.364 ,  1.    ],\n",
       "       [ 1.    ,  0.1269,  0.7338, ...,  0.338 ,  0.3777,  1.    ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"points_y\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"points\"]=dataset[\"points_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puntos_folder = \"/app/AI-Module/Resources/Datasets/Points2\"\n",
    "files = [puntos_folder + \"/\" + file for file in os.listdir(puntos_folder)]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(files):\n",
    "    for file in files:\n",
    "        item_with_index = np.load(file, allow_pickle=True)\n",
    "        # Convertir los arrays a float32 para reducir memoria\n",
    "        item = item_with_index[0].astype(np.float32)\n",
    "        index = item_with_index[1]\n",
    "        yield item, index\n",
    "\n",
    "puntos_list = []\n",
    "ids_list = []\n",
    "\n",
    "for item, index in load_points(files):\n",
    "    puntos_list.append(item)\n",
    "    ids_list.append(index)\n",
    "\n",
    "df_puntos = pd.DataFrame({\n",
    "    'points': puntos_list,\n",
    "    'id': ids_list\n",
    "})\n",
    "df_puntos = df_puntos.sort_values([\"id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = dataset_2.merge(df_puntos, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el formato de points para que sea una lista de 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# # Agrega la ruta donde está ubicado Points2VecClass.py\n",
    "# sys.path.append('/app/AI-Module/Modules')\n",
    "# from Points2VecClass import Point2Vec\n",
    "# def pointsToCnnInputForm(points):\n",
    "#     p2v = Point2Vec(4)\n",
    "#     return p2v.CNNMatrix(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_cnn_input_form(serie):\n",
    "#     for index, element in enumerate(serie):\n",
    "#         element= pointsToCnnInputForm(element)\n",
    "#         serie.at[index]=element\n",
    "#     return serie\n",
    "# dataset['points']=to_cnn_input_form(dataset['points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizo las traducciones para que sean compatibles con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Cargar el archivo JSON\n",
    "with open('./tokenizer_entero.json') as json_file:\n",
    "    tokenizer_json = json.load(json_file)\n",
    "\n",
    "# Reconstruir el tokenizer desde el archivo JSON\n",
    "tokenizer = tokenizer_from_json(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# # Crear el tokenizador\n",
    "# tokenizer = Tokenizer()\n",
    "\n",
    "# # Ajustar el tokenizador al texto de las traducciones\n",
    "# tokenizer.fit_on_texts(dataset['translation'])\n",
    "\n",
    "# # Convertir las oraciones en secuencias de enteros\n",
    "dataset['translation'] = tokenizer.texts_to_sequences(dataset['translation'])\n",
    "dataset_2['translation']=tokenizer.texts_to_sequence(dataset['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_val size: 400, Test size: 100\n",
      "Train size: 300, Val size: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Divide en train y test\n",
    "points_train_val, points_test, translation_train_val, translation_test = train_test_split(\n",
    "    dataset[[\"points\", \"len_keyframes\"]], dataset[\"translation\"], test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Train_val size: {len(points_train_val)}, Test size: {len(points_test)}\")\n",
    "points_train, points_val, translation_train, translation_val=train_test_split(\n",
    "    points_train_val,translation_train_val, test_size=0.25,random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(points_train)}, Val size: {len(points_val)}\")\n",
    "train = {\n",
    "    \"points\": points_train[\"points\"],\n",
    "    \"len_keyframes\": points_train[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_train\n",
    "}\n",
    "\n",
    "val = {\n",
    "    \"points\": points_val[\"points\"],\n",
    "    \"len_keyframes\": points_val[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_val\n",
    "}\n",
    "\n",
    "test = {\n",
    "    \"points\": points_test[\"points\"],\n",
    "    \"len_keyframes\": points_test[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_test\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_train, points_val, translation_train, translation_val = train_test_split(\n",
    "    dataset_2[[\"points\", \"len_keyframes\"]], dataset_2[\"translation\"], test_size=0.15, random_state=42\n",
    ")\n",
    "print(f\"Train_val size: {len(points_train_val)}, Test size: {len(points_test)}\")\n",
    "\n",
    "train2 = {\n",
    "    \"points\": points_train[\"points\"],\n",
    "    \"len_keyframes\": points_train[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_train\n",
    "}\n",
    "\n",
    "val2 = {\n",
    "    \"points\": points_val[\"points\"],\n",
    "    \"len_keyframes\": points_val[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "globals().pop('dataset', None)\n",
    "globals().pop('puntos_folder', None)\n",
    "globals().pop('puntos_list', None)\n",
    "globals().pop('df_puntos', None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['translation_sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo datasets de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(df):\n",
    "    def generator():\n",
    "        for i in range(len(df['points'])):\n",
    "            points = df['points'].values[i]\n",
    "            sequence = df['translation_sequence'].values[i]\n",
    "            yield points, sequence\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            # tf.TensorSpec(shape=(None, 48, 48), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 2172), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None), dtype=tf.int32)\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728582509.321837       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728582509.322024       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728582509.322070       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728582515.248880       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728582515.248992       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-10 17:48:35.249016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-10 17:48:35.249080: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:198] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1728582515.249752       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-10 17:48:35.259282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9355 MB memory:  -> device: 0, name: NVIDIA RTX 3500 Ada Generation Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "test_dataset = create_tf_dataset(test)\n",
    "val_dataset=create_tf_dataset(val)\n",
    "train_dataset= create_tf_dataset(train)\n",
    "train_dataset_2=create_tf_dataset(train2)\n",
    "val_dataset_2=create_tf_dataset(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().pop('test', None)\n",
    "globals().pop('train', None)\n",
    "globals().pop('val', None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.shuffle(buffer_size=10000)\n",
    "val_dataset=val_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset=test_dataset.shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save('./train')\n",
    "val_dataset.save('./val')\n",
    "test_dataset.save('./test')\n",
    "train_dataset_2.save('./train2')\n",
    "val_dataset_2.save('./val2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = train_dataset.reduce(0, lambda x, _: x + 1)\n",
    "count.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 2172), dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(268, 2172), dtype=float32, numpy=\n",
      "array([[1.    , 0.097 , 0.6709, ..., 0.3187, 0.3373, 1.    ],\n",
      "       [1.    , 0.1203, 0.7383, ..., 0.3126, 0.3504, 1.    ],\n",
      "       [1.    , 0.104 , 0.7192, ..., 0.3115, 0.3603, 1.    ],\n",
      "       ...,\n",
      "       [1.    , 0.6054, 1.    , ..., 0.3259, 0.3816, 1.    ],\n",
      "       [1.    , 0.5996, 1.    , ..., 0.3262, 0.3763, 1.    ],\n",
      "       [1.    , 0.5957, 1.    , ..., 0.324 , 0.3744, 1.    ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(200,), dtype=int32, numpy=\n",
      "array([  14,   41,   19,   59, 3203,   92,    8,    4,   86,    2,  104,\n",
      "         11,  572,   53,   33,  140, 1911,   27,    9,    7, 1337, 6602,\n",
      "         38,   22, 1159,   41,   19,  645,    6, 6603,  101,   10,  573,\n",
      "          7,   50,   12,    2, 6604,  234,  111,  329,    4,  376,   31,\n",
      "          5,  248,  210,    9,  542,  233,   32,    8,   22,   95,  904,\n",
      "          6,   91,   14,   23,  929,    6,    8,   45,   82,    8,   10,\n",
      "        573,    7,   50,   11,    5, 6605,  527,    6,    5, 1139,   27,\n",
      "          4,   19,  140,    8,   27,  105,  523,   19,  549,   28,   27,\n",
      "         47,   19, 2338,   29,   12,    2,   23, 1309,   22,   71,    2,\n",
      "         26,    1, 1911,    3, 3584,    4,   25,   78,    2,   23,    5,\n",
      "       2046,    2,   10,  573,    4,   64,   25,    2,   23, 2610,   10,\n",
      "        573, 2529,   28, 6606,   28,  313,    5,  573,    2, 6607,    4,\n",
      "         22,    1,  123,    8,    4,   19,  281,    2,   26,   22,   71,\n",
      "         14,  841,  966,    2,   56,   10,  573,    7,  811,  841,  966,\n",
      "          2,   35,   10,  573,    7, 2021,    4,  841,  966,    2,  105,\n",
      "       6608,    3,   45,   82,    8,    4,   19,   11,    5,  559,    6,\n",
      "        572,    2,   26,    1,  123,   20,   10,  573, 2520,  405,  200,\n",
      "       6609,   28,   21,   50,  242, 1233,   11,    1,  123,    8,    4,\n",
      "         19,  140], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 14:22:52.542883: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for element in train_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: test\n",
      "Valores entre 0 y 300: 17\n",
      "Valores entre 300 y 500: 52\n",
      "Valores entre 500 y 700: 21\n",
      "Valores entre 700 y 1000: 7\n",
      "Dataset: train\n",
      "Valores entre 0 y 300: 57\n",
      "Valores entre 300 y 500: 154\n",
      "Valores entre 500 y 700: 55\n",
      "Valores entre 700 y 1000: 24\n",
      "Dataset: val\n",
      "Valores entre 0 y 300: 18\n",
      "Valores entre 300 y 500: 51\n",
      "Valores entre 500 y 700: 20\n",
      "Valores entre 700 y 1000: 11\n"
     ]
    }
   ],
   "source": [
    "def print_lenForBucketing(ds,limites):\n",
    "\n",
    "    # Crear un DataFrame vacío para almacenar los resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Recorrer los pares consecutivos de la lista de límites\n",
    "    for lower, upper in zip(limites[:-1], limites[1:]):\n",
    "        # Contar cuántos valores están entre 'lower' y 'upper'\n",
    "        count = ((ds['len_keyframes'] > lower) & (ds['len_keyframes'] <= upper)).sum()\n",
    "        \n",
    "        # Guardar el resultado en una lista\n",
    "        resultados.append((lower, upper, count))\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    for lower, upper, count in resultados:\n",
    "        print(f\"Valores entre {lower} y {upper}: {count}\")\n",
    "\n",
    "\n",
    "limites = [0,300,500,700, 1000]\n",
    "print(f'Dataset: test')\n",
    "print_lenForBucketing(test,limites)\n",
    "print(f'Dataset: train')\n",
    "print_lenForBucketing(train,limites)\n",
    "print(f'Dataset: val')\n",
    "\n",
    "print_lenForBucketing(val,limites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FlatMapDataset element_spec=(TensorSpec(shape=(None, 48, 48), dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_fn(points, translations):\n",
    "    # La longitud será la cantidad de keyframes (primera dimensión de los puntos)\n",
    "    longitud=tf.shape(points)[0]\n",
    "    return longitud\n",
    "\n",
    "\n",
    "# limites = [300,500,700,1100]\n",
    "    \n",
    "# bucket_batch_sizes = [32, 32, 16, 8,8]\n",
    "\n",
    "def bucketed_dataset(dataset):\n",
    "    limites = [300,500,700,1100]\n",
    "    \n",
    "    bucket_batch_sizes = [8, 8, 8, 4, 4]\n",
    "    return dataset.bucket_by_sequence_length(\n",
    "            element_length_func=length_fn,       \n",
    "            bucket_boundaries=limites, # Límites de los buckets\n",
    "            bucket_batch_sizes=bucket_batch_sizes,  # Tamaños de batch para cada bucket\n",
    "            padded_shapes=(\n",
    "                # [None,48,48],\n",
    "                [None, 2172],  \n",
    "                [None]           \n",
    "            ),\n",
    "            padding_values=(\n",
    "                tf.constant(0, dtype=tf.float32), # Relleno con ceros para los puntos \n",
    "                tf.constant(0, dtype=tf.int32) # Relleno con ceros para las traducciones \n",
    "            )\n",
    "    )\n",
    "\n",
    "# Aplicar bucketing y padding\n",
    "train_dataset= train_dataset.apply(bucketed_dataset)\n",
    "val_dataset= val_dataset.apply(bucketed_dataset)\n",
    "test_dataset= test_dataset.apply(bucketed_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.shuffle(buffer_size=100)\n",
    "val_dataset=val_dataset.shuffle(buffer_size=100)\n",
    "test_dataset=test_dataset.shuffle(buffer_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 17:49:12.013961: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de batches en train: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 17:49:12.483850: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de batches en val: 16\n",
      "Cantidad de batches en test: 16\n"
     ]
    }
   ],
   "source": [
    "def count_batches(dataset):\n",
    "    batch_count = 0\n",
    "    for _ in dataset:\n",
    "        batch_count += 1\n",
    "    return batch_count\n",
    "print(f\"Cantidad de batches en train: {count_batches(train_dataset)}\")\n",
    "print(f\"Cantidad de batches en val: {count_batches(val_dataset)}\")\n",
    "print(f\"Cantidad de batches en test: {count_batches(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_loss(translation, prediction):\n",
    "    \n",
    "#     input_length = tf.fill([tf.shape(prediction)[0], 1], tf.shape(prediction)[1])  # Longitud de la secuencia de entrada\n",
    "#     label_length = tf.math.count_nonzero(translation, axis=-1, keepdims=True)   # Longitud de la secuencia de traducción\n",
    "#     # Calcular la pérdida CTC usando ctc_batch_cost\n",
    "#     loss = tf.keras.backend.ctc_batch_cost(translation, prediction, input_length, label_length)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscar mejores parámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_tuner as kt\n",
    "# num_classes=len(tokenizer.word_index) + 1\n",
    "\n",
    "# # Definir el modelo para la búsqueda de hiperparámetros\n",
    "# def build_model(hp):\n",
    "\n",
    "#     def createCNNkt():\n",
    "\n",
    "#         model= models.Sequential()\n",
    "#         # Añadir capas CNN dinámicamente según el número de capas que se elija (1 a 5 capas CNN)\n",
    "#         for i in range(hp.Int('num_cnn_layers', 2, 5)):  # De 1 a 5 capas CNN\n",
    "#             model.add(layers.Conv2D(\n",
    "#                 filters=hp.Int(f'conv_{i+1}_filters', min_value=64, max_value=256, step=32),\n",
    "#                 kernel_size=(3, 3),\n",
    "#                 activation='relu',\n",
    "#                 input_shape=(48, 48, 1) if i == 0 else None))\n",
    "#             model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "#         model.add(layers.Flatten())\n",
    "#         return model\n",
    "\n",
    "#     def createLSTMkt():\n",
    "\n",
    "#         model= models.Sequential()\n",
    "#         # Añadir capas LSTM dinámicamente según el número de capas que se elija (1 a 5 capas LSTM)\n",
    "#         for i in range(hp.Int('num_lstm_layers', 2, 5)):  # De 1 a 5 capas LSTM\n",
    "#             model.add(layers.LSTM(\n",
    "#                 units=hp.Int(f'lstm_{i+1}_units', min_value=64, max_value=256, step=32),\n",
    "#                 return_sequences=True))\n",
    "\n",
    "#             # Añadir Dropout para cada capa LSTM\n",
    "#             model.add(layers.Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.4, step=0.1)))\n",
    "#         return model\n",
    "\n",
    "#     cnn = createCNNkt()\n",
    "\n",
    "#     video_input = layers.Input(shape=(None, 48, 48, 1))\n",
    "\n",
    "#     # Aplicar CNN a cada frame usando TimeDistributed\n",
    "#     cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "\n",
    "#     lstm=createLSTMkt()\n",
    "#     lstm_out= lstm(cnn_features)\n",
    "#     # Capa final de salida\n",
    "#     output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "\n",
    "#     # Compilar el modelo\n",
    "#     model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "#     model = models.Model(inputs=video_input, outputs=output)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Definir la búsqueda de hiperparámetros con Keras Tuner\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_loss', # Métrica de evaluación\n",
    "#     max_epochs= 50,\n",
    "#     factor=3,\n",
    "#     directory='',\n",
    "#     project_name='cnn_lstm_hyperparam'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Realizar la búsqueda de hiperparámetros\n",
    "# tuner.search(train_dataset, validation_data=val_dataset)\n",
    "\n",
    "# best_model=tuner.get_best_model(1)[0]\n",
    "# # Obtener los mejores hiperparámetros encontrados\n",
    "# best_hyperparams = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crear Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_cnn():\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.Input(shape=(48, 48, 1)))\n",
    "#     model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Flatten())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(2172, 1)))\n",
    "    model.add(layers.Conv1D(64, kernel_size=5, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_example_model = create_conv1d()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "    model.add(layers.Dropout(0.3)) # Añadir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    conv1d = create_conv1d()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 2172, 1)) \n",
    "    # masked_input = layers.Masking(mask_value=0.0)(video_input)\n",
    "\n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(conv1d)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.8/792.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.9.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(tokenizer.word_index) + 1\n",
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/best_model.keras',\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_loss',\n",
    "    mode='min'  \n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 10 17:49:33 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   45C    P8              11W /  35W |    567MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A         9      C   /python3.11                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss=tf.keras.losses.CTC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions,greedy):\n",
    "    predicted_classes = tf.argmax(predictions, axis=-1)\n",
    "    mask = tf.not_equal(predicted_classes, 0)\n",
    "    seq_lengths = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-1)\n",
    "    \n",
    "    predictions=tf.transpose(predictions,[1,0,2])\n",
    "\n",
    "    if greedy:\n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    \n",
    "    else:\n",
    "        decoded, _ = tf.nn.ctc_beam_search_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    decoded_dense = tf.sparse.to_dense(decoded[0], default_value=-1).numpy()\n",
    "    \n",
    "    # Extraer las secuencias decodificadas eliminando los -1 (valores de relleno)\n",
    "    decoded_sequences = []\n",
    "    for seq in decoded_dense:\n",
    "        decoded_sequences.append([val for val in seq if val != -1])\n",
    "    \n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7664\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainstep(model, points_batch, translation_batch,step, epoch_loss_avg, epoch_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(points_batch, training=True)\n",
    "        loss = tf.reduce_mean(ctc_loss(translation_batch, predictions))\n",
    "        \n",
    "    # Calcular y aplicar gradientes\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # if step % 50 == 0:\n",
    "    #     print(f\"Step {step}: loss = {loss.numpy()}\")\n",
    "    predictions_decoded=decode(predictions, greedy=True)\n",
    "    epoch_accuracy.update_state(translation_batch, predictions_decoded)\n",
    "    epoch_loss_avg.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg):\n",
    "    # Evaluar en el conjunto de validación al final de cada epoch usando BLEU\n",
    "   \n",
    "    predictions = model(points_batch, training=False)\n",
    "\n",
    "    val_loss=tf.reduce_mean(ctc_loss(translation_batch,predictions))\n",
    "    val_loss_avg.update_state(val_loss)\n",
    "\n",
    "    val_predictions = decode(predictions, greedy=False)\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in val_predictions]\n",
    "\n",
    "\n",
    "    val_acc_metric.update_state(translation_batch,val_predictions)\n",
    "    val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "    val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "    val_bleu_avg.update_state(val_bleu)\n",
    "    return val_acc_metric.result(), val_loss_avg.result(), val_bleu_avg.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save('./train')\n",
    "val_dataset.save('./val')\n",
    "test_dataset.save('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 10 17:49:50 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   45C    P8              N/A /  35W |    567MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A         9      C   /python3.11                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=tf.data.Dataset.load('/train')\n",
    "val_dataset=tf.data.Dataset.load('/val')\n",
    "test_dataset=tf.data.Dataset.load('/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 10 17:53:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   44C    P8               3W /  35W |    567MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A         9      C   /python3.11                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0batch [00:01, ?batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([8, 297, 2172]), TensorShape([8, 231]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "points_batch, translation_batch = next(iter(train_dataset))\n",
    "points_batch.shape, translation_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 10 14:47:57 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   45C    P8               4W /  35W |  10913MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A         9      C   /python3.11                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba)\n",
      "  Downloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (1.26.4)\n",
      "Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.43.0 numba-0.60.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 14:40:53.808555: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:310] gpu_async_0 cuMemAllocAsync failed to allocate 29687025152 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 0/12878086144\n",
      "2024-10-10 14:40:53.808609: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:315] Stats: Limit:                      9809428480\n",
      "InUse:                      9869051535\n",
      "MaxInUse:                  10913235449\n",
      "NumAllocs:                       21964\n",
      "MaxAllocSize:               1625704544\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-10-10 14:40:53.808733: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:64] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-10-10 14:40:53.808739: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1, 3\n",
      "2024-10-10 14:40:53.808742: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4, 24\n",
      "2024-10-10 14:40:53.808745: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 8, 8\n",
      "2024-10-10 14:40:53.808747: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 32, 1\n",
      "2024-10-10 14:40:53.808749: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 40, 3\n",
      "2024-10-10 14:40:53.808752: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 256, 18\n",
      "2024-10-10 14:40:53.808754: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 512, 8\n",
      "2024-10-10 14:40:53.808756: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1024, 4\n",
      "2024-10-10 14:40:53.808759: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1028, 1\n",
      "2024-10-10 14:40:53.808762: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1280, 1\n",
      "2024-10-10 14:40:53.808764: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2048, 10\n",
      "2024-10-10 14:40:53.808767: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4096, 4\n",
      "2024-10-10 14:40:53.808769: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 7840, 2\n",
      "2024-10-10 14:40:53.808772: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 8864, 3\n",
      "2024-10-10 14:40:53.808774: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 10688, 2\n",
      "2024-10-10 14:40:53.808776: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 13984, 1\n",
      "2024-10-10 14:40:53.808779: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 16384, 8\n",
      "2024-10-10 14:40:53.808781: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 30656, 1\n",
      "2024-10-10 14:40:53.808784: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 49152, 1\n",
      "2024-10-10 14:40:53.808786: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 65536, 10\n",
      "2024-10-10 14:40:53.808788: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 262144, 2\n",
      "2024-10-10 14:40:53.808791: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 447488, 2\n",
      "2024-10-10 14:40:53.808793: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 528384, 1\n",
      "2024-10-10 14:40:53.808795: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 894976, 3\n",
      "2024-10-10 14:40:53.808798: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1107968, 437\n",
      "2024-10-10 14:40:53.808800: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1789952, 6\n",
      "2024-10-10 14:40:53.808802: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2215936, 874\n",
      "2024-10-10 14:40:53.808805: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2220032, 437\n",
      "2024-10-10 14:40:53.808809: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 3887552, 1\n",
      "2024-10-10 14:40:53.808837: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 3923968, 1\n",
      "2024-10-10 14:40:53.808839: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4440064, 874\n",
      "2024-10-10 14:40:53.808842: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4474896, 2\n",
      "2024-10-10 14:40:53.808844: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 5092000, 1\n",
      "2024-10-10 14:40:53.808847: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 6693312, 1\n",
      "2024-10-10 14:40:53.808849: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 8863744, 8\n",
      "2024-10-10 14:40:53.808852: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 8949776, 1\n",
      "2024-10-10 14:40:53.808854: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 30373248, 2\n",
      "2024-10-10 14:40:53.808857: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 35454976, 2\n",
      "2024-10-10 14:40:53.808859: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 35522560, 2\n",
      "2024-10-10 14:40:53.808886: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 60085760, 1\n",
      "2024-10-10 14:40:53.808888: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 67933696, 1\n",
      "2024-10-10 14:40:53.808891: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 81912832, 1\n",
      "2024-10-10 14:40:53.808893: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 104475648, 3\n",
      "2024-10-10 14:40:53.808897: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 107173376, 4\n",
      "2024-10-10 14:40:53.808899: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 116492800, 3\n",
      "2024-10-10 14:40:53.808902: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 484182016, 2\n",
      "2024-10-10 14:40:53.808907: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:98] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 11844714496\n",
      "2024-10-10 14:40:53.808911: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 9869051535\n",
      "2024-10-10 14:40:53.808913: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 14092861440\n",
      "2024-10-10 14:40:53.808916: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:102] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 10913235449\n",
      "2024-10-10 14:40:53.808937: W tensorflow/core/framework/op_kernel.cc:1828] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     21\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(points_batch, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslation_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Calcular y aplicar gradientes\u001b[39;00m\n\u001b[1;32m     25\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/loss.py:43\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_pred\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_true\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(losses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:27\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[1;32m     26\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:2074\u001b[0m, in \u001b[0;36mctc\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   2069\u001b[0m input_length \u001b[38;5;241m=\u001b[39m input_length \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mones((batch_length,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2070\u001b[0m label_length \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(\n\u001b[1;32m   2071\u001b[0m     ops\u001b[38;5;241m.\u001b[39msum(y_true \u001b[38;5;241m!=\u001b[39m mask_index, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2072\u001b[0m )\n\u001b[0;32m-> 2074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_index\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:1871\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(target, output, target_length, output_length, mask_index)\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((target, output, target_length, output_length)):\n\u001b[1;32m   1868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CTCLoss(mask_index)\u001b[38;5;241m.\u001b[39msymbolic_call(\n\u001b[1;32m   1869\u001b[0m         target, output, target_length, output_length\n\u001b[1;32m   1870\u001b[0m     )\n\u001b[0;32m-> 1871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_index\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py:799\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(target, output, target_length, output_length, mask_index)\u001b[0m\n\u001b[1;32m    797\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m result_dtype\n\u001b[1;32m    798\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(output, compute_dtype)\n\u001b[0;32m--> 799\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblank_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_time_major\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(loss, result_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 100\n",
    "model.compile(optimizer=optimizador, loss=tf.keras.losses.CTC(), metrics=[calculate_bleu])\n",
    "steps_per_epoch = len(train_dataset)\n",
    "validation_steps = len(val_dataset)\n",
    "\n",
    "# Iterar a través de los batches\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(total=steps_per_epoch, desc=f'Epoch {epoch+1}/{epochs}', unit='batch') as pbar:\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "\n",
    "        for step, (points_batch, translation_batch) in enumerate(train_dataset):\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": epoch_loss_avg.result().numpy(),\n",
    "                \"Accuracy\": epoch_accuracy.result().numpy()\n",
    "            })\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(points_batch, training=True)\n",
    "                loss = tf.reduce_mean(ctc_loss(translation_batch, predictions))\n",
    "                \n",
    "            # Calcular y aplicar gradientes\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            predictions_decoded=decode(predictions, greedy=True)\n",
    "            epoch_accuracy.update_state(translation_batch, predictions_decoded)\n",
    "            epoch_loss_avg.update_state(loss)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Métricas para la validación\n",
    "        val_loss_avg = tf.keras.metrics.Mean()\n",
    "        val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        val_bleu_avg = tf.keras.metrics.Mean()\n",
    "        with tqdm(total=validation_steps, desc=f'Epoch {epoch+1}/{epochs} (Validation)', unit='batch') as pbar_val:\n",
    "\n",
    "            for step, (points_batch, translation_batch) in enumerate(val_dataset):\n",
    "                \n",
    "                print(\"a\")\n",
    "                pbar_val.set_postfix({\n",
    "                    \"Val Loss\": val_loss_avg.result().numpy(),\n",
    "                    \"Val Accuracy\": val_acc_metric.result().numpy(),\n",
    "                    \"Val BLEU\": val_bleu_avg.result().numpy()\n",
    "                })\n",
    "                \n",
    "                # Evaluar en el conjunto de validación al final de cada epoch usando BLEU\n",
    "                predictions = model(points_batch, training=False)\n",
    "\n",
    "                val_loss=tf.reduce_mean(ctc_loss(translation_batch,predictions))\n",
    "                val_loss_avg.update_state(val_loss)\n",
    "\n",
    "                val_predictions = decode(predictions, greedy=False)\n",
    "                val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in val_predictions]\n",
    "\n",
    "\n",
    "                val_acc_metric.update_state(translation_batch,val_predictions)\n",
    "                val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "                val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "                val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "                val_bleu_avg.update_state(val_bleu)\n",
    "            \n",
    "            \n",
    "\n",
    "        checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss_avg, 'val_accuracy': val_acc_metric, 'val_bleu': val_bleu_avg})\n",
    "\n",
    "        if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss_avg, 'val_accuracy': val_acc_metric, 'val_bleu': val_bleu_avg}):\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "        print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "        print(f'Epoch: {epoch}, val_loss: {val_loss_avg}, val_acc: {val_acc_metric}, val_bleu: {val_bleu_avg}')\n",
    "\n",
    "    # Evaluar el modelo en el dataset de test\n",
    "    #test_loss, test_acc = model.evaluate(test_dataset)\n",
    "    #print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
