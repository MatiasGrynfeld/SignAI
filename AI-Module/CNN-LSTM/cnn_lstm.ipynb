{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 13:56:24.975550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-08 13:56:25.101077: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-08 13:56:25.132419: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-08 13:56:25.425742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728395794.684240       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728395794.809159       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728395794.809225       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntos_folder = \"/app/AI-Module/Resources/Datasets/Points\"\n",
    "files = [puntos_folder + \"/\" + file for file in os.listdir(puntos_folder)]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(files):\n",
    "    for file in files:\n",
    "        item_with_index = np.load(file, allow_pickle=True)\n",
    "        # Convertir los arrays a float32 para reducir memoria\n",
    "        item = item_with_index[0].astype(np.float32)\n",
    "        index = item_with_index[1]\n",
    "        yield item, index\n",
    "\n",
    "puntos_list = []\n",
    "ids_list = []\n",
    "\n",
    "for item, index in load_points(files):\n",
    "    puntos_list.append(item)\n",
    "    ids_list.append(index)\n",
    "\n",
    "df_puntos = pd.DataFrame({\n",
    "    'points': puntos_list,\n",
    "    'id': ids_list\n",
    "})\n",
    "df_puntos = df_puntos.sort_values([\"id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(df_puntos, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points_x</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "      <th>points_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>If you like to find more about my services you...</td>\n",
       "      <td>448</td>\n",
       "      <td>817</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>Hello welcome my name is Julio Nutt and I am a...</td>\n",
       "      <td>210</td>\n",
       "      <td>401</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "      <td>All right, the drink we're about to make is ca...</td>\n",
       "      <td>228</td>\n",
       "      <td>308</td>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "      <td>Okay, another thing that for me is important. ...</td>\n",
       "      <td>187</td>\n",
       "      <td>367</td>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>All right, we're ready to start our fire. What...</td>\n",
       "      <td>99</td>\n",
       "      <td>453</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            points_x  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...   \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...   \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "\n",
       "                                         translation   id  len_keyframes  \\\n",
       "0  If you like to find more about my services you...  448            817   \n",
       "1  Hello welcome my name is Julio Nutt and I am a...  210            401   \n",
       "2  All right, the drink we're about to make is ca...  228            308   \n",
       "3  Okay, another thing that for me is important. ...  187            367   \n",
       "4  All right, we're ready to start our fire. What...   99            453   \n",
       "\n",
       "                                            points_y  \n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...  \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...  \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"points_y\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"points\"]=dataset[\"points_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el formato de points para que sea una lista de 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Agrega la ruta donde est√° ubicado Points2VecClass.py\n",
    "sys.path.append('/app/AI-Module/Modules')\n",
    "from Points2VecClass import Point2Vec\n",
    "def pointsToCnnInputForm(points):\n",
    "    p2v = Point2Vec(4)\n",
    "    return p2v.CNNMatrix(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cnn_input_form(serie):\n",
    "    for index, element in enumerate(serie):\n",
    "        element= pointsToCnnInputForm(element)\n",
    "        serie.at[index]=element\n",
    "    return serie\n",
    "dataset['points']=to_cnn_input_form(dataset['points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizo las traducciones para que sean compatibles con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Crear el tokenizador\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustar el tokenizador al texto de las traducciones\n",
    "tokenizer.fit_on_texts(dataset['translation'])\n",
    "\n",
    "# Convertir las oraciones en secuencias de enteros\n",
    "dataset['translation'] = tokenizer.texts_to_sequences(dataset['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_val size: 400, Test size: 100\n",
      "Train size: 300, Test size: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Divide en train y test\n",
    "points_train_val, points_test, translation_train_val, translation_test = train_test_split(\n",
    "    dataset[[\"points\", \"len_keyframes\"]], dataset[\"translation\"], test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Train_val size: {len(points_train_val)}, Test size: {len(points_test)}\")\n",
    "points_train, points_val, translation_train, translation_val=train_test_split(\n",
    "    points_train_val,translation_train_val, test_size=0.25,random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(points_train)}, Test size: {len(points_val)}\")\n",
    "train = {\n",
    "    \"points\": points_train[\"points\"],\n",
    "    \"len_keyframes\": points_train[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_train\n",
    "}\n",
    "\n",
    "val = {\n",
    "    \"points\": points_val[\"points\"],\n",
    "    \"len_keyframes\": points_val[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_val\n",
    "}\n",
    "\n",
    "test = {\n",
    "    \"points\": points_test[\"points\"],\n",
    "    \"len_keyframes\": points_test[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_test\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "globals().pop('dataset', None)\n",
    "globals().pop('puntos_folder', None)\n",
    "globals().pop('puntos_list', None)\n",
    "globals().pop('df_puntos', None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['translation_sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo datasets de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(df):\n",
    "    def generator():\n",
    "        for i in range(len(df['points'])):\n",
    "            points = df['points'].values[i]\n",
    "            sequence = df['translation_sequence'].values[i]\n",
    "            yield points, sequence\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 48, 48), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None), dtype=tf.int32)\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728396095.667180       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728396095.667375       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728396095.667446       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728396102.289023       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728396102.289121       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-08 14:01:42.289134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-08 14:01:42.289174: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:198] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1728396102.290614       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-08 14:01:42.291342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9355 MB memory:  -> device: 0, name: NVIDIA RTX 3500 Ada Generation Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "test_dataset = create_tf_dataset(test)\n",
    "val_dataset=create_tf_dataset(val)\n",
    "train_dataset= create_tf_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = train_dataset.reduce(0, lambda x, _: x + 1)\n",
    "count.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 48, 48), dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1133, 48, 48), dtype=float32, numpy=\n",
      "array([[[ 1.    ,  0.6049,  1.    , ...,  0.7828,  0.3171,  1.    ],\n",
      "        [ 0.308 ,  0.7701,  0.2518, ...,  0.198 ,  0.5576,  1.    ],\n",
      "        [ 0.811 ,  0.109 ,  0.3641, ...,  0.7605,  0.3062,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4812,  0.3349,  0.2868, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[ 1.    ,  0.5889,  1.    , ...,  0.7625,  0.3176,  1.    ],\n",
      "        [ 0.0208,  0.7787,  0.2834, ...,  0.1822,  0.5087,  1.    ],\n",
      "        [ 0.7561,  0.1014,  0.2969, ...,  0.7796,  0.3118,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4853,  0.3326,  0.2908, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[ 1.    ,  0.5205,  1.    , ...,  0.7323,  0.2591,  1.    ],\n",
      "        [ 0.0196,  0.7315,  0.2167, ...,  0.2206,  0.4395,  1.    ],\n",
      "        [ 0.7376,  0.1138,  0.2324, ...,  0.8485,  0.4687,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4757,  0.3234,  0.2857, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.5452,  0.8084,  1.    ],\n",
      "        [ 0.5986,  0.2849,  0.6504, ...,  0.1393,  0.0702,  1.    ],\n",
      "        ...,\n",
      "        [ 0.6448,  0.3857,  0.2927, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.6777,  0.8154,  1.    ],\n",
      "        [ 1.    ,  0.3773,  0.6848, ...,  0.0315,  0.2242,  1.    ],\n",
      "        ...,\n",
      "        [ 0.6363,  0.401 ,  0.2768, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.0925,  0.4356,  1.    ],\n",
      "        [ 0.7102,  0.1923,  0.2803, ...,  0.8032,  0.4166,  1.    ],\n",
      "        ...,\n",
      "        [ 0.577 ,  0.3636,  0.2905, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(646,), dtype=int32, numpy=\n",
      "array([ 997,   32,    2,  135,  144, 2281,    6,    5, 1353,  906,  115,\n",
      "          8,    7,   50,  276, 3070,   11, 3043,    2,    1,  235,  352,\n",
      "       1681,    3,   14,   15,    3,   41,   19,   96, 7197,   43,   35,\n",
      "       3133,    8,    4, 7198,   11,    1,  906,   41,   19,   59,  117,\n",
      "          8,   19, 7199,   78,  641,    5,  526,  356,  352,  906,   87,\n",
      "         16,   64, 4410,    2,   16,  155,   24,   86,   67,    6,    1,\n",
      "        591,   42,   13,    7,  115,   63, 2978,   11, 4383,   30, 1000,\n",
      "        100,    1, 7200,    6,    1,  356,  352, 2306,    3,    1,  356,\n",
      "        352, 7201,   14,  356,  594,    6,  291,   24, 1869,  155,    6,\n",
      "         19, 1533,    3, 3520,   42,   41,   19,   98,  356,  594,   30,\n",
      "        111,  356,  944,   22,  333,   27,    4,  226,    1, 4411, 4203,\n",
      "        226, 4411, 7202,  261,  356,  944,   11,  261, 2567, 1031,   11,\n",
      "          1, 2045,    3,    8,    7,  356,  352,  128,  356, 3254,  530,\n",
      "         32,  356,  463,    3,   11,   59, 1396,   13,    7, 1577,   50,\n",
      "         48,  356,  463,  255,   66,    5,  984, 3254,    3,   35,    8,\n",
      "        268,    7,    1, 7203,    7, 3231,   15,    5,  458, 2375,   11,\n",
      "         59, 1760, 2274,    2,  104,   71,  238,    3,    2, 1013, 4412,\n",
      "         14,    5,  984, 3254, 1316,   38,    5,   58,  851,  356,  352,\n",
      "        481,   11,   51, 7204,    3,  356,  594,  133,   81,   23, 4413,\n",
      "       4194, 3788, 7205,  128,  202,  152, 1691,    6, 4414, 4414, 3980,\n",
      "          6,    5,   85,    6,   96,  339,    6,  356,  202,    3,   24,\n",
      "         17,  135,   43,    1, 3133,    1, 1199,    6,  356,  352,    3,\n",
      "          1, 1199,    6,  936,  352,   30,   16,  153,   16,   64, 4410,\n",
      "          2,    5,  526,  356,  906,   42,   16, 1012,   30,  124,   30,\n",
      "       3002, 1732,    6,    1,  906,   17,   23,  356,    3,    1,  638,\n",
      "         20,  356,  594,    4,  131,   86,    2,   23,  668,   43,  211,\n",
      "        356,  594, 7206, 4412,   32,  356,  202, 2486,    8,  131,   23,\n",
      "         15,  664,  356,  594,  137,  124,  356,  352,   11,    1,  906,\n",
      "         17,   93, 7207, 7208,    3,  664, 2698,    4,   80,   50,   25,\n",
      "          2,  637,    5,   85,    6,  356,  594,   32,   27,    5,  379,\n",
      "        259, 7209,   66, 7210,   22,  333,   87,    7,  137,  124, 4415,\n",
      "         11,    1,  157,   24,   64,   25,    2, 4159,   67, 4415,  100,\n",
      "       1463,    5,   85,    6,  356,  594,   59,    6,    1, 3565,    6,\n",
      "        936,  594,  936,  594,   93, 1237,  270,   59,    6,    1, 4094,\n",
      "         67, 1577,    2,    1,  157,   32,   98, 2379,    6, 7211,   49,\n",
      "        249,   22,  591,   52,   30,   11,    1,  360,    6, 7212,    4,\n",
      "         17,   34, 7213,   67, 7214,  100, 1237,    1, 1533,   50,   94,\n",
      "       1237,   71,   42, 7215, 7216,   30,   47,  153, 4416, 4416, 7217,\n",
      "       7218,  144, 2764,    6,    1, 7219, 1027,    3,   24,   84,    8,\n",
      "       1307,  262,   34,   67,  936, 3145,  530,   11,    1,  906,   18,\n",
      "          5,  364, 3459,    6, 7220, 7221,    8,  193,   50, 1000,   11,\n",
      "        114, 4339,    1,  356, 3145,  530,   14,  108,   24,   86,    5,\n",
      "        113,  591,    6,  356,    3,  936,  594,   11,    1,  906,  111,\n",
      "        372,    5, 3255, 1784,    2,  356,  594,    3,   29,  150,    2,\n",
      "         34,  356,  352,   11,    1,  906,   22,    1, 1134,   16,  167,\n",
      "        153,    1, 4278, 4417,   11,   41,   13,    7,  115,   32,    4,\n",
      "        131,  134,   11,   59,    6,    1,  356,  352, 2306,   13,    7,\n",
      "          5, 4418, 7222,   29,  102,   73, 2446, 1331,    3,    9,   97,\n",
      "       4418, 7223, 4419, 7224, 2632,    3,   29,  102,  527,    6,    5,\n",
      "       2115, 2446, 7225, 3356,  285, 4420,  372,  144,   37,   13,    7,\n",
      "          5, 7226,  636,   13,   97, 4413, 7227,    3, 7228,   11,   52,\n",
      "         81, 4420, 4417,   19, 2563,    1,  499, 1215,    6,  594,   14,\n",
      "         78,  170,    6, 3255,  356,  594,    4,   80,   18,    2,   89,\n",
      "        101,  171,   10,  471,  350,  594,  334,  169,   69,   67,    3,\n",
      "         67,  139, 1561,   59,    6,   78, 3255,  356, 4153,   14,   27,\n",
      "          4,   65,    5,  356,  352,   32,    5,  356, 1373,   11,    1,\n",
      "        537,    9,   54, 1321,   76,    5,  649,   27,    4,   65,    5,\n",
      "        936, 1373,   11,    1,  537,  485,   21, 7229,   14,   63,   73,\n",
      "        333,    6,    1,  499, 1215,   11,  356,  594], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 14:01:51.260875: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for element in train_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lenForBucketing(ds,limites):\n",
    "\n",
    "    # Crear un DataFrame vac√≠o para almacenar los resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Recorrer los pares consecutivos de la lista de l√≠mites\n",
    "    for lower, upper in zip(limites[:-1], limites[1:]):\n",
    "        # Contar cu√°ntos valores est√°n entre 'lower' y 'upper'\n",
    "        count = ((ds['len_keyframes'] > lower) & (ds['len_keyframes'] <= upper)).sum()\n",
    "        \n",
    "        # Guardar el resultado en una lista\n",
    "        resultados.append((lower, upper, count))\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    for lower, upper, count in resultados:\n",
    "        print(f\"Valores entre {lower} y {upper}: {count}\")\n",
    "\n",
    "\n",
    "limites = [0,300,500,700, 1000]\n",
    "print(f'Dataset: test')\n",
    "print_lenForBucketing(test,limites)\n",
    "print(f'Dataset: train')\n",
    "print_lenForBucketing(train,limites)\n",
    "print(f'Dataset: val')\n",
    "\n",
    "print_lenForBucketing(val,limites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_fn(points, translations):\n",
    "    # La longitud ser√° la cantidad de keyframes (primera dimensi√≥n de los puntos)\n",
    "    longitud=tf.shape(points)[0]\n",
    "    return longitud\n",
    "\n",
    "\n",
    "# limites = [300,500,700,1100]\n",
    "    \n",
    "# bucket_batch_sizes = [32, 32, 16, 8,8]\n",
    "\n",
    "def bucketed_dataset(dataset):\n",
    "    limites = [300,500,700,1100]\n",
    "    \n",
    "    bucket_batch_sizes = [16, 16, 16, 8, 4]\n",
    "    return dataset.bucket_by_sequence_length(\n",
    "            element_length_func=length_fn,       \n",
    "            bucket_boundaries=limites, # L√≠mites de los buckets\n",
    "            bucket_batch_sizes=bucket_batch_sizes,  # Tama√±os de batch para cada bucket\n",
    "            padded_shapes=(\n",
    "                [None, 48, 48],  \n",
    "                [None]           \n",
    "            ),\n",
    "            padding_values=(\n",
    "                tf.constant(0, dtype=tf.float32), # Relleno con ceros para los puntos \n",
    "                tf.constant(0, dtype=tf.int32) # Relleno con ceros para las traducciones \n",
    "            )\n",
    "    )\n",
    "\n",
    "# Aplicar bucketing y padding\n",
    "train_dataset= train_dataset.apply(bucketed_dataset)\n",
    "val_dataset= val_dataset.apply(bucketed_dataset)\n",
    "test_dataset= test_dataset.apply(bucketed_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 14:02:07.955294: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de batches en train: 24\n",
      "Cantidad de batches en val: 10\n",
      "Cantidad de batches en test: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 14:02:09.047866: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def count_batches(dataset):\n",
    "    batch_count = 0\n",
    "    for _ in dataset:\n",
    "        batch_count += 1\n",
    "    return batch_count\n",
    "print(f\"Cantidad de batches en train: {count_batches(train_dataset)}\")\n",
    "print(f\"Cantidad de batches en val: {count_batches(val_dataset)}\")\n",
    "print(f\"Cantidad de batches en test: {count_batches(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funci√≥n de p√©rdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_loss(translation, prediction):\n",
    "    \n",
    "#     input_length = tf.fill([tf.shape(prediction)[0], 1], tf.shape(prediction)[1])  # Longitud de la secuencia de entrada\n",
    "#     label_length = tf.math.count_nonzero(translation, axis=-1, keepdims=True)   # Longitud de la secuencia de traducci√≥n\n",
    "#     # Calcular la p√©rdida CTC usando ctc_batch_cost\n",
    "#     loss = tf.keras.backend.ctc_batch_cost(translation, prediction, input_length, label_length)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscar mejores par√°metros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_tuner as kt\n",
    "# num_classes=len(tokenizer.word_index) + 1\n",
    "\n",
    "# # Definir el modelo para la b√∫squeda de hiperpar√°metros\n",
    "# def build_model(hp):\n",
    "\n",
    "#     def createCNNkt():\n",
    "\n",
    "#         model= models.Sequential()\n",
    "#         # A√±adir capas CNN din√°micamente seg√∫n el n√∫mero de capas que se elija (1 a 5 capas CNN)\n",
    "#         for i in range(hp.Int('num_cnn_layers', 2, 5)):  # De 1 a 5 capas CNN\n",
    "#             model.add(layers.Conv2D(\n",
    "#                 filters=hp.Int(f'conv_{i+1}_filters', min_value=64, max_value=256, step=32),\n",
    "#                 kernel_size=(3, 3),\n",
    "#                 activation='relu',\n",
    "#                 input_shape=(48, 48, 1) if i == 0 else None))\n",
    "#             model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "#         model.add(layers.Flatten())\n",
    "#         return model\n",
    "\n",
    "#     def createLSTMkt():\n",
    "\n",
    "#         model= models.Sequential()\n",
    "#         # A√±adir capas LSTM din√°micamente seg√∫n el n√∫mero de capas que se elija (1 a 5 capas LSTM)\n",
    "#         for i in range(hp.Int('num_lstm_layers', 2, 5)):  # De 1 a 5 capas LSTM\n",
    "#             model.add(layers.LSTM(\n",
    "#                 units=hp.Int(f'lstm_{i+1}_units', min_value=64, max_value=256, step=32),\n",
    "#                 return_sequences=True))\n",
    "\n",
    "#             # A√±adir Dropout para cada capa LSTM\n",
    "#             model.add(layers.Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.4, step=0.1)))\n",
    "#         return model\n",
    "\n",
    "#     cnn = createCNNkt()\n",
    "\n",
    "#     video_input = layers.Input(shape=(None, 48, 48, 1))\n",
    "\n",
    "#     # Aplicar CNN a cada frame usando TimeDistributed\n",
    "#     cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "\n",
    "#     lstm=createLSTMkt()\n",
    "#     lstm_out= lstm(cnn_features)\n",
    "#     # Capa final de salida\n",
    "#     output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "\n",
    "#     # Compilar el modelo\n",
    "#     model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "#     model = models.Model(inputs=video_input, outputs=output)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Definir la b√∫squeda de hiperpar√°metros con Keras Tuner\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_loss', # M√©trica de evaluaci√≥n\n",
    "#     max_epochs= 50,\n",
    "#     factor=3,\n",
    "#     directory='',\n",
    "#     project_name='cnn_lstm_hyperparam'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Realizar la b√∫squeda de hiperpar√°metros\n",
    "# tuner.search(train_dataset, validation_data=val_dataset)\n",
    "\n",
    "# best_model=tuner.get_best_model(1)[0]\n",
    "# # Obtener los mejores hiperpar√°metros encontrados\n",
    "# best_hyperparams = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crear Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(48, 48, 1)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_example_model = create_cnn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_example_model(points_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
    "    model.add(layers.Dropout(0.3)) # A√±adir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    cnn = create_cnn()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1)) \n",
    "    # masked_input = layers.Masking(mask_value=0.0)(video_input)\n",
    "\n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaci√≥n para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m142.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m792.8/792.8 kB\u001b[0m \u001b[31m336.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.9.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(tokenizer.word_index) + 1\n",
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/best_model.keras',\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_loss',\n",
    "    mode='min'  \n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss=tf.keras.losses.CTC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.losses.losses.CTC at 0x7f4fd6869210>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions,greedy):\n",
    "    predicted_classes = tf.argmax(predictions, axis=-1)\n",
    "    mask = tf.not_equal(predicted_classes, 0)\n",
    "    seq_lengths = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-1)\n",
    "    \n",
    "    predictions=tf.transpose(predictions,[1,0,2])\n",
    "\n",
    "    if greedy:\n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    \n",
    "    else:\n",
    "        decoded, _ = tf.nn.ctc_beam_search_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    decoded_dense = tf.sparse.to_dense(decoded[0], default_value=-1).numpy()\n",
    "    \n",
    "    # Extraer las secuencias decodificadas eliminando los -1 (valores de relleno)\n",
    "    decoded_sequences = []\n",
    "    for seq in decoded_dense:\n",
    "        decoded_sequences.append([val for val in seq if val != -1])\n",
    "    \n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7664\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainstep(model, points_batch, translation_batch,step, epoch_loss_avg, epoch_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(points_batch, training=True)\n",
    "        print(tf.shape(predictions))\n",
    "        print(tf.shape(translation_batch))\n",
    "\n",
    "        loss = tf.reduce_mean(ctc_loss(translation_batch, predictions))\n",
    "        \n",
    "    # Calcular y aplicar gradientes\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # if step % 50 == 0:\n",
    "    #     print(f\"Step {step}: loss = {loss.numpy()}\")\n",
    "    predictions_decoded=greedy_decode(predictions, greedy=True)\n",
    "    epoch_accuracy.update_state(translation_batch, predictions_decoded)\n",
    "    epoch_loss_avg.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg):\n",
    "    # Evaluar en el conjunto de validaci√≥n al final de cada epoch usando BLEU\n",
    "   \n",
    "    predictions = model(points_batch, training=False)\n",
    "\n",
    "    val_loss=tf.reduce_mean(ctc_loss(translation_batch,predictions))\n",
    "    val_loss_avg.update_state(val_loss)\n",
    "\n",
    "    val_predictions = decode(predictions, greedy=False)\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in val_predictions]\n",
    "\n",
    "\n",
    "    val_acc_metric.update_state(translation_batch,val_predictions)\n",
    "    val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "    val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "    val_bleu_avg.update_state(val_bleu)\n",
    "    return val_acc_metric.result(), val_loss_avg.result(), val_bleu_avg.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 488, 48, 48]), TensorShape([16, 373]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_batch, translation_batch = next(iter(train_dataset_tqdm))\n",
    "points_batch.shape, translation_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0batch [03:37, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  16 7664], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 16 373], shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Logits `y_pred` are expected to be a tensor of shape `(batch_size, max_length, num_classes)`. Received: y_pred.shape=(16, 7664)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m train_dataset_tqdm \u001b[38;5;241m=\u001b[39m tqdm(train_dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenamiento\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (points_batch, translation_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset_tqdm):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtrainstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_loss_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_accuracy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     train_dataset_tqdm\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch_loss_avg\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch_accuracy\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     21\u001b[0m     })\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# M√©tricas para la validaci√≥n\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m, in \u001b[0;36mtrainstep\u001b[0;34m(model, points_batch, translation_batch, step, epoch_loss_avg, epoch_accuracy)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39mshape(predictions))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39mshape(translation_batch))\n\u001b[0;32m----> 7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslation_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calcular y aplicar gradientes\u001b[39;00m\n\u001b[1;32m     10\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/loss.py:43\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_pred\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_true\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(losses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:27\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[1;32m     26\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:2060\u001b[0m, in \u001b[0;36mctc\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTargets `y_true` are expected to be a tensor of shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2056\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`(batch_size, max_length)` in integer format. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2057\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: y_true.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mops\u001b[38;5;241m.\u001b[39mshape(y_true)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2058\u001b[0m     )\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ops\u001b[38;5;241m.\u001b[39mshape(y_pred)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 2060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2061\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits `y_pred` are expected to be a tensor of shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2062\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`(batch_size, max_length, num_classes)`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2063\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: y_pred.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mops\u001b[38;5;241m.\u001b[39mshape(y_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2064\u001b[0m     )\n\u001b[1;32m   2066\u001b[0m mask_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   2067\u001b[0m batch_length \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mshape(y_pred)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Logits `y_pred` are expected to be a tensor of shape `(batch_size, max_length, num_classes)`. Received: y_pred.shape=(16, 7664)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 100\n",
    "model.compile(optimizer=optimizador, loss=tf.keras.losses.CTC(), metrics=[calculate_bleu])\n",
    "\n",
    "# Iterar a trav√©s de los batches y ajustar manualmente los par√°metros\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_dataset_tqdm = tqdm(train_dataset, desc=\"Entrenamiento\", unit=\"batch\")\n",
    "    for step, (points_batch, translation_batch) in enumerate(train_dataset_tqdm):\n",
    "        \n",
    "        trainstep(model, points_batch, translation_batch, step, epoch_loss_avg, epoch_accuracy)\n",
    "        \n",
    "        train_dataset_tqdm.set_postfix({\n",
    "            \"Loss\": epoch_loss_avg.result().numpy(),\n",
    "            \"Accuracy\": epoch_accuracy.result().numpy()\n",
    "        })\n",
    "    \n",
    "    # M√©tricas para la validaci√≥n\n",
    "    val_loss_avg = tf.keras.metrics.Mean()\n",
    "    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_bleu_avg = tf.keras.metrics.Mean()\n",
    "    \n",
    "    val_dataset_tqdm = tqdm(val_dataset, desc=\"Validaci√≥n\", unit=\"batch\")\n",
    "    for step, (points_batch, translation_batch) in enumerate(val_dataset_tqdm):\n",
    "        \n",
    "        val_acc, val_loss, val_bleu = valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg)\n",
    "        \n",
    "        val_dataset_tqdm.set_postfix({\n",
    "            \"Val Loss\": val_loss.numpy(),\n",
    "            \"Val Accuracy\": val_acc.numpy(),\n",
    "            \"Val BLEU\": val_bleu\n",
    "        })\n",
    "\n",
    "    checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu})\n",
    "\n",
    "    if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu}):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "    print(f'Epoch: {epoch}, val_loss: {val_loss_avg}, val_acc: {val_acc_metric}, val_bleu: {val_bleu_avg}')\n",
    "\n",
    "# Evaluar el modelo en el dataset de test\n",
    "#test_loss, test_acc = model.evaluate(test_dataset)\n",
    "#print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
