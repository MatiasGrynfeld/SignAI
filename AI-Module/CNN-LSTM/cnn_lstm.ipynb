{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 17:03:38.782614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 17:03:38.801959: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 17:03:38.806950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 17:03:38.821979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728493426.452263    1429 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728493426.470295    1429 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728493426.470357    1429 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "gpu=tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True) #limits gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/app/AI-Module/Resources/Datasets/how2sign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntos_folder = \"/app/AI-Module/Resources/Datasets/Points\"\n",
    "files = [puntos_folder + \"/\" + file for file in os.listdir(puntos_folder)]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(files):\n",
    "    for file in files:\n",
    "        item_with_index = np.load(file, allow_pickle=True)\n",
    "        # Convertir los arrays a float32 para reducir memoria\n",
    "        item = item_with_index[0].astype(np.float32)\n",
    "        index = item_with_index[1]\n",
    "        yield item, index\n",
    "\n",
    "puntos_list = []\n",
    "ids_list = []\n",
    "\n",
    "for item, index in load_points(files):\n",
    "    puntos_list.append(item)\n",
    "    ids_list.append(index)\n",
    "\n",
    "df_puntos = pd.DataFrame({\n",
    "    'points': puntos_list,\n",
    "    'id': ids_list\n",
    "})\n",
    "df_puntos = df_puntos.sort_values([\"id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(df_puntos, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points_x</th>\n",
       "      <th>translation</th>\n",
       "      <th>id</th>\n",
       "      <th>len_keyframes</th>\n",
       "      <th>points_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>If you like to find more about my services you...</td>\n",
       "      <td>448</td>\n",
       "      <td>817</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>Hello welcome my name is Julio Nutt and I am a...</td>\n",
       "      <td>210</td>\n",
       "      <td>401</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "      <td>All right, the drink we're about to make is ca...</td>\n",
       "      <td>228</td>\n",
       "      <td>308</td>\n",
       "      <td>[[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "      <td>Okay, another thing that for me is important. ...</td>\n",
       "      <td>187</td>\n",
       "      <td>367</td>\n",
       "      <td>[[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "      <td>All right, we're ready to start our fire. What...</td>\n",
       "      <td>99</td>\n",
       "      <td>453</td>\n",
       "      <td>[[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            points_x  \\\n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...   \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...   \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...   \n",
       "\n",
       "                                         translation   id  len_keyframes  \\\n",
       "0  If you like to find more about my services you...  448            817   \n",
       "1  Hello welcome my name is Julio Nutt and I am a...  210            401   \n",
       "2  All right, the drink we're about to make is ca...  228            308   \n",
       "3  Okay, another thing that for me is important. ...  187            367   \n",
       "4  All right, we're ready to start our fire. What...   99            453   \n",
       "\n",
       "                                            points_y  \n",
       "0  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  \n",
       "1  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  \n",
       "2  [[1.0, 0.0589, 1.0, 1.0, 0.7364, 0.0, 0.6998, ...  \n",
       "3  [[1.0, 0.0515, 1.0, 1.0, 0.7142, 0.0, 0.6933, ...  \n",
       "4  [[-1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, 0.0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"points_y\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"points\"]=dataset[\"points_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el formato de points para que sea una lista de 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Agrega la ruta donde est√° ubicado Points2VecClass.py\n",
    "sys.path.append('/app/AI-Module/Modules')\n",
    "from Points2VecClass import Point2Vec\n",
    "def pointsToCnnInputForm(points):\n",
    "    p2v = Point2Vec(4)\n",
    "    return p2v.CNNMatrix(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cnn_input_form(serie):\n",
    "    for index, element in enumerate(serie):\n",
    "        element= pointsToCnnInputForm(element)\n",
    "        serie.at[index]=element\n",
    "    return serie\n",
    "dataset['points']=to_cnn_input_form(dataset['points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizo las traducciones para que sean compatibles con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Crear el tokenizador\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustar el tokenizador al texto de las traducciones\n",
    "tokenizer.fit_on_texts(dataset['translation'])\n",
    "\n",
    "# Convertir las oraciones en secuencias de enteros\n",
    "dataset['translation'] = tokenizer.texts_to_sequences(dataset['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_val size: 400, Test size: 100\n",
      "Train size: 300, Test size: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Divide en train y test\n",
    "points_train_val, points_test, translation_train_val, translation_test = train_test_split(\n",
    "    dataset[[\"points\", \"len_keyframes\"]], dataset[\"translation\"], test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Train_val size: {len(points_train_val)}, Test size: {len(points_test)}\")\n",
    "points_train, points_val, translation_train, translation_val=train_test_split(\n",
    "    points_train_val,translation_train_val, test_size=0.25,random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(points_train)}, Val size: {len(points_val)}\")\n",
    "train = {\n",
    "    \"points\": points_train[\"points\"],\n",
    "    \"len_keyframes\": points_train[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_train\n",
    "}\n",
    "\n",
    "val = {\n",
    "    \"points\": points_val[\"points\"],\n",
    "    \"len_keyframes\": points_val[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_val\n",
    "}\n",
    "\n",
    "test = {\n",
    "    \"points\": points_test[\"points\"],\n",
    "    \"len_keyframes\": points_test[\"len_keyframes\"],\n",
    "    \"translation_sequence\": translation_test\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "globals().pop('dataset', None)\n",
    "globals().pop('puntos_folder', None)\n",
    "globals().pop('puntos_list', None)\n",
    "globals().pop('df_puntos', None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['translation_sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo datasets de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(df):\n",
    "    def generator():\n",
    "        for i in range(len(df['points'])):\n",
    "            points = df['points'].values[i]\n",
    "            sequence = df['translation_sequence'].values[i]\n",
    "            yield points, sequence\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 48, 48), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None), dtype=tf.int32)\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = create_tf_dataset(test)\n",
    "val_dataset=create_tf_dataset(val)\n",
    "train_dataset= create_tf_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.shuffle(buffer_size=100)\n",
    "val_dataset=val_dataset.shuffle(buffer_size=100)\n",
    "test_dataset=test_dataset.shuffle(buffer_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = train_dataset.reduce(0, lambda x, _: x + 1)\n",
    "count.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 48, 48), dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1133, 48, 48), dtype=float32, numpy=\n",
      "array([[[ 1.    ,  0.6049,  1.    , ...,  0.7828,  0.3171,  1.    ],\n",
      "        [ 0.308 ,  0.7701,  0.2518, ...,  0.198 ,  0.5576,  1.    ],\n",
      "        [ 0.811 ,  0.109 ,  0.3641, ...,  0.7605,  0.3062,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4812,  0.3349,  0.2868, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[ 1.    ,  0.5889,  1.    , ...,  0.7625,  0.3176,  1.    ],\n",
      "        [ 0.0208,  0.7787,  0.2834, ...,  0.1822,  0.5087,  1.    ],\n",
      "        [ 0.7561,  0.1014,  0.2969, ...,  0.7796,  0.3118,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4853,  0.3326,  0.2908, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[ 1.    ,  0.5205,  1.    , ...,  0.7323,  0.2591,  1.    ],\n",
      "        [ 0.0196,  0.7315,  0.2167, ...,  0.2206,  0.4395,  1.    ],\n",
      "        [ 0.7376,  0.1138,  0.2324, ...,  0.8485,  0.4687,  1.    ],\n",
      "        ...,\n",
      "        [ 0.4757,  0.3234,  0.2857, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.5452,  0.8084,  1.    ],\n",
      "        [ 0.5986,  0.2849,  0.6504, ...,  0.1393,  0.0702,  1.    ],\n",
      "        ...,\n",
      "        [ 0.6448,  0.3857,  0.2927, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.6777,  0.8154,  1.    ],\n",
      "        [ 1.    ,  0.3773,  0.6848, ...,  0.0315,  0.2242,  1.    ],\n",
      "        ...,\n",
      "        [ 0.6363,  0.401 ,  0.2768, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
      "\n",
      "       [[-1.    , -1.    , -1.    , ..., -1.    , -1.    ,  0.    ],\n",
      "        [-1.    , -1.    , -1.    , ...,  0.0925,  0.4356,  1.    ],\n",
      "        [ 0.7102,  0.1923,  0.2803, ...,  0.8032,  0.4166,  1.    ],\n",
      "        ...,\n",
      "        [ 0.577 ,  0.3636,  0.2905, ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
      "        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(646,), dtype=int32, numpy=\n",
      "array([ 997,   32,    2,  135,  144, 2281,    6,    5, 1353,  906,  115,\n",
      "          8,    7,   50,  276, 3070,   11, 3043,    2,    1,  235,  352,\n",
      "       1681,    3,   14,   15,    3,   41,   19,   96, 7197,   43,   35,\n",
      "       3133,    8,    4, 7198,   11,    1,  906,   41,   19,   59,  117,\n",
      "          8,   19, 7199,   78,  641,    5,  526,  356,  352,  906,   87,\n",
      "         16,   64, 4410,    2,   16,  155,   24,   86,   67,    6,    1,\n",
      "        591,   42,   13,    7,  115,   63, 2978,   11, 4383,   30, 1000,\n",
      "        100,    1, 7200,    6,    1,  356,  352, 2306,    3,    1,  356,\n",
      "        352, 7201,   14,  356,  594,    6,  291,   24, 1869,  155,    6,\n",
      "         19, 1533,    3, 3520,   42,   41,   19,   98,  356,  594,   30,\n",
      "        111,  356,  944,   22,  333,   27,    4,  226,    1, 4411, 4203,\n",
      "        226, 4411, 7202,  261,  356,  944,   11,  261, 2567, 1031,   11,\n",
      "          1, 2045,    3,    8,    7,  356,  352,  128,  356, 3254,  530,\n",
      "         32,  356,  463,    3,   11,   59, 1396,   13,    7, 1577,   50,\n",
      "         48,  356,  463,  255,   66,    5,  984, 3254,    3,   35,    8,\n",
      "        268,    7,    1, 7203,    7, 3231,   15,    5,  458, 2375,   11,\n",
      "         59, 1760, 2274,    2,  104,   71,  238,    3,    2, 1013, 4412,\n",
      "         14,    5,  984, 3254, 1316,   38,    5,   58,  851,  356,  352,\n",
      "        481,   11,   51, 7204,    3,  356,  594,  133,   81,   23, 4413,\n",
      "       4194, 3788, 7205,  128,  202,  152, 1691,    6, 4414, 4414, 3980,\n",
      "          6,    5,   85,    6,   96,  339,    6,  356,  202,    3,   24,\n",
      "         17,  135,   43,    1, 3133,    1, 1199,    6,  356,  352,    3,\n",
      "          1, 1199,    6,  936,  352,   30,   16,  153,   16,   64, 4410,\n",
      "          2,    5,  526,  356,  906,   42,   16, 1012,   30,  124,   30,\n",
      "       3002, 1732,    6,    1,  906,   17,   23,  356,    3,    1,  638,\n",
      "         20,  356,  594,    4,  131,   86,    2,   23,  668,   43,  211,\n",
      "        356,  594, 7206, 4412,   32,  356,  202, 2486,    8,  131,   23,\n",
      "         15,  664,  356,  594,  137,  124,  356,  352,   11,    1,  906,\n",
      "         17,   93, 7207, 7208,    3,  664, 2698,    4,   80,   50,   25,\n",
      "          2,  637,    5,   85,    6,  356,  594,   32,   27,    5,  379,\n",
      "        259, 7209,   66, 7210,   22,  333,   87,    7,  137,  124, 4415,\n",
      "         11,    1,  157,   24,   64,   25,    2, 4159,   67, 4415,  100,\n",
      "       1463,    5,   85,    6,  356,  594,   59,    6,    1, 3565,    6,\n",
      "        936,  594,  936,  594,   93, 1237,  270,   59,    6,    1, 4094,\n",
      "         67, 1577,    2,    1,  157,   32,   98, 2379,    6, 7211,   49,\n",
      "        249,   22,  591,   52,   30,   11,    1,  360,    6, 7212,    4,\n",
      "         17,   34, 7213,   67, 7214,  100, 1237,    1, 1533,   50,   94,\n",
      "       1237,   71,   42, 7215, 7216,   30,   47,  153, 4416, 4416, 7217,\n",
      "       7218,  144, 2764,    6,    1, 7219, 1027,    3,   24,   84,    8,\n",
      "       1307,  262,   34,   67,  936, 3145,  530,   11,    1,  906,   18,\n",
      "          5,  364, 3459,    6, 7220, 7221,    8,  193,   50, 1000,   11,\n",
      "        114, 4339,    1,  356, 3145,  530,   14,  108,   24,   86,    5,\n",
      "        113,  591,    6,  356,    3,  936,  594,   11,    1,  906,  111,\n",
      "        372,    5, 3255, 1784,    2,  356,  594,    3,   29,  150,    2,\n",
      "         34,  356,  352,   11,    1,  906,   22,    1, 1134,   16,  167,\n",
      "        153,    1, 4278, 4417,   11,   41,   13,    7,  115,   32,    4,\n",
      "        131,  134,   11,   59,    6,    1,  356,  352, 2306,   13,    7,\n",
      "          5, 4418, 7222,   29,  102,   73, 2446, 1331,    3,    9,   97,\n",
      "       4418, 7223, 4419, 7224, 2632,    3,   29,  102,  527,    6,    5,\n",
      "       2115, 2446, 7225, 3356,  285, 4420,  372,  144,   37,   13,    7,\n",
      "          5, 7226,  636,   13,   97, 4413, 7227,    3, 7228,   11,   52,\n",
      "         81, 4420, 4417,   19, 2563,    1,  499, 1215,    6,  594,   14,\n",
      "         78,  170,    6, 3255,  356,  594,    4,   80,   18,    2,   89,\n",
      "        101,  171,   10,  471,  350,  594,  334,  169,   69,   67,    3,\n",
      "         67,  139, 1561,   59,    6,   78, 3255,  356, 4153,   14,   27,\n",
      "          4,   65,    5,  356,  352,   32,    5,  356, 1373,   11,    1,\n",
      "        537,    9,   54, 1321,   76,    5,  649,   27,    4,   65,    5,\n",
      "        936, 1373,   11,    1,  537,  485,   21, 7229,   14,   63,   73,\n",
      "        333,    6,    1,  499, 1215,   11,  356,  594], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 16:40:39.442502: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for element in train_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: test\n",
      "Valores entre 0 y 300: 17\n",
      "Valores entre 300 y 500: 52\n",
      "Valores entre 500 y 700: 21\n",
      "Valores entre 700 y 1000: 7\n",
      "Dataset: train\n",
      "Valores entre 0 y 300: 57\n",
      "Valores entre 300 y 500: 154\n",
      "Valores entre 500 y 700: 55\n",
      "Valores entre 700 y 1000: 24\n",
      "Dataset: val\n",
      "Valores entre 0 y 300: 18\n",
      "Valores entre 300 y 500: 51\n",
      "Valores entre 500 y 700: 20\n",
      "Valores entre 700 y 1000: 11\n"
     ]
    }
   ],
   "source": [
    "def print_lenForBucketing(ds,limites):\n",
    "\n",
    "    # Crear un DataFrame vac√≠o para almacenar los resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Recorrer los pares consecutivos de la lista de l√≠mites\n",
    "    for lower, upper in zip(limites[:-1], limites[1:]):\n",
    "        # Contar cu√°ntos valores est√°n entre 'lower' y 'upper'\n",
    "        count = ((ds['len_keyframes'] > lower) & (ds['len_keyframes'] <= upper)).sum()\n",
    "        \n",
    "        # Guardar el resultado en una lista\n",
    "        resultados.append((lower, upper, count))\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    for lower, upper, count in resultados:\n",
    "        print(f\"Valores entre {lower} y {upper}: {count}\")\n",
    "\n",
    "\n",
    "limites = [0,300,500,700, 1000]\n",
    "print(f'Dataset: test')\n",
    "print_lenForBucketing(test,limites)\n",
    "print(f'Dataset: train')\n",
    "print_lenForBucketing(train,limites)\n",
    "print(f'Dataset: val')\n",
    "\n",
    "print_lenForBucketing(val,limites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FlatMapDataset element_spec=(TensorSpec(shape=(None, 48, 48), dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_fn(points, translations):\n",
    "    # La longitud ser√° la cantidad de keyframes (primera dimensi√≥n de los puntos)\n",
    "    longitud=tf.shape(points)[0]\n",
    "    return longitud\n",
    "\n",
    "\n",
    "# limites = [300,500,700,1100]\n",
    "    \n",
    "# bucket_batch_sizes = [32, 32, 16, 8,8]\n",
    "\n",
    "def bucketed_dataset(dataset):\n",
    "    limites = [300,500,700,1100]\n",
    "    \n",
    "    bucket_batch_sizes = [4, 4, 4, 4, 4]\n",
    "    return dataset.bucket_by_sequence_length(\n",
    "            element_length_func=length_fn,       \n",
    "            bucket_boundaries=limites, # L√≠mites de los buckets\n",
    "            bucket_batch_sizes=bucket_batch_sizes,  # Tama√±os de batch para cada bucket\n",
    "            padded_shapes=(\n",
    "                [None, 48, 48],  \n",
    "                [None]           \n",
    "            ),\n",
    "            padding_values=(\n",
    "                tf.constant(0, dtype=tf.float32), # Relleno con ceros para los puntos \n",
    "                tf.constant(0, dtype=tf.int32) # Relleno con ceros para las traducciones \n",
    "            )\n",
    "    )\n",
    "\n",
    "# Aplicar bucketing y padding\n",
    "train_dataset= train_dataset.apply(bucketed_dataset)\n",
    "val_dataset= val_dataset.apply(bucketed_dataset)\n",
    "test_dataset= test_dataset.apply(bucketed_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.shuffle(buffer_size=100)\n",
    "val_dataset=val_dataset.shuffle(buffer_size=100)\n",
    "test_dataset=test_dataset.shuffle(buffer_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 17:08:18.973898: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de batches en train: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 17:08:19.621846: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de batches en val: 17\n",
      "Cantidad de batches en test: 17\n"
     ]
    }
   ],
   "source": [
    "def count_batches(dataset):\n",
    "    batch_count = 0\n",
    "    for _ in dataset:\n",
    "        batch_count += 1\n",
    "    return batch_count\n",
    "print(f\"Cantidad de batches en train: {count_batches(train_dataset)}\")\n",
    "print(f\"Cantidad de batches en val: {count_batches(val_dataset)}\")\n",
    "print(f\"Cantidad de batches en test: {count_batches(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funci√≥n de p√©rdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_loss(translation, prediction):\n",
    "    \n",
    "#     input_length = tf.fill([tf.shape(prediction)[0], 1], tf.shape(prediction)[1])  # Longitud de la secuencia de entrada\n",
    "#     label_length = tf.math.count_nonzero(translation, axis=-1, keepdims=True)   # Longitud de la secuencia de traducci√≥n\n",
    "#     # Calcular la p√©rdida CTC usando ctc_batch_cost\n",
    "#     loss = tf.keras.backend.ctc_batch_cost(translation, prediction, input_length, label_length)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscar mejores par√°metros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_tuner as kt\n",
    "# num_classes=len(tokenizer.word_index) + 1\n",
    "\n",
    "# # Definir el modelo para la b√∫squeda de hiperpar√°metros\n",
    "# def build_model(hp):\n",
    "\n",
    "#     def createCNNkt():\n",
    "\n",
    "#         model= models.Sequential()\n",
    "#         # A√±adir capas CNN din√°micamente seg√∫n el n√∫mero de capas que se elija (1 a 5 capas CNN)\n",
    "#         for i in range(hp.Int('num_cnn_layers', 2, 5)):  # De 1 a 5 capas CNN\n",
    "#             model.add(layers.Conv2D(\n",
    "#                 filters=hp.Int(f'conv_{i+1}_filters', min_value=64, max_value=256, step=32),\n",
    "#                 kernel_size=(3, 3),\n",
    "#                 activation='relu',\n",
    "#                 input_shape=(48, 48, 1) if i == 0 else None))\n",
    "#             model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "#         model.add(layers.Flatten())\n",
    "#         return model\n",
    "\n",
    "#     def createLSTMkt():\n",
    "\n",
    "#         model= models.Sequential()\n",
    "#         # A√±adir capas LSTM din√°micamente seg√∫n el n√∫mero de capas que se elija (1 a 5 capas LSTM)\n",
    "#         for i in range(hp.Int('num_lstm_layers', 2, 5)):  # De 1 a 5 capas LSTM\n",
    "#             model.add(layers.LSTM(\n",
    "#                 units=hp.Int(f'lstm_{i+1}_units', min_value=64, max_value=256, step=32),\n",
    "#                 return_sequences=True))\n",
    "\n",
    "#             # A√±adir Dropout para cada capa LSTM\n",
    "#             model.add(layers.Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.4, step=0.1)))\n",
    "#         return model\n",
    "\n",
    "#     cnn = createCNNkt()\n",
    "\n",
    "#     video_input = layers.Input(shape=(None, 48, 48, 1))\n",
    "\n",
    "#     # Aplicar CNN a cada frame usando TimeDistributed\n",
    "#     cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "\n",
    "#     lstm=createLSTMkt()\n",
    "#     lstm_out= lstm(cnn_features)\n",
    "#     # Capa final de salida\n",
    "#     output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "\n",
    "#     # Compilar el modelo\n",
    "#     model.compile(optimizer='adam', loss=ctc_loss)\n",
    "\n",
    "#     model = models.Model(inputs=video_input, outputs=output)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Definir la b√∫squeda de hiperpar√°metros con Keras Tuner\n",
    "# tuner = kt.Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_loss', # M√©trica de evaluaci√≥n\n",
    "#     max_epochs= 50,\n",
    "#     factor=3,\n",
    "#     directory='',\n",
    "#     project_name='cnn_lstm_hyperparam'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Realizar la b√∫squeda de hiperpar√°metros\n",
    "# tuner.search(train_dataset, validation_data=val_dataset)\n",
    "\n",
    "# best_model=tuner.get_best_model(1)[0]\n",
    "# # Obtener los mejores hiperpar√°metros encontrados\n",
    "# best_hyperparams = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crear Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(48, 48, 1)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_example_model = create_cnn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "    model.add(layers.Dropout(0.3)) # A√±adir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    cnn = create_cnn()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1)) \n",
    "    # masked_input = layers.Masking(mask_value=0.0)(video_input)\n",
    "\n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaci√≥n para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(tokenizer.word_index) + 1\n",
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/best_model.keras',\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_loss',\n",
    "    mode='min'  \n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss=tf.keras.losses.CTC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions,greedy):\n",
    "    predicted_classes = tf.argmax(predictions, axis=-1)\n",
    "    mask = tf.not_equal(predicted_classes, 0)\n",
    "    seq_lengths = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-1)\n",
    "    \n",
    "    predictions=tf.transpose(predictions,[1,0,2])\n",
    "\n",
    "    if greedy:\n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    \n",
    "    else:\n",
    "        decoded, _ = tf.nn.ctc_beam_search_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    decoded_dense = tf.sparse.to_dense(decoded[0], default_value=-1).numpy()\n",
    "    \n",
    "    # Extraer las secuencias decodificadas eliminando los -1 (valores de relleno)\n",
    "    decoded_sequences = []\n",
    "    for seq in decoded_dense:\n",
    "        decoded_sequences.append([val for val in seq if val != -1])\n",
    "    \n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7664\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainstep(model, points_batch, translation_batch,step, epoch_loss_avg, epoch_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(points_batch, training=True)\n",
    "        loss = tf.reduce_mean(ctc_loss(translation_batch, predictions))\n",
    "        \n",
    "    # Calcular y aplicar gradientes\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # if step % 50 == 0:\n",
    "    #     print(f\"Step {step}: loss = {loss.numpy()}\")\n",
    "    predictions_decoded=decode(predictions, greedy=True)\n",
    "    epoch_accuracy.update_state(translation_batch, predictions_decoded)\n",
    "    epoch_loss_avg.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg):\n",
    "    # Evaluar en el conjunto de validaci√≥n al final de cada epoch usando BLEU\n",
    "   \n",
    "    predictions = model(points_batch, training=False)\n",
    "\n",
    "    val_loss=tf.reduce_mean(ctc_loss(translation_batch,predictions))\n",
    "    val_loss_avg.update_state(val_loss)\n",
    "\n",
    "    val_predictions = decode(predictions, greedy=False)\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in val_predictions]\n",
    "\n",
    "\n",
    "    val_acc_metric.update_state(translation_batch,val_predictions)\n",
    "    val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "    val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "    val_bleu_avg.update_state(val_bleu)\n",
    "    return val_acc_metric.result(), val_loss_avg.result(), val_bleu_avg.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 0batch [00:02, ?batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 258, 48, 48]), TensorShape([4, 179]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_dataset_tqdm = tqdm(train_dataset, desc=\"Entrenamiento\", unit=\"batch\")\n",
    "val_dataset_tqdm = tqdm(val_dataset, desc=\"Validaci√≥n\", unit=\"batch\")\n",
    "points_batch, translation_batch = next(iter(train_dataset_tqdm))\n",
    "points_batch.shape, translation_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1728494167.364504    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.370222    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.374569    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.379075    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.383565    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.389785    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.395499    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.400656    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.405807    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.411304    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.416413    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.421248    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.426387    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.431396    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.436105    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.440953    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.445882    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.450906    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.455634    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.462625    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.467645    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.472512    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.478779    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.484489    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.489715    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.494829    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.501017    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.529303    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.535217    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.539719    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.544244    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.548742    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.553173    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.557378    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.561549    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.566036    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.570407    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.574825    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.579034    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.583905    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.588459    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.592901    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.598468    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.602819    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.607306    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.611882    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.616106    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.620734    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.625203    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.629900    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.634253    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.638696    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.643377    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.648991    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.653595    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.658289    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.663919    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.668344    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728494167.672129    1429 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "2024-10-09 17:38:23.765817: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:310] gpu_async_0 cuMemAllocAsync failed to allocate 5161984 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 0/12878086144\n",
      "2024-10-09 17:38:23.766497: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:315] Stats: Limit:                      9809428480\n",
      "InUse:                     23856258540\n",
      "MaxInUse:                  23856258540\n",
      "NumAllocs:                       24068\n",
      "MaxAllocSize:              10376075008\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-10-09 17:38:23.767030: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:64] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2024-10-09 17:38:23.767037: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1, 12\n",
      "2024-10-09 17:38:23.767039: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4, 1053\n",
      "2024-10-09 17:38:23.767041: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 8, 9\n",
      "2024-10-09 17:38:23.767042: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 16, 1740\n",
      "2024-10-09 17:38:23.767044: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 32, 4\n",
      "2024-10-09 17:38:23.767045: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 40, 6\n",
      "2024-10-09 17:38:23.767046: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 128, 2\n",
      "2024-10-09 17:38:23.767047: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 256, 2\n",
      "2024-10-09 17:38:23.767049: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 512, 96\n",
      "2024-10-09 17:38:23.767053: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1028, 1\n",
      "2024-10-09 17:38:23.767055: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1152, 2\n",
      "2024-10-09 17:38:23.767056: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2048, 32\n",
      "2024-10-09 17:38:23.767058: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2272, 329\n",
      "2024-10-09 17:38:23.767061: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4096, 32\n",
      "2024-10-09 17:38:23.767062: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4528, 3\n",
      "2024-10-09 17:38:23.767063: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4784, 2\n",
      "2024-10-09 17:38:23.767064: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4800, 2\n",
      "2024-10-09 17:38:23.767066: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 9088, 3053\n",
      "2024-10-09 17:38:23.767067: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 10944, 3\n",
      "2024-10-09 17:38:23.767068: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 11136, 3\n",
      "2024-10-09 17:38:23.767069: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 15200, 1\n",
      "2024-10-09 17:38:23.767071: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 15776, 1\n",
      "2024-10-09 17:38:23.767072: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 30656, 1\n",
      "2024-10-09 17:38:23.767073: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 65536, 48\n",
      "2024-10-09 17:38:23.767074: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 73728, 2\n",
      "2024-10-09 17:38:23.767076: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 102400, 299\n",
      "2024-10-09 17:38:23.767077: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 131072, 24\n",
      "2024-10-09 17:38:23.767078: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 204800, 968\n",
      "2024-10-09 17:38:23.767079: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 262144, 4\n",
      "2024-10-09 17:38:23.767081: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 270848, 299\n",
      "2024-10-09 17:38:23.767082: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 306176, 2\n",
      "2024-10-09 17:38:23.767083: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 451584, 598\n",
      "2024-10-09 17:38:23.767084: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 524288, 2\n",
      "2024-10-09 17:38:23.767086: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 541696, 968\n",
      "2024-10-09 17:38:23.767087: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 612352, 6\n",
      "2024-10-09 17:38:23.767089: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 790528, 6\n",
      "2024-10-09 17:38:23.767090: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 903168, 1936\n",
      "2024-10-09 17:38:23.767091: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 972800, 2\n",
      "2024-10-09 17:38:23.767092: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1009664, 2\n",
      "2024-10-09 17:38:23.767094: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1083392, 598\n",
      "2024-10-09 17:38:23.767095: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1224704, 7\n",
      "2024-10-09 17:38:23.767097: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1353872, 1\n",
      "2024-10-09 17:38:23.767098: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1358656, 1\n",
      "2024-10-09 17:38:23.767099: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 1945600, 6\n",
      "2024-10-09 17:38:23.767101: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2019328, 6\n",
      "2024-10-09 17:38:23.767102: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2166784, 1936\n",
      "2024-10-09 17:38:23.767103: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2712528, 1\n",
      "2024-10-09 17:38:23.767104: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2717312, 1\n",
      "2024-10-09 17:38:23.767105: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 2726400, 3\n",
      "2024-10-09 17:38:23.767107: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 3061776, 4\n",
      "2024-10-09 17:38:23.767108: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 3276800, 24\n",
      "2024-10-09 17:38:23.767109: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 3891200, 7\n",
      "2024-10-09 17:38:23.767110: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 4038656, 7\n",
      "2024-10-09 17:38:23.767112: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 5161984, 661\n",
      "2024-10-09 17:38:23.767113: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 5304800, 1\n",
      "2024-10-09 17:38:23.767114: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 5411168, 1\n",
      "2024-10-09 17:38:23.767115: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 7847936, 1\n",
      "2024-10-09 17:38:23.767116: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 9728016, 4\n",
      "2024-10-09 17:38:23.767118: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 10096656, 4\n",
      "2024-10-09 17:38:23.767119: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 11022336, 2\n",
      "2024-10-09 17:38:23.767120: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 13107200, 2\n",
      "2024-10-09 17:38:23.767121: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 13373440, 6\n",
      "2024-10-09 17:38:23.767123: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 30617600, 2\n",
      "2024-10-09 17:38:23.767124: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 34702592, 1\n",
      "2024-10-09 17:38:23.767125: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 35020800, 2\n",
      "2024-10-09 17:38:23.767126: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 36347904, 2\n",
      "2024-10-09 17:38:23.767128: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 36664576, 4\n",
      "2024-10-09 17:38:23.767129: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 83874816, 1\n",
      "2024-10-09 17:38:23.767131: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 85346304, 1\n",
      "2024-10-09 17:38:23.767132: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 97280000, 2\n",
      "2024-10-09 17:38:23.767133: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 100966400, 2\n",
      "2024-10-09 17:38:23.767134: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 116492800, 4\n",
      "2024-10-09 17:38:23.767135: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 120907264, 4\n",
      "2024-10-09 17:38:23.767141: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:67] 10376075008, 1\n",
      "2024-10-09 17:38:23.767147: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:98] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 25769803776\n",
      "2024-10-09 17:38:23.767148: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 23856258540\n",
      "2024-10-09 17:38:23.767150: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 25769803776\n",
      "2024-10-09 17:38:23.767151: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:102] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 23856258540\n",
      "2024-10-09 17:38:23.767162: W tensorflow/core/framework/op_kernel.cc:1828] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Exp_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Exp] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (points_batch, translation_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset_tqdm):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtrainstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_loss_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_accuracy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     train_dataset_tqdm\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch_loss_avg\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch_accuracy\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     21\u001b[0m     })\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# M√©tricas para la validaci√≥n\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36mtrainstep\u001b[0;34m(model, points_batch, translation_batch, step, epoch_loss_avg, epoch_accuracy)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(points_batch, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslation_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calcular y aplicar gradientes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/loss.py:43\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_pred\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_true\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(losses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:27\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[1;32m     26\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:2074\u001b[0m, in \u001b[0;36mctc\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   2069\u001b[0m input_length \u001b[38;5;241m=\u001b[39m input_length \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mones((batch_length,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2070\u001b[0m label_length \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(\n\u001b[1;32m   2071\u001b[0m     ops\u001b[38;5;241m.\u001b[39msum(y_true \u001b[38;5;241m!=\u001b[39m mask_index, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2072\u001b[0m )\n\u001b[0;32m-> 2074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_index\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:1871\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(target, output, target_length, output_length, mask_index)\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((target, output, target_length, output_length)):\n\u001b[1;32m   1868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CTCLoss(mask_index)\u001b[38;5;241m.\u001b[39msymbolic_call(\n\u001b[1;32m   1869\u001b[0m         target, output, target_length, output_length\n\u001b[1;32m   1870\u001b[0m     )\n\u001b[0;32m-> 1871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_index\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py:799\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(target, output, target_length, output_length, mask_index)\u001b[0m\n\u001b[1;32m    797\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m result_dtype\n\u001b[1;32m    798\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(output, compute_dtype)\n\u001b[0;32m--> 799\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblank_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_time_major\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(loss, result_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Exp_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Exp] name: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 100\n",
    "model.compile(optimizer=optimizador, loss=tf.keras.losses.CTC(), metrics=[calculate_bleu])\n",
    "\n",
    "# Iterar a trav√©s de los batches y ajustar manualmente los par√°metros\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    \n",
    "    for step, (points_batch, translation_batch) in enumerate(train_dataset_tqdm):\n",
    "        train_dataset_tqdm.set_postfix({\n",
    "            \"Loss\": epoch_loss_avg.result().numpy(),\n",
    "            \"Accuracy\": epoch_accuracy.result().numpy()\n",
    "        })\n",
    "        \n",
    "        trainstep(model, points_batch, translation_batch, step, epoch_loss_avg, epoch_accuracy)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # M√©tricas para la validaci√≥n\n",
    "    val_loss_avg = tf.keras.metrics.Mean()\n",
    "    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_bleu_avg = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for step, (points_batch, translation_batch) in enumerate(val_dataset_tqdm):\n",
    "        \n",
    "        val_dataset_tqdm.set_postfix({\n",
    "            \"Val Loss\": val_loss_avg.result().numpy(),\n",
    "            \"Val Accuracy\": val_acc_metric.result().numpy(),\n",
    "            \"Val BLEU\": val_bleu_avg.result().numpy()\n",
    "        })\n",
    "        \n",
    "        valstep(model, points_batch, translation_batch,val_loss_avg, val_acc_metric, val_bleu_avg)\n",
    "        \n",
    "        \n",
    "\n",
    "    checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss_avg, 'val_accuracy': val_acc_metric, 'val_bleu': val_bleu_avg})\n",
    "\n",
    "    if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss_avg, 'val_accuracy': val_acc_metric, 'val_bleu': val_bleu_avg}):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "    print(f'Epoch: {epoch}, val_loss: {val_loss_avg}, val_acc: {val_acc_metric}, val_bleu: {val_bleu_avg}')\n",
    "\n",
    "# Evaluar el modelo en el dataset de test\n",
    "#test_loss, test_acc = model.evaluate(test_dataset)\n",
    "#print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
