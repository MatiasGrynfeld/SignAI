{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 18:54:19.842929: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 18:54:19.871643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 18:54:19.879490: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 18:54:19.897762: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728586465.966692     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728586465.988300     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728586465.988342     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "gpus=tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Cargar el archivo JSON\n",
    "with open('tokenizer.json') as json_file:\n",
    "    tokenizer_json = json_file.read()\n",
    "\n",
    "# Reconstruir el tokenizer desde el archivo JSON\n",
    "tokenizer = tokenizer_from_json(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(2172, 1)))\n",
    "    model.add(layers.Conv1D(64, kernel_size=5, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(64, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3)) # AÃ±adir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    conv1d = create_conv1d()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 2172, 1)) \n",
    "    # masked_input = layers.Masking(mask_value=0.0)(video_input)\n",
    "\n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(conv1d)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728586470.095621     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728586470.097565     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728586470.097744     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728586473.683146     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728586473.683215     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-10 18:54:33.683230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-10 18:54:33.683256: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:198] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1728586473.683856     518 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-10 18:54:33.683891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9355 MB memory:  -> device: 0, name: NVIDIA RTX 3500 Ada Generation Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "num_classes=len(tokenizer.word_index) + 1\n",
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/best_model.keras',\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_loss',\n",
    "    mode='min'  \n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7664"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss=tf.keras.losses.CTC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions,greedy):\n",
    "    predicted_classes = tf.argmax(predictions, axis=-1)\n",
    "    mask = tf.not_equal(predicted_classes, 0)\n",
    "    seq_lengths = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-1)\n",
    "    \n",
    "    predictions=tf.transpose(predictions,[1,0,2])\n",
    "\n",
    "    if greedy:\n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    \n",
    "    else:\n",
    "        decoded, _ = tf.nn.ctc_beam_search_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    decoded_dense = tf.sparse.to_dense(decoded[0], default_value=-1).numpy()\n",
    "    \n",
    "    # Extraer las secuencias decodificadas eliminando los -1 (valores de relleno)\n",
    "    decoded_sequences = []\n",
    "    for seq in decoded_dense:\n",
    "        decoded_sequences.append([val for val in seq if val != -1])\n",
    "    \n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 10 18:54:34 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   47C    P5              22W /  35W |    503MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=tf.data.Dataset.load('./train').unbatch()\n",
    "val_dataset=tf.data.Dataset.load('./val').unbatch()\n",
    "test_dataset=tf.data.Dataset.load('./test').unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_fn(points, translations):\n",
    "    # La longitud serÃ¡ la cantidad de keyframes (primera dimensiÃ³n de los puntos)\n",
    "    longitud=tf.shape(points)[0]\n",
    "    return longitud\n",
    "\n",
    "\n",
    "# limites = [300,500,700,1100]\n",
    "    \n",
    "# bucket_batch_sizes = [32, 32, 16, 8,8]\n",
    "\n",
    "def bucketed_dataset(dataset):\n",
    "    limites = [300,500,700,1100]\n",
    "    \n",
    "    bucket_batch_sizes = [4, 4,4, 4, 4]\n",
    "    return dataset.bucket_by_sequence_length(\n",
    "            element_length_func=length_fn,       \n",
    "            bucket_boundaries=limites, # LÃ­mites de los buckets\n",
    "            bucket_batch_sizes=bucket_batch_sizes,  # TamaÃ±os de batch para cada bucket\n",
    "            padded_shapes=(\n",
    "                # [None,48,48],\n",
    "                [None, 2172],  \n",
    "                [None]           \n",
    "            ),\n",
    "            padding_values=(\n",
    "                tf.constant(0, dtype=tf.float32), # Relleno con ceros para los puntos \n",
    "                tf.constant(0, dtype=tf.int32) # Relleno con ceros para las traducciones \n",
    "            )\n",
    "    )\n",
    "\n",
    "# Aplicar bucketing y padding\n",
    "train_dataset= train_dataset.apply(bucketed_dataset)\n",
    "val_dataset= val_dataset.apply(bucketed_dataset)\n",
    "test_dataset= test_dataset.apply(bucketed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 10 18:54:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   47C    P5              21W /  35W |    503MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 453, 2172]), TensorShape([4, 277]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "points_batch, translation_batch = next(iter(train_dataset))\n",
    "points_batch.shape, translation_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_dataset.cardinality().numpy()\n",
    "validation_steps = val_dataset.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 0batch [00:00, ?batch/s]\n",
      "Epoch:   0%|                                                                                                                                  | 0/100 [00:00<?, ?it/s]2024-10-10 18:54:44.913446: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n",
      "W0000 00:00:1728586487.489335     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.530873     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.538483     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.548140     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.560029     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.573126     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.581582     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.591021     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.600703     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.611412     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.619886     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.630190     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.641206     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.651576     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.663110     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.672473     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.681317     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.691601     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.703685     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.712669     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.723071     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.733657     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.742217     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.878825     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.888454     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.898985     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.908369     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.920890     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.930963     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.946601     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.955932     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.967086     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.979235     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586487.994862     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.007178     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.017748     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.028068     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.038823     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.046536     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.061958     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.071471     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.081725     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.091559     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.102920     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.114264     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.123362     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.133285     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.144227     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.153362     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1728586488.166421     518 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 100\n",
    "model.compile(optimizer=optimizador, loss=tf.keras.losses.CTC(), metrics=[calculate_bleu])\n",
    "\n",
    "\n",
    "# Iterar a travÃ©s de los batches\n",
    "for epoch in range(epochs):\n",
    "  with tf.device('/GPU:0'):\n",
    "    with tqdm(total=steps_per_epoch, desc=f'Epoch {epoch+1}/{epochs}', unit='batch') as pbar:\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "\n",
    "        for step, (points_batch, translation_batch) in enumerate(train_dataset):\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": epoch_loss_avg.result().numpy(),\n",
    "                \"Accuracy\": epoch_accuracy.result().numpy()\n",
    "            })\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(points_batch, training=True)\n",
    "                loss = tf.reduce_mean(ctc_loss(translation_batch, predictions))\n",
    "                \n",
    "            # Calcular y aplicar gradientes\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            predictions_decoded=decode(predictions, greedy=True)\n",
    "            epoch_accuracy.update_state(translation_batch, predictions_decoded)\n",
    "            epoch_loss_avg.update_state(loss)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # MÃ©tricas para la validaciÃ³n\n",
    "        val_loss_avg = tf.keras.metrics.Mean()\n",
    "        val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        val_bleu_avg = tf.keras.metrics.Mean()\n",
    "        with tqdm(total=validation_steps, desc=f'Epoch {epoch+1}/{epochs} (Validation)', unit='batch') as pbar_val:\n",
    "\n",
    "            for step, (points_batch, translation_batch) in enumerate(val_dataset):\n",
    "                \n",
    "                pbar_val.set_postfix({\n",
    "                    \"Val Loss\": val_loss_avg.result().numpy(),\n",
    "                    \"Val Accuracy\": val_acc_metric.result().numpy(),\n",
    "                    \"Val BLEU\": val_bleu_avg.result().numpy()\n",
    "                })\n",
    "                \n",
    "                # Evaluar en el conjunto de validaciÃ³n al final de cada epoch usando BLEU\n",
    "                predictions = model(points_batch, training=False)\n",
    "\n",
    "                val_loss=tf.reduce_mean(ctc_loss(translation_batch,predictions))\n",
    "                val_loss_avg.update_state(val_loss)\n",
    "\n",
    "                val_predictions = decode(predictions, greedy=False)\n",
    "                val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in val_predictions]\n",
    "\n",
    "\n",
    "                val_acc_metric.update_state(translation_batch,val_predictions)\n",
    "                val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "                val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "                val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "                val_bleu_avg.update_state(val_bleu)\n",
    "            \n",
    "            \n",
    "\n",
    "        checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss_avg, 'val_accuracy': val_acc_metric, 'val_bleu': val_bleu_avg})\n",
    "\n",
    "        if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss_avg, 'val_accuracy': val_acc_metric, 'val_bleu': val_bleu_avg}):\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "        print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "        print(f'Epoch: {epoch}, val_loss: {val_loss_avg}, val_acc: {val_acc_metric}, val_bleu: {val_bleu_avg}')\n",
    "\n",
    "    # Evaluar el modelo en el dataset de test\n",
    "    #test_loss, test_acc = model.evaluate(test_dataset)\n",
    "    #print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
