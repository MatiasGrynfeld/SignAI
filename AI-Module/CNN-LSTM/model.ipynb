{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  libcudnn8                       8.9.6.50-1+cuda12.2                     amd64        cuDNN runtime libraries\n"
     ]
    }
   ],
   "source": [
    "!dpkg -l | grep libcudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: docker: not found\n"
     ]
    }
   ],
   "source": [
    "!docker -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Wed_Nov_22_10:17:15_PST_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.107\n",
      "Build cuda_12.3.r12.3/compiler.33567101_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 14 17:09:18 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   43C    P5              23W /  35W |      0MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x86_64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.3 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n"
     ]
    }
   ],
   "source": [
    "!uname -m\n",
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/compilers/bin:$PATH\n",
    "!export PATH=/opt/nvidia//hpc_sdk/Linux_x86_64/21.3/cuda/12.3/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 16:14:17.185621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 16:14:17.348550: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 16:14:17.396845: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-16 16:14:17.710353: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729095270.927927       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729095271.022583       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729095271.022651       9 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "gpus=tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Cargar el archivo JSON\n",
    "with open('tokenizer.json') as json_file:\n",
    "    tokenizer_json = json_file.read()\n",
    "\n",
    "# Reconstruir el tokenizer desde el archivo JSON\n",
    "tokenizer = tokenizer_from_json(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(2172, 1)))\n",
    "    model.add(layers.Conv1D(64, kernel_size=5, activation='relu'))\n",
    "    model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(64, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3)) # Añadir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    conv1d = create_conv1d()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 2172, 1)) \n",
    "    # masked_input = layers.Masking(mask_value=0.0)(video_input)\n",
    "\n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(conv1d)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='linear')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.8/792.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.9.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss=tf.keras.losses.CTC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(predictions,greedy):\n",
    "    predicted_classes = tf.argmax(predictions, axis=-1)\n",
    "    mask = tf.not_equal(predicted_classes, 0)\n",
    "    seq_lengths = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-1)\n",
    "    \n",
    "    predictions=tf.transpose(predictions,[1,0,2])\n",
    "\n",
    "    if greedy:\n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    \n",
    "    else:\n",
    "        decoded, _ = tf.nn.ctc_beam_search_decoder(inputs=predictions, sequence_length=seq_lengths)\n",
    "    decoded_dense = tf.sparse.to_dense(decoded[0], default_value=-1).numpy()\n",
    "    \n",
    "    # Extraer las secuencias decodificadas eliminando los -1 (valores de relleno)\n",
    "    decoded_sequences = []\n",
    "    for seq in decoded_dense:\n",
    "        decoded_sequences.append([val for val in seq if val != -1])\n",
    "    \n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_fn(points, translations):\n",
    "    # La longitud será la cantidad de keyframes (primera dimensión de los puntos)\n",
    "    longitud=tf.shape(points)[0]\n",
    "    return longitud\n",
    "\n",
    "\n",
    "# limites = [300,500,700,1100]\n",
    "    \n",
    "# bucket_batch_sizes = [32, 32, 16, 8,8]\n",
    "\n",
    "def bucketed_dataset(dataset):\n",
    "    limites = [300,500,700,1100]\n",
    "    \n",
    "    bucket_batch_sizes = [1, 1, 1, 1, 1]\n",
    "    return dataset.bucket_by_sequence_length(\n",
    "            element_length_func=length_fn,       \n",
    "            bucket_boundaries=limites, # Límites de los buckets\n",
    "            bucket_batch_sizes=bucket_batch_sizes,  # Tamaños de batch para cada bucket\n",
    "            padded_shapes=(\n",
    "                # [None,48,48],\n",
    "                [None, 2172],  \n",
    "                [None]           \n",
    "            ),\n",
    "            padding_values=(\n",
    "                tf.constant(0, dtype=tf.float32), # Relleno con ceros para los puntos \n",
    "                tf.constant(0, dtype=tf.int32) # Relleno con ceros para las traducciones \n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    train_dataset=tf.data.Dataset.load('./train').unbatch()\n",
    "    val_dataset=tf.data.Dataset.load('./val').unbatch()\n",
    "    test_dataset=tf.data.Dataset.load('./test').unbatch()\n",
    "    # Aplicar bucketing y padding\n",
    "    train_dataset= train_dataset.apply(bucketed_dataset)\n",
    "    val_dataset= val_dataset.apply(bucketed_dataset)\n",
    "    test_dataset= test_dataset.apply(bucketed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 16 16:15:14 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   42C    P5              19W /  35W |    375MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A         9      C   /python3.11                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset element_spec=(TensorSpec(shape=(None, None, 2172), dtype=tf.float32, name=None), TensorSpec(shape=(None, None), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.cache('./cache_temporal')\n",
    "test_dataset.cache('./cache_temporal')\n",
    "val_dataset.cache('./cache_temporal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 2172), dtype=tf.float32, name=None), TensorSpec(shape=(None, None), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Rehabilita la GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 14 17:14:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.157                Driver Version: 538.18       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...    On  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   44C    P8               4W /  35W |    407MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A         8      C   /python3.11                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 453, 2172]), TensorShape([2, 277]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "points_batch, translation_batch = next(iter(train_dataset))\n",
    "points_batch.shape, translation_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_dataset.cardinality().numpy()\n",
    "validation_steps = val_dataset.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "def training(model,optimizador,checkpoint_callback,early_stopping):\n",
    "    epochs = 100\n",
    "    model.compile(optimizer=optimizador, loss=tf.keras.losses.CTC(), metrics=[calculate_bleu])\n",
    "\n",
    "\n",
    "# Iterar a través de los batches\n",
    "    for epoch in range(epochs):\n",
    "     with tf.device('/GPU:0'):\n",
    "        with tqdm(total=steps_per_epoch, desc=f'Epoch {epoch+1}/{epochs}', unit='batch') as pbar:\n",
    "            epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "            epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            # Métricas para la validación\n",
    "            val_loss_avg = tf.keras.metrics.Mean()\n",
    "            val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            val_bleu_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "            for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "                epoch_loss_avg.reset_states()\n",
    "                epoch_accuracy.reset_states()\n",
    "                val_acc_metric.reset_states()\n",
    "                val_loss_avg.reset_states()\n",
    "                val_bleu_avg.reset_states()\n",
    "\n",
    "                for step, (points_batch, translation_batch) in enumerate(train_dataset):\n",
    "                    pbar.set_postfix({\n",
    "                        \"Loss\": epoch_loss_avg.result().numpy(),\n",
    "                        \"Accuracy\": epoch_accuracy.result().numpy()\n",
    "                    })\n",
    "                    \n",
    "                    with tf.GradientTape() as tape:\n",
    "                        predictions = model(points_batch, training=True)\n",
    "                        loss = tf.reduce_mean(ctc_loss(translation_batch, predictions))\n",
    "                        \n",
    "                    # Calcular y aplicar gradientes\n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    predictions_decoded=decode(predictions, greedy=True)\n",
    "                    epoch_accuracy.update_state(translation_batch, predictions_decoded)\n",
    "                    epoch_loss_avg.update_state(loss)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                with tqdm(total=validation_steps, desc=f'Epoch {epoch+1}/{epochs} (Validation)', unit='batch') as pbar_val:\n",
    "\n",
    "                    for step, (points_batch, translation_batch) in enumerate(val_dataset):\n",
    "                        \n",
    "                        pbar_val.set_postfix({\n",
    "                            \"Val Loss\": val_loss_avg.result().numpy(),\n",
    "                            \"Val Accuracy\": val_acc_metric.result().numpy(),\n",
    "                            \"Val BLEU\": val_bleu_avg.result().numpy()\n",
    "                        })\n",
    "                        \n",
    "                        # Evaluar en el conjunto de validación al final de cada epoch usando BLEU\n",
    "                        predictions = model(points_batch, training=False)\n",
    "\n",
    "                        val_loss=tf.reduce_mean(ctc_loss(translation_batch,predictions))\n",
    "                        val_loss_avg.update_state(val_loss)\n",
    "\n",
    "                        val_predictions = decode(predictions, greedy=False)\n",
    "                        val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in val_predictions]\n",
    "\n",
    "\n",
    "                        val_acc_metric.update_state(translation_batch,val_predictions)\n",
    "                        val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in translation_batch]\n",
    "                        val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "                        val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "                        val_bleu_avg.update_state(val_bleu)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss_avg, 'val_accuracy': val_acc_metric, 'val_bleu': val_bleu_avg})\n",
    "\n",
    "                if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss_avg, 'val_accuracy': val_acc_metric, 'val_bleu': val_bleu_avg}):\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "                print(f\"Epoch: {epoch}, Loss: {epoch_loss_avg}, Accuracy: {epoch_accuracy}\")\n",
    "                print(f'Epoch: {epoch}, val_loss: {val_loss_avg}, val_acc: {val_acc_metric}, val_bleu: {val_bleu_avg}')\n",
    "\n",
    "            # Evaluar el modelo en el dataset de test\n",
    "            #test_loss, test_acc = model.evaluate(test_dataset)\n",
    "            #print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 0batch [00:00, ?batch/s, Loss=0, Accuracy=0]W0000 00:00:1729095529.833107       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.840534       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.844522       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.849770       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.854826       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.860183       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.864114       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.869807       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.874223       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.879080       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.891470       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.896480       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.902607       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.909564       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.922306       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.927055       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.933248       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.939764       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.946884       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.953104       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.962151       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095529.969054       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.018732       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.025252       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.032529       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.039275       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.057749       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.075342       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.091689       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.108429       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.113680       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.118553       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.136959       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.152479       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.166576       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.181388       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.186895       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.191837       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.207373       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.221889       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.236721       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.251684       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.255705       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.273699       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.291134       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.308135       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.328747       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.342402       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095530.361334       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095579.608382       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095582.923901       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095582.944699       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095582.965232       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095582.985543       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.010938       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.033374       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.056128       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.079119       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.109175       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.159690       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.185193       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.205162       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.229521       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.268158       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.309193       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.386421       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.472063       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.491501       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.526948       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.538871       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.550550       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.561787       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.600105       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.612770       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.623809       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.634587       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.646787       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.685658       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.697907       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.711189       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.722712       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095583.755295       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095584.674292       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.234657       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.248300       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.260433       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.286953       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.295157       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.303646       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.317535       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.328444       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.375014       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.382482       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.389026       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.396651       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.404035       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.411056       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.437888       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.446958       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.479571       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.506258       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.514652       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.527138       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.540113       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.553265       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.561354       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.588821       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.595498       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.603541       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.611922       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.620264       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.627522       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.662052       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1729095585.674423       9 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "Epoch:   0%|                                                                                                                       | 0/100 [01:39<?, ?it/s]\n",
      "Epoch 1/100: 0batch [01:39, ?batch/s, Loss=3.64e+3, Accuracy=0]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ArgMax_device_/job:localhost/replica:0/task:0/device:GPU:0}} Reduction axis -1 is empty in shape [1,0] [Op:ArgMax] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m      7\u001b[0m     filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/best_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,       \u001b[38;5;66;03m# Guardar solo si es el mejor modelo hasta ahora\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizador\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, optimizador, checkpoint_callback, early_stopping)\u001b[0m\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     29\u001b[0m     predictions_decoded\u001b[38;5;241m=\u001b[39mdecode(predictions, greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mepoch_accuracy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslation_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_decoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     epoch_loss_avg\u001b[38;5;241m.\u001b[39mupdate_state(loss)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Métricas para la validación\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/metrics/reduction_metrics.py:204\u001b[0m, in \u001b[0;36mMeanMetricWrapper.update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(y_pred, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 204\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         sample_weight \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mapply_mask(\n\u001b[1;32m    207\u001b[0m             sample_weight, mask, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/metrics/accuracy_metrics.py:240\u001b[0m, in \u001b[0;36msparse_categorical_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    238\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39msqueeze(y_true, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    239\u001b[0m     reshape_matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# If the predicted output and actual output types don't match, force cast\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# them to match.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mdtype:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/numpy.py:869\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(x, axis, keepdims)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Argmax(axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[0;32m--> 869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/numpy.py:772\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(x, axis, keepdims)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(x, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 772\u001b[0m y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n\u001b[1;32m    774\u001b[0m     y \u001b[38;5;241m=\u001b[39m _keepdims(_x, y, axis)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ArgMax_device_/job:localhost/replica:0/task:0/device:GPU:0}} Reduction axis -1 is empty in shape [1,0] [Op:ArgMax] name: "
     ]
    }
   ],
   "source": [
    "num_classes=len(tokenizer.word_index) + 1\n",
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizador)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/best_model.keras',\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_loss',\n",
    "    mode='min'  \n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "training(model,optimizador,checkpoint_callback,early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
