{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">1. Import Required Libraries & Dataset</h1>"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:43.768047Z","iopub.status.busy":"2021-05-20T12:16:43.7677Z","iopub.status.idle":"2021-05-20T12:16:43.773166Z","shell.execute_reply":"2021-05-20T12:16:43.772221Z","shell.execute_reply.started":"2021-05-20T12:16:43.767999Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import ast\n","#import torch\n","#import torch.nn as nn\n","#from sklearn.metrics import classification_report\n","#from transformers import AutoModel, BertTokenizer\n","\n","# specify GPU\n","#device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:45.214458Z","iopub.status.busy":"2021-05-20T12:16:45.214126Z","iopub.status.idle":"2021-05-20T12:16:45.244325Z","shell.execute_reply":"2021-05-20T12:16:45.243406Z","shell.execute_reply.started":"2021-05-20T12:16:45.21443Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(\"train_vids.csv\")\n","df_val = pd.read_csv(\"val_vids.csv\")\n","df_test = pd.read_csv(\"test_vids.csv\")\n","df_train['points'] = df_train['points'].apply(ast.literal_eval)\n","df_val['points'] = df_val['points'].apply(ast.literal_eval)\n","df_test['points'] = df_test['points'].apply(ast.literal_eval)\n","max_len_train = df_train['points'].apply(lambda x: len(x)).max()\n","max_len_val = df_val['points'].apply(lambda x: len(x)).max()\n","max_len_test = df_test['points'].apply(lambda x: len(x)).max()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["(                                              points  \\\n"," 0  [[0.0, 1.0, 0.6193, 1.0, 0.0902, 0.6181, 0.947...   \n"," 1  [[1.0, 0.9541, 1.0, 1.0, 0.867, 0.539, 0.9602,...   \n"," \n","                                          translation  id  \n"," 0  Okay sausage today we're going to do a creole ...   0  \n"," 1  Today I am going to be showing you how to make...   1  ,\n","                                               points  \\\n"," 0  [[1.0, 0.9541, 1.0, 1.0, 0.867, 0.539, 0.9602,...   \n"," \n","                                          translation  id  \n"," 0  Today I am going to be showing you how to make...   0  ,\n","                                               points  \\\n"," 0  [[1.0, 0.9541, 1.0, 1.0, 0.867, 0.539, 0.9602,...   \n"," \n","                                          translation  id  \n"," 0  Today I am going to be showing you how to make...   0  ,\n"," 65,\n"," 65,\n"," 65)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head(), df_val.head(), df_test.head(), max_len_train, max_len_val, max_len_test"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def add_padding(max_frames, pointSeries: pd.Series):\n","    for i in range(len(pointSeries)):\n","        current_length = len(pointSeries[i])\n","        if current_length < max_frames:\n","            padding = np.full(\n","                (max_frames - current_length, 300), \n","                -1\n","            )\n","            padding[:, 3::4] = 0\n","            pointSeries[i] = np.concatenate((pointSeries[i], padding), axis=0)\n","\n","    return pointSeries"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["((2,), (1,), (1,))"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df_train[\"points\"] = add_padding(max_len_train, df_train[\"points\"])\n","df_val[\"points\"] = add_padding(max_len_val, df_val[\"points\"])\n","df_test[\"points\"] = add_padding(max_len_test, df_test[\"points\"])"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">3. Import Bert - base- uncased</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T11:49:24.18724Z","iopub.status.busy":"2021-05-20T11:49:24.186902Z","iopub.status.idle":"2021-05-20T11:50:15.137513Z","shell.execute_reply":"2021-05-20T11:50:15.135994Z","shell.execute_reply.started":"2021-05-20T11:49:24.187212Z"},"trusted":true},"outputs":[],"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenize and encode sequences in the training set\n","tokens_label_train = tokenizer.batch_encode_plus(\n","    df_train['translation'].tolist(),\n","    padding = True\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_label_val = tokenizer.batch_encode_plus(\n","    df_val['translation'].tolist(),\n","    padding = True\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_label_test = tokenizer.batch_encode_plus(\n","    df_test['translation'].tolist(),\n","    padding = True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">5. List to Tensors</h1>"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def create_attention_mask_from_points(seq_tensor):\n","    mask = torch.ones(seq_tensor.shape, dtype=torch.long)\n","    missing_data = (seq_tensor[..., :3] == -1).all(dim=-1) & (seq_tensor[..., 3] == 0)\n","    mask[missing_data] = 0\n","    return mask"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:52.707444Z","iopub.status.busy":"2021-05-20T12:16:52.707085Z","iopub.status.idle":"2021-05-20T12:16:52.744645Z","shell.execute_reply":"2021-05-20T12:16:52.743449Z","shell.execute_reply.started":"2021-05-20T12:16:52.707415Z"},"trusted":true},"outputs":[],"source":["# convert lists to tensors\n","\n","train_seq = torch.stack([torch.tensor(seq) for seq in df_train['points'].tolist()])\n","train_mask = create_attention_mask_from_points(train_seq)\n","#train_y = tokens_label_train['input_ids']\n","\n","val_seq = torch.stack([torch.tensor(seq) for seq in df_val['points'].tolist()])\n","val_mask = create_attention_mask_from_points(val_seq)\n","#val_y = tokens_label_val['input_ids']\n","\n","test_seq = torch.stack([torch.tensor(seq) for seq in df_test['points'].tolist()])\n","test_mask = create_attention_mask_from_points(test_seq)\n","#test_y = tokens_label_test['input_ids']"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">6. Data Loader</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:54.073069Z","iopub.status.busy":"2021-05-20T12:16:54.072724Z","iopub.status.idle":"2021-05-20T12:16:54.079387Z","shell.execute_reply":"2021-05-20T12:16:54.078289Z","shell.execute_reply.started":"2021-05-20T12:16:54.073019Z"},"trusted":true},"outputs":[],"source":["\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">7. Model Architecture</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab_size = tokenizer.vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:02.994288Z","iopub.status.busy":"2021-05-20T12:23:02.99389Z","iopub.status.idle":"2021-05-20T12:23:03.006243Z","shell.execute_reply":"2021-05-20T12:23:03.005349Z","shell.execute_reply.started":"2021-05-20T12:23:02.994244Z"},"trusted":true},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert, vocab_size):\n","        super(BERT_Arch, self).__init__()\n","        \n","        self.bert = bert \n","        \n","        # Dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # ReLU activation function\n","        self.relu = nn.ReLU()\n","\n","        # Dense layer 1\n","        self.fc1 = nn.Linear(768, 512)\n","        \n","        # Dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512, vocab_size)  # Use tokenizer's vocab size\n","\n","        # Softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, seq_input, mask):\n","        outputs = self.bert(seq_input, attention_mask=mask, return_dict=False)\n","        \n","        # Use the last hidden state for each token (outputs[0])\n","        x = self.fc1(outputs[0])  # (batch_size, sequence_length, 512)\n","\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","\n","        x = self.fc2(x)  # (batch_size, sequence_length, vocab_size)\n","        \n","        # Apply softmax activation (for each token in the sequence)\n","        x = self.softmax(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:03.953226Z","iopub.status.busy":"2021-05-20T12:23:03.952902Z","iopub.status.idle":"2021-05-20T12:23:03.968064Z","shell.execute_reply":"2021-05-20T12:23:03.967316Z","shell.execute_reply.started":"2021-05-20T12:23:03.953198Z"},"trusted":true},"outputs":[],"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:04.903784Z","iopub.status.busy":"2021-05-20T12:23:04.903449Z","iopub.status.idle":"2021-05-20T12:23:04.910844Z","shell.execute_reply":"2021-05-20T12:23:04.909852Z","shell.execute_reply.started":"2021-05-20T12:23:04.90375Z"},"trusted":true},"outputs":[],"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),lr = 1e-5) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:06.96248Z","iopub.status.busy":"2021-05-20T12:23:06.96215Z","iopub.status.idle":"2021-05-20T12:23:06.967108Z","shell.execute_reply":"2021-05-20T12:23:06.966225Z","shell.execute_reply.started":"2021-05-20T12:23:06.962452Z"},"trusted":true},"outputs":[],"source":["# Define the loss function\n","cross_entropy = nn.CrossEntropyLoss()\n","\n","# number of training epochs\n","epochs = 10"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">8. Fine - Tune</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:08.233269Z","iopub.status.busy":"2021-05-20T12:23:08.232922Z","iopub.status.idle":"2021-05-20T12:23:08.241591Z","shell.execute_reply":"2021-05-20T12:23:08.240484Z","shell.execute_reply.started":"2021-05-20T12:23:08.233239Z"},"trusted":true},"outputs":[],"source":["def train():\n","    \n","    model.train()\n","    total_loss = 0\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step,batch in enumerate(train_dataloader):\n","        \n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","        \n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","\n","        sent_id, mask, labels = batch\n","        \n","        # clear previously calculated gradients \n","        model.zero_grad()        \n","\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    #returns the loss and predictions\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:09.063944Z","iopub.status.busy":"2021-05-20T12:23:09.063641Z","iopub.status.idle":"2021-05-20T12:23:09.073677Z","shell.execute_reply":"2021-05-20T12:23:09.070943Z","shell.execute_reply.started":"2021-05-20T12:23:09.063916Z"},"trusted":true},"outputs":[],"source":["# function for evaluating the model\n","def evaluate():\n","    \n","    print(\"\\nEvaluating...\")\n","  \n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","    \n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step,batch in enumerate(val_dataloader):\n","        \n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            \n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","            \n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds,labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader) \n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:09.897241Z","iopub.status.busy":"2021-05-20T12:23:09.896772Z","iopub.status.idle":"2021-05-20T12:24:20.56839Z","shell.execute_reply":"2021-05-20T12:24:20.567287Z","shell.execute_reply.started":"2021-05-20T12:23:09.897201Z"},"trusted":true},"outputs":[],"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:24:47.832686Z","iopub.status.busy":"2021-05-20T12:24:47.832372Z","iopub.status.idle":"2021-05-20T12:24:48.147778Z","shell.execute_reply":"2021-05-20T12:24:48.147016Z","shell.execute_reply.started":"2021-05-20T12:24:47.832657Z"},"trusted":true},"outputs":[],"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">9. Make Predictions</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:07.436008Z","iopub.status.busy":"2021-05-20T12:25:07.435685Z","iopub.status.idle":"2021-05-20T12:25:08.019816Z","shell.execute_reply":"2021-05-20T12:25:08.018976Z","shell.execute_reply.started":"2021-05-20T12:25:07.435979Z"},"trusted":true},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:23.838719Z","iopub.status.busy":"2021-05-20T12:25:23.838389Z","iopub.status.idle":"2021-05-20T12:25:23.852193Z","shell.execute_reply":"2021-05-20T12:25:23.850795Z","shell.execute_reply.started":"2021-05-20T12:25:23.838691Z"},"trusted":true},"outputs":[],"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":863500,"sourceId":1471804,"sourceType":"datasetVersion"}],"dockerImageVersionId":30096,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
