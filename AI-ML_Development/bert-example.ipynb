{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">1. Import Required Libraries & Dataset</h1>"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:43.768047Z","iopub.status.busy":"2021-05-20T12:16:43.7677Z","iopub.status.idle":"2021-05-20T12:16:43.773166Z","shell.execute_reply":"2021-05-20T12:16:43.772221Z","shell.execute_reply.started":"2021-05-20T12:16:43.767999Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\48519558\\Desktop\\SignAI-ML\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import ast\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics import classification_report\n","from transformers import AutoModel, BertTokenizer\n","\n","# specify GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:45.214458Z","iopub.status.busy":"2021-05-20T12:16:45.214126Z","iopub.status.idle":"2021-05-20T12:16:45.244325Z","shell.execute_reply":"2021-05-20T12:16:45.243406Z","shell.execute_reply.started":"2021-05-20T12:16:45.21443Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(\"train_vids.csv\")\n","df_val = pd.read_csv(\"val_vids.csv\")\n","df_test = pd.read_csv(\"test_vids.csv\")\n","df_train['points'] = df_train['points'].apply(ast.literal_eval)\n","df_val['points'] = df_val['points'].apply(ast.literal_eval)\n","df_test['points'] = df_test['points'].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_train.head(), df_val.head(), df_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">3. Import Bert - base- uncased</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T11:49:24.18724Z","iopub.status.busy":"2021-05-20T11:49:24.186902Z","iopub.status.idle":"2021-05-20T11:50:15.137513Z","shell.execute_reply":"2021-05-20T11:50:15.135994Z","shell.execute_reply.started":"2021-05-20T11:49:24.187212Z"},"trusted":true},"outputs":[],"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenize and encode sequences in the training set\n","tokens_label_train = tokenizer.batch_encode_plus(\n","    df_train['translation'].tolist(),\n","    padding = True\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_label_val = tokenizer.batch_encode_plus(\n","    df_val['translation'].tolist(),\n","    padding = True\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_label_test = tokenizer.batch_encode_plus(\n","    df_test['translation'].tolist(),\n","    padding = True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">5. List to Tensors</h1>"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def cell2listTensor(list_of_lists):\n","    return torch.tensor(list_of_lists)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def create_attention_mask_from_points(seq_tensor):\n","    mask = torch.ones(seq_tensor.shape, dtype=torch.long)\n","    missing_data = (seq_tensor[..., :3] == -1).all(dim=-1) & (seq_tensor[..., 3] == 0)\n","    mask[missing_data] = 0\n","    return mask"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:52.707444Z","iopub.status.busy":"2021-05-20T12:16:52.707085Z","iopub.status.idle":"2021-05-20T12:16:52.744645Z","shell.execute_reply":"2021-05-20T12:16:52.743449Z","shell.execute_reply.started":"2021-05-20T12:16:52.707415Z"},"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [50, 300] at entry 0 and [65, 300] at entry 1","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# convert lists to tensors\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m train_seq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m train_mask \u001b[38;5;241m=\u001b[39m create_attention_mask_from_points(train_seq)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#train_y = tokens_label_train['input_ids']\u001b[39;00m\n","\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [50, 300] at entry 0 and [65, 300] at entry 1"]}],"source":["# convert lists to tensors\n","\n","train_seq = torch.stack([torch.tensor(seq) for seq in df_train['points'].tolist()])\n","train_mask = create_attention_mask_from_points(train_seq)\n","#train_y = tokens_label_train['input_ids']\n","\n","val_seq = torch.stack([torch.tensor(seq) for seq in df_val['points'].tolist()])\n","val_mask = create_attention_mask_from_points(val_seq)\n","#val_y = tokens_label_val['input_ids']\n","\n","test_seq = torch.stack([torch.tensor(seq) for seq in df_test['points'].tolist()])\n","test_mask = create_attention_mask_from_points(test_seq)\n","#test_y = tokens_label_test['input_ids']"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">6. Data Loader</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:54.073069Z","iopub.status.busy":"2021-05-20T12:16:54.072724Z","iopub.status.idle":"2021-05-20T12:16:54.079387Z","shell.execute_reply":"2021-05-20T12:16:54.078289Z","shell.execute_reply.started":"2021-05-20T12:16:54.073019Z"},"trusted":true},"outputs":[],"source":["\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">7. Model Architecture</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:16:55.343696Z","iopub.status.busy":"2021-05-20T12:16:55.343359Z","iopub.status.idle":"2021-05-20T12:16:55.349996Z","shell.execute_reply":"2021-05-20T12:16:55.348883Z","shell.execute_reply.started":"2021-05-20T12:16:55.343669Z"},"trusted":true},"outputs":[],"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:02.994288Z","iopub.status.busy":"2021-05-20T12:23:02.99389Z","iopub.status.idle":"2021-05-20T12:23:03.006243Z","shell.execute_reply":"2021-05-20T12:23:03.005349Z","shell.execute_reply.started":"2021-05-20T12:23:02.994244Z"},"trusted":true},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","        \n","        self.bert = bert \n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # relu activation function\n","        self.relu =  nn.ReLU()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768,512)\n","        \n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512,2)\n","\n","        #softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","        \n","        #pass the inputs to the model  \n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","        \n","        x = self.fc1(cls_hs)\n","\n","        x = self.relu(x)\n","\n","        x = self.dropout(x)\n","\n","        # output layer\n","        x = self.fc2(x)\n","      \n","        # apply softmax activation\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:03.953226Z","iopub.status.busy":"2021-05-20T12:23:03.952902Z","iopub.status.idle":"2021-05-20T12:23:03.968064Z","shell.execute_reply":"2021-05-20T12:23:03.967316Z","shell.execute_reply.started":"2021-05-20T12:23:03.953198Z"},"trusted":true},"outputs":[],"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:04.903784Z","iopub.status.busy":"2021-05-20T12:23:04.903449Z","iopub.status.idle":"2021-05-20T12:23:04.910844Z","shell.execute_reply":"2021-05-20T12:23:04.909852Z","shell.execute_reply.started":"2021-05-20T12:23:04.90375Z"},"trusted":true},"outputs":[],"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),lr = 1e-5) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:05.840566Z","iopub.status.busy":"2021-05-20T12:23:05.840224Z","iopub.status.idle":"2021-05-20T12:23:05.849927Z","shell.execute_reply":"2021-05-20T12:23:05.84887Z","shell.execute_reply.started":"2021-05-20T12:23:05.840537Z"},"trusted":true},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(\"Class Weights:\",class_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:06.96248Z","iopub.status.busy":"2021-05-20T12:23:06.96215Z","iopub.status.idle":"2021-05-20T12:23:06.967108Z","shell.execute_reply":"2021-05-20T12:23:06.966225Z","shell.execute_reply.started":"2021-05-20T12:23:06.962452Z"},"trusted":true},"outputs":[],"source":["\n","# converting list of class weights to a tensor\n","weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","# push to GPU\n","weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","# number of training epochs\n","epochs = 10"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">8. Fine - Tune</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:08.233269Z","iopub.status.busy":"2021-05-20T12:23:08.232922Z","iopub.status.idle":"2021-05-20T12:23:08.241591Z","shell.execute_reply":"2021-05-20T12:23:08.240484Z","shell.execute_reply.started":"2021-05-20T12:23:08.233239Z"},"trusted":true},"outputs":[],"source":["# function to train the model\n","def train():\n","    \n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","  \n","    # empty list to save model predictions\n","    total_preds=[]\n","  \n","    # iterate over batches\n","    for step,batch in enumerate(train_dataloader):\n","        \n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","        \n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n"," \n","        sent_id, mask, labels = batch\n","        \n","        # clear previously calculated gradients \n","        model.zero_grad()        \n","\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","  \n","      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","      # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    #returns the loss and predictions\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:09.063944Z","iopub.status.busy":"2021-05-20T12:23:09.063641Z","iopub.status.idle":"2021-05-20T12:23:09.073677Z","shell.execute_reply":"2021-05-20T12:23:09.070943Z","shell.execute_reply.started":"2021-05-20T12:23:09.063916Z"},"trusted":true},"outputs":[],"source":["# function for evaluating the model\n","def evaluate():\n","    \n","    print(\"\\nEvaluating...\")\n","  \n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","    \n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step,batch in enumerate(val_dataloader):\n","        \n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            \n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","            \n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds,labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader) \n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:23:09.897241Z","iopub.status.busy":"2021-05-20T12:23:09.896772Z","iopub.status.idle":"2021-05-20T12:24:20.56839Z","shell.execute_reply":"2021-05-20T12:24:20.567287Z","shell.execute_reply.started":"2021-05-20T12:23:09.897201Z"},"trusted":true},"outputs":[],"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:24:47.832686Z","iopub.status.busy":"2021-05-20T12:24:47.832372Z","iopub.status.idle":"2021-05-20T12:24:48.147778Z","shell.execute_reply":"2021-05-20T12:24:48.147016Z","shell.execute_reply.started":"2021-05-20T12:24:47.832657Z"},"trusted":true},"outputs":[],"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:white;\">9. Make Predictions</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:07.436008Z","iopub.status.busy":"2021-05-20T12:25:07.435685Z","iopub.status.idle":"2021-05-20T12:25:08.019816Z","shell.execute_reply":"2021-05-20T12:25:08.018976Z","shell.execute_reply.started":"2021-05-20T12:25:07.435979Z"},"trusted":true},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T12:25:23.838719Z","iopub.status.busy":"2021-05-20T12:25:23.838389Z","iopub.status.idle":"2021-05-20T12:25:23.852193Z","shell.execute_reply":"2021-05-20T12:25:23.850795Z","shell.execute_reply.started":"2021-05-20T12:25:23.838691Z"},"trusted":true},"outputs":[],"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":863500,"sourceId":1471804,"sourceType":"datasetVersion"}],"dockerImageVersionId":30096,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
