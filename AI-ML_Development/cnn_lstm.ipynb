{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "df_train = pd.read_csv(\"train_vids.csv\")\n",
    "df_val = pd.read_csv(\"val_vids.csv\")\n",
    "df_test = pd.read_csv(\"test_vids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_points(points_str):\n",
    "    points = np.array(eval(points_str))\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointsToCnnInputForm(frame):\n",
    "    returnVector=[]\n",
    "    for keyFrame in frame:\n",
    "        zero_row = [0] * 36\n",
    "        keyFrame = np.concatenate([keyFrame, zero_row])\n",
    "        keyFrame = keyFrame.reshape(46,48)\n",
    "        zeros_5x20 = np.zeros((2, 48))\n",
    "        returnVector.append(np.vstack((keyFrame, zeros_5x20)))\n",
    "    return returnVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio el formato de points para que sea una lista de 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['points']=df_train['points'].apply(parse_points)\n",
    "df_val['points']=df_val['points'].apply(parse_points)\n",
    "df_test['points']=df_test['points'].apply(parse_points)\n",
    "df_train['points']=df_train['points'].apply(pointsToCnnInputForm)\n",
    "df_val['points']=df_val['points'].apply(pointsToCnnInputForm)\n",
    "df_test['points']=df_test['points'].apply(pointsToCnnInputForm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordenar por longitud\n",
    "df_train=df_train.sort_values(by='len_keyframes')\n",
    "df_test=df_test.sort_values(by='len_keyframes')\n",
    "df_val=df_val.sort_values(by='len_keyframes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar texto de traducciones para que sea compatible con el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Crear el tokenizador\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustar el tokenizador al texto de las traducciones\n",
    "tokenizer.fit_on_texts(df_train['translation'])\n",
    "\n",
    "# Convertir las oraciones en secuencias de enteros\n",
    "df_train['translation_sequence'] = tokenizer.texts_to_sequences(df_train['translation'])\n",
    "df_val['translation_sequence'] = tokenizer.texts_to_sequences(df_val['translation'])\n",
    "df_test['translation_sequence'] = tokenizer.texts_to_sequences(df_test['translation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo datasets de Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(df):\n",
    "    points = np.stack(df['points'].values)\n",
    "    sequence = np.stack(df['translation_sequence'].values)\n",
    "    return tf.data.Dataset.from_tensor_slices((points, sequence))\n",
    "\n",
    "train_dataset = create_tf_dataset(df_train)\n",
    "val_dataset = create_tf_dataset(df_val)\n",
    "test_dataset = create_tf_dataset(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el padding para que las secuencias dentro de los batches tengan la misma longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_sequences_fn(points, maxlen):\n",
    "    return pad_sequences(points, maxlen=maxlen, dtype='float32', padding='post')\n",
    "\n",
    "def dynamic_padding_fn(max_len, batch):\n",
    "    def pad_batch(batch):\n",
    "        points, labels = batch\n",
    "        # Convertir el batch a una lista de secuencias\n",
    "        points_padded = tf.numpy_function(pad_sequences_fn, [points, max_len], tf.float32)\n",
    "        return points_padded, labels\n",
    "    return pad_batch\n",
    "\n",
    "def get_max_len(points):\n",
    "    # Encuentra la longitud máxima en el batch actual\n",
    "    lengths = tf.map_fn(lambda x: tf.shape(x)[0], points, dtype=tf.int32)\n",
    "    return tf.reduce_max(lengths)\n",
    "\n",
    "def pad_batches(dataset):\n",
    "    def pad_batch_fn(batch):\n",
    "        points, sequence = batch\n",
    "        max_len = get_max_len(points)  # Encuentra la longitud máxima en el batch\n",
    "        return dynamic_padding_fn(max_len,batch)  # Aplica el padding dinámico\n",
    "    mapped_ds=dataset.map(pad_batch_fn)\n",
    "    return mapped_ds\n",
    "\n",
    "train_dataset = pad_batches(train_dataset.batch(32))\n",
    "val_dataset = pad_batches(val_dataset.batch(32))\n",
    "test_dataset = pad_batches(test_dataset.batch(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear modelo,**EJEMPLO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model=models.Sequential()\n",
    "    # Primera capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3)) # Añadir Dropout\n",
    "    \n",
    "    # Segunda capa LSTM con return_sequences=True\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Tercera capa LSTM con return_sequences=False\n",
    "    model.add(layers.LSTM(64, return_sequences=False))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(num_classes):\n",
    "    cnn = create_cnn()\n",
    "    \n",
    "    video_input = layers.Input(shape=(None, 48, 48, 1)) \n",
    "    \n",
    "    # Aplicar CNN a cada frame usando TimeDistributed\n",
    "    cnn_features = layers.TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    lstm=create_lstm()\n",
    "    lstm_out= lstm(cnn_features)\n",
    "    # Capa final de salida\n",
    "    output = layers.Dense(num_classes, activation='softmax')(lstm_out)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return np.mean([sentence_bleu([ref], hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(tokenizer.word_index) + 1\n",
    "model = create_cnn_lstm_model(num_classes)\n",
    "\n",
    "# Compilar el modelo\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='RUTA',  # Ruta donde se guarda el modelo\n",
    "    save_best_only=True,       # Guardar solo si es el mejor modelo hasta ahora\n",
    "    monitor='val_bleu',        # Métrica a monitorear\n",
    "    mode='max'                 # Modo: minimizar la métrica monitorizada\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo usando un ciclo de entrenamiento personalizado\n",
    "epochs = 20  # Ajusta el número de épocas según sea necesario\n",
    "\n",
    "# Iterar a través de los batches y ajustar manualmente los parámetros\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for step, (points_batch, translation_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(points_batch, training=True)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(translation_batch, predictions))\n",
    "        \n",
    "        # Calcular y aplicar gradientes\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}: loss = {loss.numpy()}\")\n",
    "\n",
    "    val_loss, val_acc = model.evaluate(val_dataset)\n",
    "    print(f\"Validation loss: {val_loss}, Validation accuracy: {val_acc}\")\n",
    "    \n",
    "    # Evaluar en el conjunto de validación al final de cada epoch usando BLEU\n",
    "    val_references = [tokenizer.sequences_to_texts([ref.numpy()]) for ref in val_dataset.map(lambda x, y: y)]\n",
    "    val_predictions = model.predict(val_dataset.map(lambda x, y: x))\n",
    "    val_hypotheses = [tokenizer.sequences_to_texts([pred]) for pred in np.argmax(val_predictions, axis=-1)]\n",
    "    val_bleu = calculate_bleu(references = val_references, hypotheses = val_hypotheses)\n",
    "    print(f\"Validation BLEU: {val_bleu}\")\n",
    "\n",
    "    checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu})\n",
    "    if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc, 'val_bleu': val_bleu}):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    \n",
    "    checkpoint_callback.on_epoch_end(epoch, logs={'val_bleu': val_bleu, 'val_accuracy': val_acc})\n",
    "    # Comprobación temprana\n",
    "    if early_stopping.on_epoch_end(epoch, logs={'val_loss': val_loss, 'val_accuracy': val_acc}):\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Evaluar el modelo en el dataset de test\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
