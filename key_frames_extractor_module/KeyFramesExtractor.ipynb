{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mediapipe():\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    return mp_hands, mp_drawing, hands\n",
    "\n",
    "def procesar_frame(frame, hands):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image)\n",
    "    return results\n",
    "\n",
    "def calcular_diferencia(puntos_previos,puntos_actuales ):\n",
    "    if puntos_previos is None or puntos_actuales is None:\n",
    "        return float(1.0)\n",
    "    \n",
    "    # Comprobamos si alguna de las listas está vacía o si las dos listas tienen diferentes longitudes\n",
    "    if len(puntos_previos) != len(puntos_actuales):\n",
    "        return float(1.0)\n",
    "\n",
    "    num_hands_previas = len(puntos_previos)\n",
    "    num_hands_actuales = len(puntos_actuales)\n",
    "\n",
    "    if num_hands_previas != num_hands_actuales:\n",
    "        return float(1.0)\n",
    "\n",
    "    total_diff = float(0.0)\n",
    "\n",
    "    for i in range(num_hands_previas):\n",
    "        hand_prev = puntos_previos[i].landmark\n",
    "        hand_actual = puntos_actuales[i].landmark\n",
    "\n",
    "        diff = sum([\n",
    "            abs(lm1.x - lm2.x) + abs(lm1.y - lm2.y) + abs(lm1.z - lm2.z)\n",
    "            for lm1, lm2 in zip(hand_prev, hand_actual)\n",
    "        ])\n",
    "        \n",
    "        total_diff += diff\n",
    "\n",
    "    if num_hands_previas >= 2:\n",
    "        total_diff /= num_hands_previas\n",
    "    return float(total_diff)\n",
    "\n",
    "#def extract_key_frames(video_path, threshold=0.5, min_interval_frames=10):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f\"Error: El archivo {video_path} no existe.\")\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir el video.\")\n",
    "        return []\n",
    "\n",
    "    mp_hands, mp_drawing, hands = initialize_mediapipe()\n",
    "    puntos_previos = []\n",
    "    key_frames = []\n",
    "    print(\"Iniciando la extracción de frames clave\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Fin del video o error al leer frame\")\n",
    "            break\n",
    "        results = procesar_frame(frame, hands)\n",
    "        if results.multi_hand_landmarks:\n",
    "            puntos_actuales = results.multi_hand_landmarks\n",
    "            print(type(puntos_actuales))\n",
    "            #for hand_landmarks in puntos_actuales:\n",
    "             #   mp_drawing.draw_landmarks(\n",
    "              #      frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "               #     mp_drawing.DrawingSpec(color=(255,255,0), thickness=4, circle_radius=5),\n",
    "                #    mp_drawing.DrawingSpec(color=(255,0,255), thickness=4))\n",
    "                #no sé como mostrarlo\n",
    "\n",
    "            diff = calcular_diferencia(puntos_previos, puntos_actuales)\n",
    "\n",
    "            if diff > threshold:\n",
    "                key_frames.append(frame)\n",
    "                puntos_previos = puntos_actuales\n",
    "        else:\n",
    "            puntos_previos = None\n",
    "\n",
    "    cap.release()\n",
    "    return key_frames\n",
    "def extract_key_frames(video_path, threshold=0.4, min_frame_interval=6):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f\"Error: El archivo {video_path} no existe.\")\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir el video.\")\n",
    "        return []\n",
    "\n",
    "    mp_hands, mp_drawing, hands = initialize_mediapipe()\n",
    "    puntos_previos = []\n",
    "    key_frames = []\n",
    "    frame_count = 0\n",
    "    print(\"Iniciando la extracción de frames clave\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Fin del video o error al leer frame\")\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        results = procesar_frame(frame, hands)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            puntos_actuales = results.multi_hand_landmarks\n",
    "            diff = calcular_diferencia(puntos_previos, puntos_actuales)\n",
    "\n",
    "            if diff > threshold and (len(key_frames) == 0 or frame_count - key_frames[-1][1] > min_frame_interval):\n",
    "                key_frames.append((frame, frame_count))\n",
    "                puntos_previos = puntos_actuales\n",
    "        else:\n",
    "            puntos_previos = None\n",
    "\n",
    "    cap.release()\n",
    "    return [frame for frame, _ in key_frames]\n",
    "\n",
    "def save_key_frames(key_frames, output_folder):\n",
    "    for idx, key_frame in enumerate(key_frames):\n",
    "        cv2.imwrite(f'{output_folder}/key_frame_{idx}.png', key_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la extracción de frames clave\n",
      "Fin del video o error al leer frame\n"
     ]
    }
   ],
   "source": [
    "video_path = 'C:\\\\Users\\\\48113164\\\\Documents\\\\GitHub\\\\SignAI-IA.dev\\\\key frames extractor\\\\videoprueba.mp4'\n",
    "output_folder='C:\\\\Users\\\\48113164\\\\Documents\\\\GitHub\\\\SignAI-IA.dev\\\\frames'\n",
    "key_frames = extract_key_frames(video_path)\n",
    "save_key_frames(key_frames, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
